{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Print the CUDA version that PyTorch was built with\n",
    "print(\"Built CUDA Version:\", torch.version.cuda)\n",
    "\n",
    "# Check if CUDA is available on the current system\n",
    "if torch.cuda.is_available():\n",
    "    # Print the CUDA runtime version (compiled version)\n",
    "    print(\"CUDA Runtime Version:\", torch._C._cuda_getCompiledVersion())\n",
    "    \n",
    "    # Print the name of the first available GPU\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    # Inform the user if CUDA is not available\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e738113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of class IDs to class names and their corresponding RGB color codes\n",
    "class_id_to_name = {\n",
    "    0:  ('unlabeled', [28, 42, 168]),         # Background or unclassified area\n",
    "    1:  ('pool', [0, 50, 89]),                # Swimming pool\n",
    "    2:  ('vegetation', [107, 142, 35]),       # Trees, grass, or bushes\n",
    "    3:  ('roof', [70, 70, 70]),               # Building rooftops\n",
    "    4:  ('wall', [102, 102, 156]),            # Building walls\n",
    "    5:  ('window', [254, 228, 12]),           # Windows\n",
    "    6:  ('person', [255, 22, 96]),            # People\n",
    "    7:  ('dog', [102, 51, 0]),                # Dogs\n",
    "    8:  ('car', [9, 143, 150]),               # Cars\n",
    "    9:  ('bicycle', [119, 11, 32]),           # Bicycles\n",
    "    10: ('tree', [51, 51, 0]),                # Trees\n",
    "    11: ('truck', [160, 160, 60]),            # Trucks (added)\n",
    "    12: ('bus', [200, 80, 80]),               # Buses (added)\n",
    "    13: ('vehicle', [20, 80, 80]),            # General vehicle category (added)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75103a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NumPy - fundamental package for numerical computations\n",
    "!pip install numpy\n",
    "\n",
    "# Install OpenCV - library for computer vision tasks\n",
    "!pip install opencv-python\n",
    "\n",
    "# Install Pillow - image processing library\n",
    "!pip install pillow\n",
    "\n",
    "# Install Matplotlib - plotting and visualization library\n",
    "!pip install matplotlib\n",
    "\n",
    "# Install tqdm - progress bar utility\n",
    "!pip install tqdm\n",
    "\n",
    "# Install scikit-learn - machine learning tools\n",
    "!pip install scikit-learn\n",
    "\n",
    "# Install PyTorch and TorchVision - deep learning framework and its vision tools\n",
    "!pip install torch torchvision\n",
    "\n",
    "# Install Ultralytics - YOLO model implementation and training tools\n",
    "!pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os                     # Operating system interfaces\n",
    "import gc                     # Garbage collection interface\n",
    "import json                   # Working with JSON data\n",
    "import shutil                 # File operations like copy, move, etc.\n",
    "import zipfile                # Extracting zip archives\n",
    "import random                 # Random number generation\n",
    "from glob import glob         # Pattern matching for file paths\n",
    "from pathlib import Path      # Object-oriented file path handling\n",
    "from collections import defaultdict  # Dictionary with default value support\n",
    "import xml.etree.ElementTree as ET  # Parsing XML files\n",
    "\n",
    "# Scientific computing and data manipulation\n",
    "import numpy as np            # Numerical operations\n",
    "import pandas as pd           # Data analysis and manipulation\n",
    "from sklearn.model_selection import train_test_split  # Train-test split\n",
    "\n",
    "# Image processing and visualization\n",
    "import cv2                    # OpenCV for computer vision\n",
    "from PIL import Image, ImageDraw, ImageFont  # PIL for image handling\n",
    "import matplotlib.pyplot as plt              # Plotting library\n",
    "import matplotlib.patches as mpatches        # Drawing patches on plots\n",
    "\n",
    "# Progress bar utility\n",
    "from tqdm.auto import tqdm    # Progress bars for loops\n",
    "\n",
    "# PyTorch and related imports\n",
    "import torch\n",
    "import torch.nn as nn         # Neural network modules\n",
    "from torch.utils.data import DataLoader       # Efficient data loading\n",
    "import torchvision.models as models           # Pretrained models\n",
    "import torchvision.transforms as transforms   # Image transformations\n",
    "import torchvision.models.segmentation as segmentation  # Segmentation models\n",
    "\n",
    "# YOLO from Ultralytics\n",
    "from ultralytics import YOLO  # YOLO object detection models\n",
    "\n",
    "# Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Google Drive downloader\n",
    "import gdown                  # Downloading files from Google Drive\n",
    "\n",
    "# Environment configuration\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"   # Avoids OpenMP duplicate library error\n",
    "\n",
    "# Set device for computation (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe928bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_drone_dataset_download(gdrive_url, extract_to=\"extracted\"):\n",
    "    \"\"\"\n",
    "    Downloads and extracts the Semantic Drone Dataset from a Google Drive URL.\n",
    "    \n",
    "    Parameters:\n",
    "        gdrive_url (str): The shared Google Drive link to the ZIP file.\n",
    "        extract_to (str): Directory to extract contents into. Default is 'extracted'.\n",
    "    \"\"\"\n",
    "    # Extract the file ID from the Google Drive shareable URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[INFO] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[INFO] Extracting ZIP...\")\n",
    "    # Extract contents of the ZIP file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Delete the ZIP file after extraction to save space\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[DONE] Extracted files to: {extract_to}\")\n",
    "\n",
    "\n",
    "def uavdt_dataset_download(gdrive_url, extract_to=\"extracted\"):\n",
    "    \"\"\"\n",
    "    Downloads and extracts the UAVDT Dataset from a Google Drive URL.\n",
    "    \n",
    "    Parameters:\n",
    "        gdrive_url (str): The shared Google Drive link to the ZIP file.\n",
    "        extract_to (str): Directory to extract contents into. Default is 'extracted'.\n",
    "    \"\"\"\n",
    "    # Extract the file ID from the Google Drive shareable URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[+] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[+] Extracting ZIP...\")\n",
    "    # Extract contents of the ZIP file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Delete the ZIP file after extraction to save space\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[+] Extracted files to: {extract_to}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Parse polygon and convert to YOLO bbox\n",
    "# ----------------------------\n",
    "\n",
    "# Parses XML annotation and converts polygon objects to YOLO-style bounding boxes\n",
    "def parse_yolo_style_bbox_from_xml(xml_path, class_id_to_name):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "        if class_name in [value[0] for value in class_id_to_name.values()]:\n",
    "            polygon = obj.find('polygon')\n",
    "            if polygon is not None:\n",
    "                # Extract points from polygon\n",
    "                points = polygon.findall('pt')\n",
    "                coords = [(float(pt.find('x').text), float(pt.find('y').text)) for pt in points]\n",
    "                # Convert polygon to bounding box\n",
    "                x_min = min(coord[0] for coord in coords)\n",
    "                y_min = min(coord[1] for coord in coords)\n",
    "                x_max = max(coord[0] for coord in coords)\n",
    "                y_max = max(coord[1] for coord in coords)\n",
    "                bboxes.append(((x_min, y_min), (x_max, y_max), class_name))\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Save YOLO-format txt\n",
    "# ----------------------------\n",
    "\n",
    "# Saves the bounding boxes in YOLO format: <class_id> <x_center> <y_center> <width> <height>\n",
    "def save_yolo_format(image_id, bboxes, image_width, image_height, output_path, class_id_to_name):\n",
    "    with open(output_path, 'w') as f:\n",
    "        for (x_min, y_min), (x_max, y_max), class_name in bboxes:\n",
    "            class_id = next(cid for cid, (name, _) in class_id_to_name.items() if name == class_name)\n",
    "            x_center = (x_min + x_max) / 2 / image_width\n",
    "            y_center = (y_min + y_max) / 2 / image_height\n",
    "            width = (x_max - x_min) / image_width\n",
    "            height = (y_max - y_min) / image_height\n",
    "            # Write to file with six decimal precision\n",
    "            f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Convert dataset (YOLO only)\n",
    "# ----------------------------\n",
    "\n",
    "# Converts the full dataset by extracting YOLO-style annotations and saving them\n",
    "def convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name):\n",
    "    # Get list of image IDs (without extension)\n",
    "    image_ids = [img.split('.')[0] for img in os.listdir(f\"{dataset_path}/images\") if img.endswith(\".jpg\")]\n",
    "\n",
    "    # Create output folders\n",
    "    os.makedirs(f\"{output_dir}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/labels\", exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(image_ids, desc=\"Converting to YOLO\"):\n",
    "        img_path = f\"{dataset_path}/images/{image_id}.jpg\"\n",
    "        bbox_xml_path = f\"{dataset_path}/gt/bounding_box/label_me_xml/{image_id}.xml\"\n",
    "        semantic_xml_path = f\"{dataset_path}/gt/semantic/label_me_xml/{image_id}.xml\"\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"[WARNING] Image not found: {img_path}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Parse bounding box and semantic annotations\n",
    "            bboxes1 = parse_yolo_style_bbox_from_xml(bbox_xml_path, class_id_to_name)\n",
    "            bboxes2 = parse_yolo_style_bbox_from_xml(semantic_xml_path, class_id_to_name)\n",
    "            all_bboxes = bboxes1 + bboxes2\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Skipping image {image_id} due to parse error: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(img_path)\n",
    "            image_np = np.array(image)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not load image {image_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save image to output directory\n",
    "        image.save(f\"{output_dir}/images/{image_id}.jpg\")\n",
    "\n",
    "        # Save YOLO-format labels to output directory\n",
    "        yolo_annotation_path = f\"{output_dir}/labels/{image_id}.txt\"\n",
    "        save_yolo_format(image_id, all_bboxes, image_np.shape[1], image_np.shape[0], yolo_annotation_path, class_id_to_name)\n",
    "\n",
    "    print(\"[+] YOLO-format annotation conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c96868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Mapping UAVDT class IDs to extended class IDs used in the combined dataset\n",
    "uavdt_to_extended = {\n",
    "    0: 8,   # car\n",
    "    1: 11,  # truck\n",
    "    2: 12,  # bus\n",
    "    3: 13   # other vehicle\n",
    "}\n",
    "\n",
    "# === Function to convert a single annotation file to YOLO format ===\n",
    "def convert_annotation(anno_path, label_path, image_path, stats):\n",
    "    if not os.path.exists(image_path):\n",
    "        stats[\"missing_image\"] += 1\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        height, width = img.shape[:2]\n",
    "    except:\n",
    "        stats[\"missing_image\"] += 1\n",
    "        return\n",
    "\n",
    "    with open(anno_path, 'r') as fin, open(label_path, 'w') as fout:\n",
    "        for line in fin:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) < 8:\n",
    "                stats[\"malformed\"] += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Parse bounding box and class info\n",
    "                x, y, w, h = map(float, parts[0:4])\n",
    "                original_cls = int(parts[5])\n",
    "\n",
    "                # Skip classes not in our mapping\n",
    "                if original_cls not in uavdt_to_extended:\n",
    "                    stats[\"skipped\"][original_cls] += 1\n",
    "                    continue\n",
    "\n",
    "                # Convert to new class ID\n",
    "                cls = uavdt_to_extended[original_cls]\n",
    "\n",
    "                # Convert to YOLO format (normalized center_x, center_y, width, height)\n",
    "                x_center = (x + w / 2) / width\n",
    "                y_center = (y + h / 2) / height\n",
    "                w /= width\n",
    "                h /= height\n",
    "\n",
    "                # Validate normalized coordinates\n",
    "                if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and w > 0 and h > 0):\n",
    "                    stats[\"skipped\"][cls] += 1\n",
    "                    continue\n",
    "\n",
    "                # Write label line\n",
    "                fout.write(f\"{cls} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "                stats[\"converted\"] += 1\n",
    "            except Exception:\n",
    "                stats[\"malformed\"] += 1\n",
    "                continue\n",
    "\n",
    "            stats[\"total\"] += 1\n",
    "\n",
    "# === Step 1: Convert all UAVDT annotations to YOLO format ===\n",
    "def convert_dataset(root_dir):\n",
    "    # Find all annotation files inside any Mxxxx/annotations/ directory\n",
    "    annotation_paths = glob(os.path.join(root_dir, \"M*/annotations/*.txt\"))\n",
    "    total_files = len(annotation_paths)\n",
    "\n",
    "    # Stats for tracking issues and progress\n",
    "    stats = {\n",
    "        \"total\": 0,\n",
    "        \"converted\": 0,\n",
    "        \"malformed\": 0,\n",
    "        \"missing_image\": 0,\n",
    "        \"skipped\": defaultdict(int)\n",
    "    }\n",
    "\n",
    "    print(f\"ðŸ”„ Converting {total_files} annotation files to YOLO format...\")\n",
    "\n",
    "    for anno_path in tqdm(annotation_paths, desc=\"Converting\", unit=\"file\"):\n",
    "        # Get sequence directory (e.g., M0101)\n",
    "        sequence_dir = os.path.dirname(os.path.dirname(anno_path))\n",
    "        file_name = os.path.basename(anno_path)\n",
    "\n",
    "        # Output label directory\n",
    "        label_dir = os.path.join(sequence_dir, \"labels\")\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "        # Output label file path\n",
    "        label_path = os.path.join(label_dir, file_name)\n",
    "\n",
    "        # Corresponding image path\n",
    "        image_name = file_name.replace(\".txt\", \".jpg\")\n",
    "        image_path = os.path.join(sequence_dir, \"images\", image_name)\n",
    "\n",
    "        # Perform the actual conversion\n",
    "        convert_annotation(anno_path, label_path, image_path, stats)\n",
    "\n",
    "    # Print summary of the conversion process\n",
    "    print(\"\\nConversion complete.\")\n",
    "    print(f\"Total boxes:     {stats['total']}\")\n",
    "    print(f\"Converted boxes: {stats['converted']}\")\n",
    "    print(f\"Skipped boxes:   {sum(stats['skipped'].values())}\")\n",
    "    for cls, count in sorted(stats[\"skipped\"].items()):\n",
    "        print(f\"   - Skipped class {cls}: {count}\")\n",
    "    print(f\"Malformed lines: {stats['malformed']}\")\n",
    "    print(f\"Missing images:  {stats['missing_image']}\")\n",
    "\n",
    "# === Step 2: Split dataset into train/val and copy files ===\n",
    "def copy_split_sequences(src_root, dst_root, train_ratio=0.8):\n",
    "    # Find all sequences (Mxxxx folders)\n",
    "    all_sequences = sorted(glob(os.path.join(src_root, \"M*\")))\n",
    "\n",
    "    # Split into training and validation sequences\n",
    "    train_seqs, val_seqs = train_test_split(all_sequences, train_size=train_ratio, random_state=42)\n",
    "\n",
    "    # Copy files into respective folders\n",
    "    for split_name, split_list in zip(['train', 'val'], [train_seqs, val_seqs]):\n",
    "        for seq_path in tqdm(split_list, desc=f\"Copying {split_name}\"):\n",
    "            images_src = os.path.join(seq_path, \"images\")\n",
    "            labels_src = os.path.join(seq_path, \"labels\")\n",
    "\n",
    "            images_dst = os.path.join(dst_root, split_name, \"images\")\n",
    "            labels_dst = os.path.join(dst_root, split_name, \"labels\")\n",
    "\n",
    "            os.makedirs(images_dst, exist_ok=True)\n",
    "            os.makedirs(labels_dst, exist_ok=True)\n",
    "\n",
    "            # Copy image files\n",
    "            for img_file in glob(os.path.join(images_src, \"*.jpg\")):\n",
    "                shutil.copy(img_file, os.path.join(images_dst, os.path.basename(img_file)))\n",
    "\n",
    "            # Copy label files\n",
    "            for label_file in glob(os.path.join(labels_src, \"*.txt\")):\n",
    "                shutil.copy(label_file, os.path.join(labels_dst, os.path.basename(label_file)))\n",
    "\n",
    "    print(\"\\n[+] Dataset split into 'train/' and 'val/' folders with images and YOLO-format labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to move files from source directories to target directories\n",
    "def move_files(file_list, \n",
    "               source_image_dir, \n",
    "               source_annotation_dir,\n",
    "               target_image_dir, \n",
    "               target_annotation_dir):\n",
    "    \n",
    "    # Create target directories if they don't exist\n",
    "    os.makedirs(target_image_dir, exist_ok=True)\n",
    "    os.makedirs(target_annotation_dir, exist_ok=True)\n",
    "\n",
    "    # Loop through each file in the provided list and move the corresponding image and annotation\n",
    "    for image_id in tqdm(file_list, desc=f\"Moving to {os.path.basename(os.path.dirname(target_image_dir))}\"):\n",
    "        # Construct paths for the image and annotation\n",
    "        image_path = os.path.join(source_image_dir, f\"{image_id}.jpg\")\n",
    "        annotation_path = os.path.join(source_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        # Construct target paths for the image and annotation\n",
    "        target_image_path = os.path.join(target_image_dir, f\"{image_id}.jpg\")\n",
    "        target_annotation_path = os.path.join(target_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        # Check if both the image and annotation files exist, then copy them to target directories\n",
    "        if os.path.exists(image_path) and os.path.exists(annotation_path):\n",
    "            shutil.copy(image_path, target_image_path)\n",
    "            shutil.copy(annotation_path, target_annotation_path)\n",
    "\n",
    "# Function to split the dataset into training and validation sets, and move the files\n",
    "def split_and_move_dataset(source_base_dir=\"./datasets/semantic_yolo\",\n",
    "                           target_base_dir=\"./datasets/new_dataset_yolo_split\",\n",
    "                           split_ratio=0.8,\n",
    "                           seed=42):\n",
    "    \n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Define paths for the image and label directories\n",
    "    image_dir = os.path.join(source_base_dir, \"images\")\n",
    "    label_dir = os.path.join(source_base_dir, \"labels\")\n",
    "\n",
    "    # Get all image IDs (file names without extensions) from the image directory\n",
    "    image_ids = [os.path.splitext(f)[0] for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    # Shuffle the image IDs to randomize the split\n",
    "    random.shuffle(image_ids)\n",
    "\n",
    "    # Determine the split index based on the split ratio\n",
    "    split_idx = int(len(image_ids) * split_ratio)\n",
    "\n",
    "    # Split the image IDs into training and validation sets\n",
    "    train_ids = image_ids[:split_idx]\n",
    "    val_ids = image_ids[split_idx:]\n",
    "\n",
    "    # Move the training images and annotations to the target directories\n",
    "    move_files(train_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"train/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"train/labels\"))\n",
    "\n",
    "    # Move the validation images and annotations to the target directories\n",
    "    move_files(val_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"val/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"val/labels\"))\n",
    "\n",
    "    # Print the summary of the dataset split\n",
    "    print(f\"\\n[âœ“] Dataset split completed: {len(train_ids)} train / {len(val_ids)} val samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_label_file(label_file, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Normalize the label coordinates in a label file to ensure they are within [0, 1] range.\n",
    "    The label file is updated with the normalized values.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    with open(label_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            # Split the line by spaces to get the class and coordinates\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, width, height = map(float, parts[1:])\n",
    "            \n",
    "            # Normalize the coordinates to be within the range [0, 1]\n",
    "            x_center = min(1.0, max(0.0, x_center))\n",
    "            y_center = min(1.0, max(0.0, y_center))\n",
    "            width = min(1.0, max(0.0, width))\n",
    "            height = min(1.0, max(0.0, height))\n",
    "\n",
    "            # Write the normalized values back to the file\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "\n",
    "def get_image_size(img_path):\n",
    "    \"\"\"\n",
    "    Get the width and height of the image to normalize the coordinates properly.\n",
    "    This function uses PIL (Pillow) to open the image and return its dimensions.\n",
    "    \"\"\"\n",
    "    with Image.open(img_path) as img:\n",
    "        return img.size  # returns (width, height)\n",
    "\n",
    "\n",
    "def normalize_all_labels(labels_dir, img_dir):\n",
    "    \"\"\"\n",
    "    Normalize all label files in the specified directory.\n",
    "    It reads each label file, gets the corresponding image size, and normalizes the label coordinates.\n",
    "    \"\"\"\n",
    "    for label_file in tqdm(os.listdir(labels_dir)):  # Iterate over all files in the labels directory\n",
    "       \n",
    "        if label_file.endswith('.txt'):  # Process only label files\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            img_path = os.path.join(img_dir, label_file.replace('.txt', '.jpg'))  # Assuming JPG images\n",
    "            \n",
    "            if os.path.exists(img_path):\n",
    "                # Get the image dimensions to normalize the labels\n",
    "                img_width, img_height = get_image_size(img_path)\n",
    "                normalize_label_file(label_path, img_width, img_height)\n",
    "            else:\n",
    "                # Warning if the corresponding image is missing\n",
    "                print(f\"Warning: Image for label {label_file} not found!\")\n",
    "    \n",
    "    print(\"Normalization Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184d91d",
   "metadata": {},
   "source": [
    "## Calling Funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41576e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive URL for the Semantic Drone Dataset\n",
    "gdrive_url = \"https://drive.google.com/file/d/1UppumYqYOi-kto6BWPfFxwJK2Eph46oY/view?usp=sharing\"\n",
    "# Call the function to download and extract the Semantic Drone Dataset\n",
    "semantic_drone_dataset_download(gdrive_url, extract_to=\"datasets\")\n",
    "\n",
    "# Google Drive URL for the UAVDT Dataset\n",
    "gdrive_url = \"https://drive.google.com/file/d/12cbrTaBAMIsuU-mwAA7IgDk9wSLC9cC-/view?usp=sharing\"\n",
    "# Call the function to download and extract the UAVDT Dataset\n",
    "uavdt_dataset_download(gdrive_url, extract_to=\"datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Semantic Drone Dataset training set\n",
    "dataset_path = \"./datasets/semantic_drone_dataset/training_set\" \n",
    "\n",
    "# Output directory where the YOLO formatted dataset will be saved\n",
    "output_dir = \"./datasets/semantic_yolo\"\n",
    "\n",
    "# Call the function to convert the full dataset into YOLO format\n",
    "# The function converts annotations and images from the Semantic Drone Dataset into YOLO format\n",
    "convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa649fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UAVDT-2024 Dataset Processing\n",
    "\n",
    "# Path to the source UAVDT-2024 dataset\n",
    "source_root = \"./datasets/UAVDT-2024\"\n",
    "\n",
    "# Output directory where the new YOLO format dataset will be saved\n",
    "output_root = \"./datasets/new_dataset_yolo_split\"\n",
    "\n",
    "# Convert the UAVDT-2024 dataset into YOLO format\n",
    "# The function processes the dataset and converts annotations and images into YOLO format\n",
    "convert_dataset(source_root)\n",
    "\n",
    "# Split the dataset into training and validation sets with a ratio of 80:20\n",
    "# This function copies the relevant sequences into the respective directories for training and validation\n",
    "copy_split_sequences(source_root, output_root, train_ratio=0.8)\n",
    "\n",
    "\n",
    "# Semantic Drone Datasets Processing\n",
    "\n",
    "# Split and move the Semantic Drone dataset into training and validation sets\n",
    "# The function handles the splitting of the dataset and moves the images and annotations into separate directories\n",
    "split_and_move_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your paths for the training dataset\n",
    "dataset_path = \"./datasets/new_dataset_yolo_split/train\"\n",
    "\n",
    "# Directory where the images are stored in the training dataset\n",
    "image_dir = os.path.join(dataset_path, \"images\")\n",
    "\n",
    "# Directory where the label files are stored in the training dataset\n",
    "annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# Normalize all label files in the training dataset by adjusting coordinates\n",
    "# This function ensures that the labels follow the expected YOLO format (normalized coordinates)\n",
    "normalize_all_labels(annotations_dir, image_dir)\n",
    "\n",
    "# Set your paths for the validation dataset\n",
    "dataset_path = \"./datasets/new_dataset_yolo_split/val\"\n",
    "\n",
    "# Directory where the images are stored in the validation dataset\n",
    "image_dir = os.path.join(dataset_path, \"images\")\n",
    "\n",
    "# Directory where the label files are stored in the validation dataset\n",
    "annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# Normalize all label files in the validation dataset\n",
    "normalize_all_labels(annotations_dir, image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16bb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the training labels directory\n",
    "labels_dir = './datasets/new_dataset_yolo_split/train/labels'\n",
    "\n",
    "# Get the list of rare class IDs by analyzing the label files in the specified directory\n",
    "# The function `get_rare_class_ids` will count the number of occurrences of each class\n",
    "# and return those with occurrences below the specified threshold (in this case, 3000)\n",
    "rare_class_ids = get_rare_class_ids(label_dir=labels_dir, class_id_to_name=class_id_to_name, rare_threshold=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe2989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfefb1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
