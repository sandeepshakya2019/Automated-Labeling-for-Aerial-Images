{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c62209f8",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8ff18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install opencv-python\n",
    "# !pip install pillow\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch torchvision\n",
    "# !pip install ultralytics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8431144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core packages\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "# Math and array handling\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Image and visualization\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep Learning Frameworks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.models.segmentation as segmentation\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Object Detection and Segmentation\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Automatically use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275651f4",
   "metadata": {},
   "source": [
    "### Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42512998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def download_and_extract_from_gdrive(gdrive_url, extract_to=\"extracted\"):\n",
    "    # Convert shared drive URL to direct download URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create output folder\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[INFO] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[INFO] Extracting ZIP...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Optionally, remove the ZIP file after extraction\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[DONE] Extracted files to: {extract_to}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1e94f",
   "metadata": {},
   "source": [
    "### Convert downloaded Dataset into yolo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7dbcea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Function to parse XML files for bounding boxes (YOLO annotations)\n",
    "def parse_bbox_xml(xml_path, class_id_to_name):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "        if class_name in [value[0] for value in class_id_to_name.values()]:  # Only the specified classes\n",
    "            polygon = obj.find('polygon')\n",
    "            if polygon is not None:\n",
    "                points = polygon.findall('pt')\n",
    "                coords = [(float(pt.find('x').text), float(pt.find('y').text)) for pt in points]\n",
    "                # Get bounding box (min, max x and y)\n",
    "                x_min = min([coord[0] for coord in coords])\n",
    "                y_min = min([coord[1] for coord in coords])\n",
    "                x_max = max([coord[0] for coord in coords])\n",
    "                y_max = max([coord[1] for coord in coords])\n",
    "                bboxes.append(((x_min, y_min), (x_max, y_max), class_name))\n",
    "    return bboxes\n",
    "\n",
    "# Function to parse XML files for segmentation polygons (mask annotations)\n",
    "def parse_segmentation_xml(xml_path, class_id_to_name):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "        if class_name in [value[0] for value in class_id_to_name.values()]:  # Only the specified classes\n",
    "            polygon = obj.find('polygon')\n",
    "            if polygon is not None:\n",
    "                points = polygon.findall('pt')\n",
    "                coords = [(float(pt.find('x').text), float(pt.find('y').text)) for pt in points]\n",
    "                # Get bounding box (min, max x and y)\n",
    "                x_min = min([coord[0] for coord in coords])\n",
    "                y_min = min([coord[1] for coord in coords])\n",
    "                x_max = max([coord[0] for coord in coords])\n",
    "                y_max = max([coord[1] for coord in coords])\n",
    "                bboxes.append(((x_min, y_min), (x_max, y_max), class_name))\n",
    "    return bboxes\n",
    "\n",
    "# Function to create a segmentation mask based on the class color mapping\n",
    "def create_segmentation_mask(image_size, bboxes, class_id_to_name):\n",
    "    segmentation_mask = np.zeros((image_size[0], image_size[1], 3), dtype=np.uint8)\n",
    "    for (x_min, y_min), (x_max, y_max), class_name in bboxes:\n",
    "        class_id, class_rgb = next(((cid, rgb) for cid, (name, rgb) in class_id_to_name.items() if name == class_name), (None, None))\n",
    "        if class_rgb is not None:\n",
    "            segmentation_mask[int(y_min):int(y_max), int(x_min):int(x_max)] = class_rgb\n",
    "    return segmentation_mask\n",
    "\n",
    "# Function to save annotations in YOLO format\n",
    "def save_yolo_format(image_id, bboxes, image_width, image_height, output_path, class_id_to_name):\n",
    "    with open(output_path, 'w') as f:\n",
    "        for (x_min, y_min), (x_max, y_max), class_name in bboxes:\n",
    "            class_id = next(cid for cid, (name, _) in class_id_to_name.items() if name == class_name)\n",
    "            # Normalize bounding box coordinates\n",
    "            x_center = (x_min + x_max) / 2 / image_width\n",
    "            y_center = (y_min + y_max) / 2 / image_height\n",
    "            width = (x_max - x_min) / image_width\n",
    "            height = (y_max - y_min) / image_height\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "def convert_fulldataset(dataset_path, output_dir, class_id_to_name):\n",
    "    # Loop through all the images in the dataset\n",
    "    image_ids = [img.split('.')[0] for img in os.listdir(f\"{dataset_path}/images\") if img.endswith(\".jpg\")]\n",
    "\n",
    "    # Create the output directory structure if it doesn't exist\n",
    "    os.makedirs(f\"{output_dir}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/segmentation_mask\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/labels\", exist_ok=True)\n",
    "\n",
    "    # Process each image\n",
    "    for image_id in tqdm(image_ids):\n",
    "        img_path = f\"{dataset_path}/images/{image_id}.jpg\"\n",
    "        xml_bbox_path = f\"{dataset_path}/gt/bounding_box/label_me_xml/{image_id}.xml\"\n",
    "        xml_mask_path = f\"{dataset_path}/gt/semantic/label_me_xml/{image_id}.xml\"\n",
    "\n",
    "        # Check if image file exists\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"[WARNING] Image not found: {img_path}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Try parsing bounding box and mask annotations\n",
    "        try:\n",
    "            bboxes = parse_bbox_xml(xml_bbox_path, class_id_to_name)  # for YOLO annotation\n",
    "            mask_boxes = parse_segmentation_xml(xml_mask_path, class_id_to_name)  # for segmentation mask\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Skipping image {image_id} due to parse error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            image = np.array(image)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not load image {image_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Create segmentation mask from polygons\n",
    "        image_size = image.shape[:2]\n",
    "        segmentation_mask = create_segmentation_mask(image_size, mask_boxes, class_id_to_name)\n",
    "\n",
    "        # Save original image\n",
    "        image_pil = Image.fromarray(image)\n",
    "        image_pil.save(f\"{output_dir}/images/{image_id}.jpg\")\n",
    "\n",
    "        # Save segmentation mask\n",
    "        segmentation_mask_image = Image.fromarray(segmentation_mask)\n",
    "        segmentation_mask_image.save(f\"{output_dir}/segmentation_mask/{image_id}_segmentation_mask.png\")\n",
    "\n",
    "        # Save YOLO annotations (bounding boxes)\n",
    "        yolo_annotation_path = f\"{output_dir}/labels/{image_id}.txt\"\n",
    "        save_yolo_format(image_id, bboxes, image_size[1], image_size[0], yolo_annotation_path, class_id_to_name)\n",
    "\n",
    "        print(\"[+] Processing completed for all images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4683246",
   "metadata": {},
   "source": [
    "### Visualizing images with bounding boxes and segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8af9cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Path to the dataset\n",
    "dataset_path = \"./datasets/new_dataset\"\n",
    "image_dir = os.path.join(dataset_path, \"images\")\n",
    "segmentation_mask_dir = os.path.join(dataset_path, \"segmentation_mask\")\n",
    "annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "\n",
    "# Function to load YOLO bounding box annotations\n",
    "def load_yolo_annotations(anno_file):\n",
    "    with open(anno_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    boxes = []\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "        boxes.append((class_id, x_center, y_center, width, height))\n",
    "    return boxes\n",
    "\n",
    "# Function to visualize the segmentation mask and bounding boxes\n",
    "def visualize_data(image_id, class_id_to_name):\n",
    "    # Load image\n",
    "    img_path = os.path.join(image_dir, f\"{image_id}.jpg\")\n",
    "    segmentation_mask_path = os.path.join(segmentation_mask_dir, f\"{image_id}_segmentation_mask.png\")\n",
    "    annotation_path = os.path.join(annotations_dir, f\"{image_id}.txt\")\n",
    "    \n",
    "    if not os.path.exists(img_path) or not os.path.exists(segmentation_mask_path) or not os.path.exists(annotation_path):\n",
    "        print(f\"[WARNING] Missing files for {image_id}, skipping...\")\n",
    "        return\n",
    "\n",
    "    # Load image and segmentation mask\n",
    "    image = Image.open(img_path)\n",
    "    image = np.array(image)\n",
    "    segmentation_mask = Image.open(segmentation_mask_path)\n",
    "    segmentation_mask = np.array(segmentation_mask)\n",
    "\n",
    "    # Load YOLO annotations\n",
    "    boxes = load_yolo_annotations(annotation_path)\n",
    "\n",
    "    # Plotting the image with bounding boxes and segmentation mask overlay\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Plot original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Original Image: {image_id}\")\n",
    "    \n",
    "    # Plot segmentation mask with overlay\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(segmentation_mask, alpha=0.7)  # Overlay segmentation mask with transparency\n",
    "    plt.title(f\"Segmentation Mask: {image_id}\")\n",
    "\n",
    "    # Draw bounding boxes on the image\n",
    "    for box in boxes:\n",
    "        class_id, x_center, y_center, width, height = box\n",
    "        class_name = class_id_to_name[int(class_id)][0]\n",
    "        x_min = int((x_center - width / 2) * image.shape[1])\n",
    "        y_min = int((y_center - height / 2) * image.shape[0])\n",
    "        x_max = int((x_center + width / 2) * image.shape[1])\n",
    "        y_max = int((y_center + height / 2) * image.shape[0])\n",
    "\n",
    "        # Draw bounding box\n",
    "        color = class_id_to_name[int(class_id)][1]\n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "        cv2.putText(image, class_name, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 4, color, 16)\n",
    "    \n",
    "    # Show the image with bounding boxes\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Image with Bounding Boxes: {image_id}\")\n",
    "\n",
    "    # Add color legend to indicate class-to-color mapping\n",
    "    legend_handles = []\n",
    "    for class_id, (class_name, class_rgb) in class_id_to_name.items():\n",
    "        color = np.array(class_rgb) / 255.0  # Normalize to [0, 1] range for plotting\n",
    "        legend_handles.append(mpatches.Patch(color=color, label=f\"{class_name} ({class_id})\"))\n",
    "    \n",
    "    plt.legend(handles=legend_handles, loc='upper right', fontsize=10)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ab554",
   "metadata": {},
   "source": [
    "### Convert into train set and val set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d7373103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(file_list, source_image_dir, source_mask_dir, source_annotation_dir,\n",
    "               target_image_dir, target_mask_dir, target_annotation_dir):\n",
    "    for image_id in tqdm(file_list, desc=\"Moving files\"):\n",
    "        image_path = os.path.join(source_image_dir, f\"{image_id}.jpg\")\n",
    "        mask_path = os.path.join(source_mask_dir, f\"{image_id}_segmentation_mask.png\")\n",
    "        annotation_path = os.path.join(source_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        # Move image file\n",
    "        if os.path.exists(image_path):\n",
    "            shutil.copy(image_path, target_image_dir)\n",
    "        \n",
    "        # Move segmentation mask file\n",
    "        if os.path.exists(mask_path):\n",
    "            shutil.copy(mask_path, target_mask_dir)\n",
    "        \n",
    "        # Move YOLO annotation file\n",
    "        if os.path.exists(annotation_path):\n",
    "            shutil.copy(annotation_path, target_annotation_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f66204",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe2d2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo(data_yaml=\"uavdt.yaml\", epochs=40, imgsz=640, batch=32, name=\"yolov8-uavdt\"):\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "    model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        show=True,\n",
    "        name=name,\n",
    "        project=\"runs/train\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Segmentation Dataset Class\n",
    "# ----------------------------\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = sorted(os.listdir(image_dir))\n",
    "        self.mask_filenames = sorted(os.listdir(mask_dir))\n",
    "\n",
    "        self.class_id_to_name = {\n",
    "            0: ('road', [28, 42, 168]),\n",
    "            1: ('pool', [0, 50, 89]),\n",
    "            2: ('vegetation', [107, 142, 35]),\n",
    "            3: ('roof', [70, 70, 70]),\n",
    "            4: ('wall', [102, 102, 156]),\n",
    "            5: ('window', [254, 228, 12]),\n",
    "            6: ('person', [255, 22, 96]),\n",
    "            7: ('dog', [102, 51, 0]),\n",
    "            8: ('car', [9, 143, 150]),\n",
    "            9: ('bicycle', [119, 11, 32]),\n",
    "            10: ('tree', [51, 51, 0]),\n",
    "        }\n",
    "\n",
    "        self.class_id_map = {orig_id: new_id for new_id, orig_id in enumerate(self.class_id_to_name.keys())}\n",
    "        self.class_names = [name[0] for name in self.class_id_to_name.values()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB').resize((256, 256))\n",
    "        mask = Image.open(mask_path).convert('RGB').resize((256, 256))\n",
    "        mask = self.convert_mask_to_class_ids(np.array(mask))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = torch.tensor(mask, dtype=torch.long)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def convert_mask_to_class_ids(self, mask):\n",
    "        class_mask = np.zeros(mask.shape[:2], dtype=np.uint8)\n",
    "        for class_id, (_, color) in self.class_id_to_name.items():\n",
    "            color_mask = np.all(mask == color, axis=-1)\n",
    "            class_mask[color_mask] = self.class_id_map[class_id]\n",
    "        return class_mask\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Segmentation Training Function\n",
    "# ----------------------------\n",
    "def train_segmentation(image_dir, mask_dir, epochs=1, batch_size=8, save_path=\"deeplabv3_model.pth\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset = SegmentationDataset(image_dir, mask_dir, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "    model.classifier[4] = nn.Conv2d(256, len(dataset.class_names), kernel_size=1)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, masks in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)['out']\n",
    "            loss = criterion(output, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"✅ Model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee96142",
   "metadata": {},
   "source": [
    "### Finding best model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb322ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_pt(base_dir='runs/'):\n",
    "    best_paths = list(Path(base_dir).rglob('best.pt'))\n",
    "    if not best_paths:\n",
    "        raise FileNotFoundError(\"No 'best.pt' file found in the 'runs/' directory.\")\n",
    "    \n",
    "    # Optionally, sort by latest modified time\n",
    "    best_paths.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    print(f\"✅ Found best.pt at: {best_paths[0]}\")\n",
    "    return str(best_paths[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69e3a88",
   "metadata": {},
   "source": [
    "### Prediciton on val images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ea00c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.segmentation as segmentation\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------- FUNCTIONS -------- #\n",
    "\n",
    "def load_deeplabv3_model(weight_path: str):\n",
    "    model = segmentation.deeplabv3_resnet101(pretrained=False)\n",
    "    model.classifier[4] = nn.Conv2d(256, 11, kernel_size=(1, 1))  # 11 classes\n",
    "    state_dict = torch.load(weight_path, map_location=device)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.to(device).eval()\n",
    "    return model\n",
    "\n",
    "def load_yolo_model(weight_path: str):\n",
    "    return YOLO(weight_path)\n",
    "\n",
    "def run_segmentation(model, image: Image.Image):\n",
    "    transform = transforms.ToTensor()\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)['out']\n",
    "        seg_mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n",
    "    return seg_mask\n",
    "\n",
    "def decode_segmentation_mask(seg_mask, CLASS_COLORS):\n",
    "    seg_rgb = np.zeros((seg_mask.shape[0], seg_mask.shape[1], 3), dtype=np.uint8)\n",
    "    for cls_id, color in CLASS_COLORS.items():\n",
    "        seg_rgb[seg_mask == cls_id] = (np.array(color) * 255).astype(np.uint8)\n",
    "    return Image.fromarray(seg_rgb)\n",
    "\n",
    "def run_yolo(model, image: Image.Image):\n",
    "    temp_path = \"temp_yolo.jpg\"\n",
    "    image.save(temp_path)\n",
    "    result = model.predict(source=temp_path, conf=0.3, save=False, verbose=False)[0]\n",
    "    os.remove(temp_path)\n",
    "    return result\n",
    "\n",
    "def draw_yolo_boxes(draw: ImageDraw.ImageDraw, result):\n",
    "    boxes = result.boxes\n",
    "    labels = result.names\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes.xyxy[i].cpu().numpy()\n",
    "        cls_id = int(boxes.cls[i])\n",
    "        conf = float(boxes.conf[i])\n",
    "        label = f\"{labels[cls_id]} {conf:.2f}\"\n",
    "        draw.rectangle(box.tolist(), outline=\"red\", width=2)\n",
    "        draw.text((box[0], box[1]), label, fill=\"red\")\n",
    "\n",
    "def generate_segmentation_legend(seg_mask, CLASS_COLORS, CLASS_NAMES):\n",
    "    legend_img = Image.new(\"RGB\", (256, 256), (0, 0, 0))\n",
    "    draw = ImageDraw.Draw(legend_img)\n",
    "    unique_classes = np.unique(seg_mask)\n",
    "    for idx, cls_id in enumerate(unique_classes):\n",
    "        color = tuple((np.array(CLASS_COLORS.get(cls_id, (1, 1, 1))) * 255).astype(np.uint8))\n",
    "        name = CLASS_NAMES.get(cls_id, \"Unknown\")\n",
    "        draw.rectangle([10, 10 + idx * 25, 30, 30 + idx * 25], fill=color)\n",
    "        draw.text((40, 10 + idx * 25), f\"{cls_id}: {name}\", fill=(255, 255, 255))\n",
    "    return legend_img\n",
    "\n",
    "def generate_yolo_legend(result):\n",
    "    legend_img = Image.new(\"RGB\", (256, 256), (0, 0, 0))\n",
    "    draw = ImageDraw.Draw(legend_img)\n",
    "    class_ids = result.boxes.cls.cpu().numpy().astype(int)\n",
    "    unique_ids = np.unique(class_ids)\n",
    "    for idx, cls_id in enumerate(unique_ids):\n",
    "        name = result.names[cls_id]\n",
    "        draw.rectangle([10, 10 + idx * 25, 30, 30 + idx * 25], fill=(255, 0, 0))\n",
    "        draw.text((40, 10 + idx * 25), f\"{cls_id}: {name}\", fill=(255, 255, 255))\n",
    "    return legend_img\n",
    "\n",
    "def predict_and_visualize_separately(image_path, deeplab_model, yolo_model, CLASS_COLORS, CLASS_NAMES):\n",
    "    original_image = Image.open(image_path).convert(\"RGB\")\n",
    "    resized_image = original_image.resize((256, 256))\n",
    "\n",
    "    # === Segmentation ===\n",
    "    seg_mask = run_segmentation(deeplab_model, resized_image)\n",
    "    seg_overlay = decode_segmentation_mask(seg_mask, CLASS_COLORS)\n",
    "    blended_seg = Image.blend(resized_image, seg_overlay, alpha=0.7)\n",
    "    seg_legend_img = generate_segmentation_legend(seg_mask, CLASS_COLORS, CLASS_NAMES)\n",
    "\n",
    "    # === YOLO Detection ===\n",
    "    yolo_result = run_yolo(yolo_model, resized_image)\n",
    "    yolo_image = resized_image.copy()\n",
    "    draw = ImageDraw.Draw(yolo_image)\n",
    "    draw_yolo_boxes(draw, yolo_result)\n",
    "    yolo_legend_img = generate_yolo_legend(yolo_result)\n",
    "\n",
    "    # === Plot all results ===\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    axs[0, 0].imshow(resized_image)\n",
    "    axs[0, 0].set_title(\"Original Image\")\n",
    "    axs[0, 0].axis(\"off\")\n",
    "\n",
    "    axs[0, 1].imshow(blended_seg)\n",
    "    axs[0, 1].set_title(\"Segmentation Mask Overlay (on Image)\")\n",
    "    axs[0, 1].axis(\"off\")\n",
    "\n",
    "    axs[1, 0].imshow(yolo_image)\n",
    "    axs[1, 0].set_title(\"YOLO Detection\")\n",
    "    axs[1, 0].axis(\"off\")\n",
    "\n",
    "    axs[1, 1].imshow(seg_legend_img)\n",
    "    axs[1, 1].set_title(\"Segmentation Class Legend\")\n",
    "    axs[1, 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0dc1d0",
   "metadata": {},
   "source": [
    "### Predcitions on videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ca55446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_base = './video_outputs'\n",
    "\n",
    "\n",
    "# ================== OUTPUT FOLDERS ==================\n",
    "img_dir = os.path.join(output_base, 'images')\n",
    "seg_dir = os.path.join(output_base, 'segmentation_mask')\n",
    "label_dir = os.path.join(output_base, 'labels')\n",
    "video_output_path = os.path.join(output_base, 'output_video.mp4')\n",
    "\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "os.makedirs(seg_dir, exist_ok=True)\n",
    "os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "# ================== LOAD MODELS ==================\n",
    "def load_models(yolo_weights_path, deeplabv3_weights_path, num_classes):\n",
    "    yolo_model = YOLO(yolo_weights_path)\n",
    "    deeplab = models.segmentation.deeplabv3_resnet101(pretrained=False, num_classes=num_classes, aux_loss=True)\n",
    "    state_dict = torch.load(deeplabv3_weights_path, map_location=device)\n",
    "    for k in list(state_dict.keys()):\n",
    "        if k.startswith(\"aux_classifier.\"):\n",
    "            del state_dict[k]\n",
    "    deeplab.load_state_dict(state_dict, strict=False)\n",
    "    deeplab.to(device)\n",
    "    deeplab.eval()\n",
    "    return yolo_model, deeplab\n",
    "\n",
    "# ================== IMAGE TRANSFORM ==================\n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# ================== COLOR MASK ==================\n",
    "def get_colored_mask(mask, class_id_to_name):\n",
    "    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for cls_id, (_, color) in class_id_to_name.items():\n",
    "        color_mask[mask == cls_id] = color\n",
    "    return color_mask\n",
    "\n",
    "# ================== PROCESS FRAME ==================\n",
    "def process_frame(frame, deeplab, yolo_model, transform, w, h, yolo_class_names, class_id_to_name):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(frame_rgb)\n",
    "\n",
    "    # === Segmentation ===\n",
    "    input_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = deeplab(input_tensor)['out']\n",
    "        pred_mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n",
    "\n",
    "    mask_resized = cv2.resize(pred_mask.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    mask_colored = get_colored_mask(mask_resized, class_id_to_name)\n",
    "\n",
    "    # === YOLO Detection ===\n",
    "    results = yolo_model(frame)\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    class_ids = results[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "    for box, cls_id in zip(boxes, class_ids):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        name = yolo_class_names[int(cls_id)] if int(cls_id) < len(yolo_class_names) else f\"Class {int(cls_id)}\"\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, name, (x1, max(y1 - 10, 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)\n",
    "\n",
    "    return frame_rgb, mask_colored, boxes, class_ids\n",
    "\n",
    "# ================== SAVE OUTPUTS ==================\n",
    "def save_outputs(frame_rgb, mask_colored, boxes, class_ids, frame_count, w, h, img_dir, seg_dir, label_dir, yolo_class_names):\n",
    "    # Save original frame\n",
    "    out_img = os.path.join(img_dir, f'frame_{frame_count:04d}.jpg')\n",
    "    out_mask = os.path.join(seg_dir, f'mask_{frame_count:04d}.png')\n",
    "    out_label = os.path.join(label_dir, f'frame_{frame_count:04d}.txt')\n",
    "\n",
    "    # Save original frame\n",
    "    cv2.imwrite(out_img, frame_rgb[..., ::-1])  # RGB -> BGR\n",
    "\n",
    "    # Save colored segmentation mask\n",
    "    cv2.imwrite(out_mask, mask_colored)\n",
    "\n",
    "    # Save YOLO labels\n",
    "    with open(out_label, 'w') as f:\n",
    "        for box, cls_id in zip(boxes, class_ids):\n",
    "            x1, y1, x2, y2 = box\n",
    "            w_box = x2 - x1\n",
    "            h_box = y2 - y1\n",
    "            cx = x1 + w_box / 2\n",
    "            cy = y1 + h_box / 2\n",
    "            cx /= w\n",
    "            cy /= h\n",
    "            w_box /= w\n",
    "            h_box /= h\n",
    "            f.write(f\"{int(cls_id)} {cx:.6f} {cy:.6f} {w_box:.6f} {h_box:.6f}  # {yolo_class_names[int(cls_id)]}\\n\")\n",
    "\n",
    "# ================== VIDEO SETUP ==================\n",
    "def setup_video_capture(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    return cap, total_frames, fps, w, h, fourcc\n",
    "\n",
    "# ================== MAIN VIDEO PROCESSING ==================\n",
    "def process_video(video_path, yolo_weights_path, deeplabv3_weights_path, yolo_class_names, class_id_to_name, num_classes, max_frames=None, ):\n",
    "    yolo_model, deeplab = load_models(yolo_weights_path, deeplabv3_weights_path, num_classes)\n",
    "    transform = get_transform()\n",
    "    \n",
    "    cap, total_frames, fps, w, h, fourcc = setup_video_capture(video_path)\n",
    "    video_writer = cv2.VideoWriter(video_output_path, fourcc, fps, (w, h))\n",
    "    \n",
    "    frame_count = 0\n",
    "    pbar = tqdm(total=max_frames if max_frames else total_frames, desc=\"Processing\", leave=False)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or (max_frames is not None and frame_count >= max_frames):\n",
    "            break\n",
    "\n",
    "        frame_rgb, mask_colored, boxes, class_ids = process_frame(frame, deeplab, yolo_model, transform, w, h, yolo_class_names, class_id_to_name)\n",
    "        save_outputs(frame_rgb, mask_colored, boxes, class_ids, frame_count, w, h, img_dir, seg_dir, label_dir, yolo_class_names)\n",
    "\n",
    "        blended = cv2.addWeighted(frame, 1.0, mask_colored, 0.5, 0)\n",
    "        video_writer.write(blended)\n",
    "\n",
    "        frame_count += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    video_writer.release()\n",
    "    pbar.close()\n",
    "\n",
    "    print(f\"\\nDone! Processed {frame_count} frames.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6299d8c",
   "metadata": {},
   "source": [
    "### Print Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d4c48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b357e76",
   "metadata": {},
   "source": [
    "### Retrain and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752af339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fea0de53",
   "metadata": {},
   "source": [
    "### Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a048bbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found best.pt at: runs\\train\\yolov8-uavdt\\weights\\best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eea6ed075a41849851903622443e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 38.1ms\n",
      "Speed: 2.0ms preprocess, 38.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 37.4ms\n",
      "Speed: 1.3ms preprocess, 37.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 37.3ms\n",
      "Speed: 1.6ms preprocess, 37.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 36.0ms\n",
      "Speed: 1.7ms preprocess, 36.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 39.0ms\n",
      "Speed: 1.3ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 37.6ms\n",
      "Speed: 1.5ms preprocess, 37.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 37.3ms\n",
      "Speed: 1.7ms preprocess, 37.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 39.5ms\n",
      "Speed: 1.4ms preprocess, 39.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 42.3ms\n",
      "Speed: 1.9ms preprocess, 42.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 37.0ms\n",
      "Speed: 1.2ms preprocess, 37.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 43.6ms\n",
      "Speed: 2.2ms preprocess, 43.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 37.4ms\n",
      "Speed: 1.8ms preprocess, 37.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 37.4ms\n",
      "Speed: 1.3ms preprocess, 37.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 35.6ms\n",
      "Speed: 1.5ms preprocess, 35.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 35.1ms\n",
      "Speed: 1.6ms preprocess, 35.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 37.8ms\n",
      "Speed: 1.8ms preprocess, 37.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 44.2ms\n",
      "Speed: 2.1ms preprocess, 44.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 37.5ms\n",
      "Speed: 1.4ms preprocess, 37.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 39.3ms\n",
      "Speed: 1.9ms preprocess, 39.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 43.4ms\n",
      "Speed: 1.4ms preprocess, 43.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 36.1ms\n",
      "Speed: 1.4ms preprocess, 36.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 37.0ms\n",
      "Speed: 1.8ms preprocess, 37.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 33.7ms\n",
      "Speed: 1.3ms preprocess, 33.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 39.7ms\n",
      "Speed: 1.7ms preprocess, 39.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 41.3ms\n",
      "Speed: 2.0ms preprocess, 41.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 41.9ms\n",
      "Speed: 1.6ms preprocess, 41.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 47.3ms\n",
      "Speed: 2.2ms preprocess, 47.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 39.9ms\n",
      "Speed: 1.9ms preprocess, 39.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 39.3ms\n",
      "Speed: 1.7ms preprocess, 39.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 41.9ms\n",
      "Speed: 1.8ms preprocess, 41.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 40.8ms\n",
      "Speed: 1.8ms preprocess, 40.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 40.9ms\n",
      "Speed: 1.9ms preprocess, 40.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 39.4ms\n",
      "Speed: 1.8ms preprocess, 39.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 36.7ms\n",
      "Speed: 1.7ms preprocess, 36.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 41.0ms\n",
      "Speed: 1.8ms preprocess, 41.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 40.8ms\n",
      "Speed: 1.7ms preprocess, 40.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.3ms\n",
      "Speed: 1.7ms preprocess, 38.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 37.3ms\n",
      "Speed: 1.7ms preprocess, 37.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 60.3ms\n",
      "Speed: 1.7ms preprocess, 60.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 40.1ms\n",
      "Speed: 1.8ms preprocess, 40.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 41.4ms\n",
      "Speed: 2.0ms preprocess, 41.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 39.2ms\n",
      "Speed: 1.9ms preprocess, 39.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 39.2ms\n",
      "Speed: 1.5ms preprocess, 39.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 40.6ms\n",
      "Speed: 1.8ms preprocess, 40.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 41.2ms\n",
      "Speed: 1.9ms preprocess, 41.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 41.5ms\n",
      "Speed: 1.8ms preprocess, 41.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 39.7ms\n",
      "Speed: 1.7ms preprocess, 39.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 37.9ms\n",
      "Speed: 1.7ms preprocess, 37.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 37.7ms\n",
      "Speed: 1.6ms preprocess, 37.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 40.5ms\n",
      "Speed: 1.9ms preprocess, 40.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.9ms\n",
      "Speed: 1.8ms preprocess, 38.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 40.9ms\n",
      "Speed: 1.9ms preprocess, 40.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 51.7ms\n",
      "Speed: 2.0ms preprocess, 51.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 37.8ms\n",
      "Speed: 1.6ms preprocess, 37.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.5ms\n",
      "Speed: 1.8ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 38.3ms\n",
      "Speed: 1.7ms preprocess, 38.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 41.2ms\n",
      "Speed: 1.8ms preprocess, 41.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 41.0ms\n",
      "Speed: 1.5ms preprocess, 41.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 38.7ms\n",
      "Speed: 1.8ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 41.5ms\n",
      "Speed: 2.0ms preprocess, 41.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 41.4ms\n",
      "Speed: 1.7ms preprocess, 41.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 40.3ms\n",
      "Speed: 1.8ms preprocess, 40.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 47.4ms\n",
      "Speed: 1.9ms preprocess, 47.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 40.6ms\n",
      "Speed: 1.3ms preprocess, 40.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 46.9ms\n",
      "Speed: 2.0ms preprocess, 46.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 44.8ms\n",
      "Speed: 2.1ms preprocess, 44.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 43.4ms\n",
      "Speed: 1.7ms preprocess, 43.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 44.0ms\n",
      "Speed: 1.9ms preprocess, 44.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 58.1ms\n",
      "Speed: 1.9ms preprocess, 58.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 43.8ms\n",
      "Speed: 1.9ms preprocess, 43.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 42.6ms\n",
      "Speed: 1.7ms preprocess, 42.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 45.9ms\n",
      "Speed: 1.8ms preprocess, 45.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 41.8ms\n",
      "Speed: 1.8ms preprocess, 41.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 64.2ms\n",
      "Speed: 2.3ms preprocess, 64.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 40.9ms\n",
      "Speed: 2.1ms preprocess, 40.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 37.0ms\n",
      "Speed: 1.9ms preprocess, 37.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 38.0ms\n",
      "Speed: 2.3ms preprocess, 38.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 34.8ms\n",
      "Speed: 1.6ms preprocess, 34.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 40.9ms\n",
      "Speed: 1.9ms preprocess, 40.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 35.7ms\n",
      "Speed: 1.7ms preprocess, 35.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 30.2ms\n",
      "Speed: 1.4ms preprocess, 30.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 34.7ms\n",
      "Speed: 1.4ms preprocess, 34.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 33.5ms\n",
      "Speed: 1.6ms preprocess, 33.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 31.7ms\n",
      "Speed: 1.2ms preprocess, 31.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 37.5ms\n",
      "Speed: 1.8ms preprocess, 37.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 38.6ms\n",
      "Speed: 1.6ms preprocess, 38.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 37.8ms\n",
      "Speed: 1.9ms preprocess, 37.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 32.9ms\n",
      "Speed: 1.4ms preprocess, 32.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 36.5ms\n",
      "Speed: 1.3ms preprocess, 36.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 38.6ms\n",
      "Speed: 1.6ms preprocess, 38.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 35.3ms\n",
      "Speed: 1.9ms preprocess, 35.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 37.1ms\n",
      "Speed: 1.3ms preprocess, 37.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 36.9ms\n",
      "Speed: 1.8ms preprocess, 36.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 36.2ms\n",
      "Speed: 1.3ms preprocess, 36.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 38.1ms\n",
      "Speed: 1.9ms preprocess, 38.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 36.8ms\n",
      "Speed: 1.6ms preprocess, 36.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 37.1ms\n",
      "Speed: 1.6ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 38.1ms\n",
      "Speed: 1.4ms preprocess, 38.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 38.3ms\n",
      "Speed: 1.8ms preprocess, 38.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 38.4ms\n",
      "Speed: 1.8ms preprocess, 38.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 39.2ms\n",
      "Speed: 1.6ms preprocess, 39.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 37.3ms\n",
      "Speed: 1.8ms preprocess, 37.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 41.1ms\n",
      "Speed: 1.5ms preprocess, 41.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 37.4ms\n",
      "Speed: 2.0ms preprocess, 37.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 46.4ms\n",
      "Speed: 2.7ms preprocess, 46.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 38.8ms\n",
      "Speed: 2.0ms preprocess, 38.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 35.6ms\n",
      "Speed: 1.5ms preprocess, 35.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 37.2ms\n",
      "Speed: 1.4ms preprocess, 37.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 39.3ms\n",
      "Speed: 1.6ms preprocess, 39.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 40.4ms\n",
      "Speed: 2.0ms preprocess, 40.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 46.2ms\n",
      "Speed: 2.1ms preprocess, 46.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 37.8ms\n",
      "Speed: 1.3ms preprocess, 37.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 40.3ms\n",
      "Speed: 1.8ms preprocess, 40.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 48.9ms\n",
      "Speed: 2.0ms preprocess, 48.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 44.8ms\n",
      "Speed: 2.0ms preprocess, 44.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 42.3ms\n",
      "Speed: 1.7ms preprocess, 42.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 42.3ms\n",
      "Speed: 1.8ms preprocess, 42.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 40.2ms\n",
      "Speed: 1.7ms preprocess, 40.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 40.3ms\n",
      "Speed: 1.9ms preprocess, 40.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 39.5ms\n",
      "Speed: 1.8ms preprocess, 39.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 39.8ms\n",
      "Speed: 1.8ms preprocess, 39.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 40.0ms\n",
      "Speed: 1.9ms preprocess, 40.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 46.3ms\n",
      "Speed: 2.4ms preprocess, 46.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 42.7ms\n",
      "Speed: 2.1ms preprocess, 42.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 42.1ms\n",
      "Speed: 1.8ms preprocess, 42.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 40.8ms\n",
      "Speed: 1.9ms preprocess, 40.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 41.2ms\n",
      "Speed: 1.8ms preprocess, 41.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 39.0ms\n",
      "Speed: 1.5ms preprocess, 39.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 44.5ms\n",
      "Speed: 1.8ms preprocess, 44.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 39.3ms\n",
      "Speed: 1.8ms preprocess, 39.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 40.7ms\n",
      "Speed: 2.0ms preprocess, 40.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 40.8ms\n",
      "Speed: 1.8ms preprocess, 40.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 40.2ms\n",
      "Speed: 1.5ms preprocess, 40.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 41.2ms\n",
      "Speed: 1.8ms preprocess, 41.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 43.3ms\n",
      "Speed: 1.8ms preprocess, 43.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 37.8ms\n",
      "Speed: 1.7ms preprocess, 37.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 41.5ms\n",
      "Speed: 1.8ms preprocess, 41.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 41.8ms\n",
      "Speed: 1.8ms preprocess, 41.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 42.9ms\n",
      "Speed: 1.9ms preprocess, 42.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 41.8ms\n",
      "Speed: 1.6ms preprocess, 41.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 40.4ms\n",
      "Speed: 1.6ms preprocess, 40.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 60.8ms\n",
      "Speed: 1.9ms preprocess, 60.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 38.9ms\n",
      "Speed: 1.7ms preprocess, 38.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 40.5ms\n",
      "Speed: 1.7ms preprocess, 40.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 39.9ms\n",
      "Speed: 1.7ms preprocess, 39.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 41.4ms\n",
      "Speed: 1.9ms preprocess, 41.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 39.3ms\n",
      "Speed: 1.9ms preprocess, 39.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 40.2ms\n",
      "Speed: 2.0ms preprocess, 40.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 39.1ms\n",
      "Speed: 1.8ms preprocess, 39.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 39.9ms\n",
      "Speed: 1.7ms preprocess, 39.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 43.5ms\n",
      "Speed: 1.9ms preprocess, 43.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 40.2ms\n",
      "Speed: 1.8ms preprocess, 40.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 39.7ms\n",
      "Speed: 1.6ms preprocess, 39.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 38.8ms\n",
      "Speed: 1.8ms preprocess, 38.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 38.7ms\n",
      "Speed: 1.5ms preprocess, 38.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 39.7ms\n",
      "Speed: 1.7ms preprocess, 39.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 41.1ms\n",
      "Speed: 1.8ms preprocess, 41.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 42.5ms\n",
      "Speed: 2.0ms preprocess, 42.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 41.5ms\n",
      "Speed: 1.7ms preprocess, 41.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 49.9ms\n",
      "Speed: 2.3ms preprocess, 49.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 38.3ms\n",
      "Speed: 1.6ms preprocess, 38.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 37.9ms\n",
      "Speed: 1.6ms preprocess, 37.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 40.1ms\n",
      "Speed: 1.8ms preprocess, 40.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 39.0ms\n",
      "Speed: 1.5ms preprocess, 39.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 38.7ms\n",
      "Speed: 1.6ms preprocess, 38.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 39.9ms\n",
      "Speed: 2.0ms preprocess, 39.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 40.6ms\n",
      "Speed: 1.8ms preprocess, 40.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 66.3ms\n",
      "Speed: 1.9ms preprocess, 66.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 41.8ms\n",
      "Speed: 1.8ms preprocess, 41.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 39.8ms\n",
      "Speed: 1.9ms preprocess, 39.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "Done! Processed 176 frames.\n"
     ]
    }
   ],
   "source": [
    "# # Class ID to name mapping with RGB values\n",
    "class_id_to_name = {\n",
    "    0: ('road', [28, 42, 168]),\n",
    "    1: ('pool', [0, 50, 89]),\n",
    "    2: ('vegetation', [107, 142, 35]),\n",
    "    3: ('roof', [70, 70, 70]),\n",
    "    4: ('wall', [102, 102, 156]),\n",
    "    5: ('window', [254, 228, 12]),\n",
    "    6: ('person', [255, 22, 96]),\n",
    "    7: ('dog', [102, 51, 0]),\n",
    "    8: ('car', [9, 143, 150]),\n",
    "    9: ('bicycle', [119, 11, 32]),\n",
    "    10: ('tree', [51, 51, 0]),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Downlaod the datasets\n",
    "\n",
    "# gdrive_url = \"https://drive.google.com/file/d/1UppumYqYOi-kto6BWPfFxwJK2Eph46oY/view?usp=sharing\"\n",
    "# download_and_extract_from_gdrive(gdrive_url, extract_to=\"datasets\")\n",
    "\n",
    "# # Path to the dataset\n",
    "# dataset_path = \"./datasets/semantic_drone_dataset/training_set\"\n",
    "# output_dir = \"./datasets/new_dataset\"\n",
    "\n",
    "# convert_fulldataset(dataset_path, output_dir, class_id_to_name)\n",
    "\n",
    "\n",
    "# # Path to the new dataset\n",
    "# dataset_path = \"./datasets/new_dataset\"\n",
    "# image_dir = os.path.join(dataset_path, \"images\")\n",
    "# segmentation_mask_dir = os.path.join(dataset_path, \"segmentation_mask\")\n",
    "# annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# # Loop through some example image IDs (you can specify more or iterate through the dataset)\n",
    "# image_ids = [img.split('.')[0] for img in os.listdir(image_dir) if img.endswith(\".jpg\")]\n",
    "# for image_id in image_ids[:5]:  # Visualizing first 5 images as an example\n",
    "#     visualize_data(image_id, class_id_to_name)\n",
    "\n",
    "\n",
    "# # Define paths for already converted datasets\n",
    "# input_image_dir = \"./datasets/new_dataset/images\"\n",
    "# input_mask_dir = \"./datasets/new_dataset/segmentation_mask\"\n",
    "# input_annotation_dir = \"./datasets/new_dataset/labels\"\n",
    "\n",
    "# # New folder for final dataset\n",
    "# output_dir = \"./datasets/suitable-dataset\"  \n",
    "\n",
    "# # Create directories for the train and val sets\n",
    "# train_image_dir = os.path.join(output_dir, 'train/images')\n",
    "# val_image_dir = os.path.join(output_dir, 'val/images')\n",
    "# train_mask_dir = os.path.join(output_dir, 'train/segmentation_mask')\n",
    "# val_mask_dir = os.path.join(output_dir, 'val/segmentation_mask')\n",
    "# train_annotation_dir = os.path.join(output_dir, 'train/labels')\n",
    "# val_annotation_dir = os.path.join(output_dir, 'val/labels')\n",
    "\n",
    "# os.makedirs(train_image_dir, exist_ok=True)\n",
    "# os.makedirs(val_image_dir, exist_ok=True)\n",
    "# os.makedirs(train_mask_dir, exist_ok=True)\n",
    "# os.makedirs(val_mask_dir, exist_ok=True)\n",
    "# os.makedirs(train_annotation_dir, exist_ok=True)\n",
    "# os.makedirs(val_annotation_dir, exist_ok=True)\n",
    "\n",
    "# # List of all image files in the dataset\n",
    "# image_ids = [img.split('.')[0] for img in os.listdir(input_image_dir) if img.endswith('.jpg')]\n",
    "\n",
    "# # Split the dataset into train and val using a 80-20 split\n",
    "# train_ids, val_ids = train_test_split(image_ids, test_size=0.2, random_state=42)\n",
    "# # Move training files\n",
    "# move_files(train_ids, input_image_dir, input_mask_dir, input_annotation_dir,\n",
    "#            train_image_dir, train_mask_dir, train_annotation_dir)\n",
    "\n",
    "# # Move validation files\n",
    "# move_files(val_ids, input_image_dir, input_mask_dir, input_annotation_dir,\n",
    "#            val_image_dir, val_mask_dir, val_annotation_dir)\n",
    "\n",
    "# print(\"[+] Dataset split into train and val, and files moved successfully.\")\n",
    "\n",
    "# # Train YOLOv8\n",
    "# train_yolo(data_yaml=\"uavdt.yaml\")\n",
    "\n",
    "# # Train Segmentation\n",
    "# train_segmentation(\n",
    "#     image_dir='./datasets/suitable-dataset/train/images',\n",
    "#     mask_dir='./datasets/suitable-dataset/train/segmentation_mask'\n",
    "# )\n",
    "\n",
    "# print(\"[+] Training is Done\")\n",
    "\n",
    "# Load the model\n",
    "best_pt_path = find_best_pt()\n",
    "\n",
    "\n",
    "# CLASS_COLORS = {k: tuple(np.array(v[1]) / 255.0) for k, v in class_id_to_name.items()}\n",
    "# CLASS_NAMES = {k: v[0] for k, v in class_id_to_name.items()}\n",
    "\n",
    "deeplab_path = 'deeplabv3_model.pth'\n",
    "yolo_path = best_pt_path  # <- Update with actual model path\n",
    "\n",
    "# deeplab_model = load_deeplabv3_model(deeplab_path)\n",
    "# yolo_model = load_yolo_model(yolo_path)\n",
    "\n",
    "# test_dir = \"./datasets/suitable-dataset/val/images\"\n",
    "# test_images = sorted(os.listdir(test_dir))[:10]\n",
    "\n",
    "# for img_file in test_images:\n",
    "#     predict_and_visualize_separately(os.path.join(test_dir, img_file), deeplab_model, yolo_model, CLASS_COLORS, CLASS_NAMES)\n",
    "\n",
    "num_classes = len(class_id_to_name)\n",
    "yolo_class_names = [class_id_to_name[i][0] for i in sorted(class_id_to_name)]\n",
    "\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "video_path = './video.mp4'\n",
    "max_frames = None  # Set to None for full video\n",
    "\n",
    "# ================== RUN ==================\n",
    "process_video(video_path, yolo_path, deeplab_path, yolo_class_names, class_id_to_name, num_classes, max_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e3842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
