{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc3ee2d",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62209f8",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f8ff18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install opencv-python\n",
    "# !pip install pillow\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch torchvision\n",
    "# !pip install ultralytics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "09e437de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch torchvision torchaudio\n",
    "# !pip cache purge  # clean out pip's install cache\n",
    "# !pip install torch torchvision torchaudio --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8431144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core packages\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "# Math and array handling\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Image and visualization\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep Learning Frameworks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.models.segmentation as segmentation\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Object Detection and Segmentation\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Automatically use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import gdown\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275651f4",
   "metadata": {},
   "source": [
    "### Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "42512998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_from_gdrive(gdrive_url, extract_to=\"extracted\"):\n",
    "    # Convert shared drive URL to direct download URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create output folder\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[INFO] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[INFO] Extracting ZIP...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Optionally, remove the ZIP file after extraction\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[DONE] Extracted files to: {extract_to}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1e94f",
   "metadata": {},
   "source": [
    "### Convert downloaded Dataset into yolo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7dbcea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Parse polygon and convert to YOLO bbox\n",
    "# ----------------------------\n",
    "def parse_yolo_style_bbox_from_xml(xml_path, class_id_to_name):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "        if class_name in [value[0] for value in class_id_to_name.values()]:\n",
    "            polygon = obj.find('polygon')\n",
    "            if polygon is not None:\n",
    "                points = polygon.findall('pt')\n",
    "                coords = [(float(pt.find('x').text), float(pt.find('y').text)) for pt in points]\n",
    "                x_min = min(coord[0] for coord in coords)\n",
    "                y_min = min(coord[1] for coord in coords)\n",
    "                x_max = max(coord[0] for coord in coords)\n",
    "                y_max = max(coord[1] for coord in coords)\n",
    "                bboxes.append(((x_min, y_min), (x_max, y_max), class_name))\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Save YOLO-format txt\n",
    "# ----------------------------\n",
    "def save_yolo_format(image_id, bboxes, image_width, image_height, output_path, class_id_to_name):\n",
    "    with open(output_path, 'w') as f:\n",
    "        for (x_min, y_min), (x_max, y_max), class_name in bboxes:\n",
    "            class_id = next(cid for cid, (name, _) in class_id_to_name.items() if name == class_name)\n",
    "            x_center = (x_min + x_max) / 2 / image_width\n",
    "            y_center = (y_min + y_max) / 2 / image_height\n",
    "            width = (x_max - x_min) / image_width\n",
    "            height = (y_max - y_min) / image_height\n",
    "            f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Convert dataset (YOLO only)\n",
    "# ----------------------------\n",
    "def convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name):\n",
    "    image_ids = [img.split('.')[0] for img in os.listdir(f\"{dataset_path}/images\") if img.endswith(\".jpg\")]\n",
    "\n",
    "    os.makedirs(f\"{output_dir}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/labels\", exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(image_ids, desc=\"Converting to YOLO\"):\n",
    "        img_path = f\"{dataset_path}/images/{image_id}.jpg\"\n",
    "        bbox_xml_path = f\"{dataset_path}/gt/bounding_box/label_me_xml/{image_id}.xml\"\n",
    "        semantic_xml_path = f\"{dataset_path}/gt/semantic/label_me_xml/{image_id}.xml\"\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"[WARNING] Image not found: {img_path}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            bboxes1 = parse_yolo_style_bbox_from_xml(bbox_xml_path, class_id_to_name)\n",
    "            bboxes2 = parse_yolo_style_bbox_from_xml(semantic_xml_path, class_id_to_name)\n",
    "            all_bboxes = bboxes1 + bboxes2\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Skipping image {image_id} due to parse error: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            image_np = np.array(image)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not load image {image_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save image\n",
    "        image.save(f\"{output_dir}/images/{image_id}.jpg\")\n",
    "\n",
    "        # Save YOLO labels\n",
    "        yolo_annotation_path = f\"{output_dir}/labels/{image_id}.txt\"\n",
    "        save_yolo_format(image_id, all_bboxes, image_np.shape[1], image_np.shape[0], yolo_annotation_path, class_id_to_name)\n",
    "\n",
    "    print(\"✅ YOLO-format annotation conversion complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4683246",
   "metadata": {},
   "source": [
    "### Visualizing images with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8af9cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load YOLO bounding box annotations\n",
    "def load_yolo_annotations(anno_file):\n",
    "    with open(anno_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    boxes = []\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "        boxes.append((class_id, x_center, y_center, width, height))\n",
    "    return boxes\n",
    "\n",
    "# Function to visualize image and bounding boxes\n",
    "def visualize_data(image_id, class_id_to_name, image_dir, annotations_dir):\n",
    "    img_path = os.path.join(image_dir, f\"{image_id}.jpg\")\n",
    "    annotation_path = os.path.join(annotations_dir, f\"{image_id}.txt\")\n",
    "\n",
    "    if not os.path.exists(img_path) or not os.path.exists(annotation_path):\n",
    "        print(f\"[WARNING] Missing files for {image_id}, skipping...\")\n",
    "        return\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(img_path)\n",
    "    image = np.array(image)\n",
    "\n",
    "    # Load annotations\n",
    "    boxes = load_yolo_annotations(annotation_path)\n",
    "\n",
    "    # Draw boxes\n",
    "    image_with_boxes = image.copy()\n",
    "    for box in boxes:\n",
    "        class_id, x_center, y_center, width, height = box\n",
    "        class_name, color = class_id_to_name[int(class_id)]\n",
    "\n",
    "        x_min = int((x_center - width / 2) * image.shape[1])\n",
    "        y_min = int((y_center - height / 2) * image.shape[0])\n",
    "        x_max = int((x_center + width / 2) * image.shape[1])\n",
    "        y_max = int((y_center + height / 2) * image.shape[0])\n",
    "\n",
    "        # Change the line width of the rectangle (increase from 4 to any desired thickness)\n",
    "        rectangle_thickness = 15  # Adjust line thickness here\n",
    "        cv2.rectangle(image_with_boxes, (x_min, y_min), (x_max, y_max), color, rectangle_thickness)\n",
    "\n",
    "        # Change font size and color of the class name (increase font scale and adjust color)\n",
    "        font_scale = 5  # Larger font size\n",
    "        font_thickness = 5  # Thickness of the text\n",
    "        text_color = (255, 0, 0)  # Set text color to white (change as desired)\n",
    "        cv2.putText(image_with_boxes, class_name, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, font_thickness)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image_with_boxes)\n",
    "    plt.title(f\"Image ID: {image_id}\")\n",
    "    \n",
    "    # Add class legend\n",
    "    legend_handles = []\n",
    "    for class_id, (class_name, class_rgb) in class_id_to_name.items():\n",
    "        legend_handles.append(mpatches.Patch(color=np.array(class_rgb)/255.0, label=f\"{class_name} ({class_id})\"))\n",
    "    plt.legend(handles=legend_handles, loc='upper right', fontsize=10)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ab554",
   "metadata": {},
   "source": [
    "### Convert into train set and val set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d7373103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(file_list, \n",
    "               source_image_dir, \n",
    "               source_annotation_dir,\n",
    "               target_image_dir, \n",
    "               target_annotation_dir):\n",
    "    \n",
    "    os.makedirs(target_image_dir, exist_ok=True)\n",
    "    os.makedirs(target_annotation_dir, exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(file_list, desc=f\"Moving to {os.path.basename(os.path.dirname(target_image_dir))}\"):\n",
    "        image_path = os.path.join(source_image_dir, f\"{image_id}.jpg\")\n",
    "        annotation_path = os.path.join(source_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        target_image_path = os.path.join(target_image_dir, f\"{image_id}.jpg\")\n",
    "        target_annotation_path = os.path.join(target_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            shutil.copy(image_path, target_image_path)\n",
    "        else:\n",
    "            print(f\"[Warning] Missing image: {image_path}\")\n",
    "\n",
    "        if os.path.exists(annotation_path):\n",
    "            shutil.copy(annotation_path, target_annotation_path)\n",
    "        else:\n",
    "            print(f\"[Warning] Missing annotation: {annotation_path}\")\n",
    "\n",
    "def split_and_move_dataset(source_base_dir=\"./datasets/new_dataset_yolo\",\n",
    "                           target_base_dir=\"./datasets/new_dataset_yolo_split\",\n",
    "                           split_ratio=0.8,\n",
    "                           seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    image_dir = os.path.join(source_base_dir, \"images\")\n",
    "    label_dir = os.path.join(source_base_dir, \"labels\")\n",
    "\n",
    "    image_ids = [os.path.splitext(f)[0] for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "    random.shuffle(image_ids)\n",
    "\n",
    "    split_idx = int(len(image_ids) * split_ratio)\n",
    "    train_ids = image_ids[:split_idx]\n",
    "    val_ids = image_ids[split_idx:]\n",
    "\n",
    "    # Train\n",
    "    move_files(train_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"train/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"train/labels\"))\n",
    "\n",
    "    # Val\n",
    "    move_files(val_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"val/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"val/labels\"))\n",
    "\n",
    "    print(f\"\\n[✓] Dataset split completed: {len(train_ids)} train / {len(val_ids)} val samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c6d895",
   "metadata": {},
   "source": [
    "### Normalize labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "02cd8027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def normalize_label_file(label_file, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Normalize the label coordinates in a label file to ensure they are within [0, 1] range.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    with open(label_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, width, height = map(float, parts[1:])\n",
    "            \n",
    "            # Normalize coordinates to ensure they are within the range [0, 1]\n",
    "            x_center = min(1.0, max(0.0, x_center))\n",
    "            y_center = min(1.0, max(0.0, y_center))\n",
    "            width = min(1.0, max(0.0, width))\n",
    "            height = min(1.0, max(0.0, height))\n",
    "\n",
    "            # Write normalized values back to file\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "\n",
    "def get_image_size(img_path):\n",
    "    \"\"\"\n",
    "    Get the width and height of the image to normalize the coordinates properly.\n",
    "    \"\"\"\n",
    "    with Image.open(img_path) as img:\n",
    "        return img.size  # returns (width, height)\n",
    "\n",
    "\n",
    "def normalize_all_labels(labels_dir, img_dir):\n",
    "    \"\"\"\n",
    "    Normalize all label files in the specified directory.\n",
    "    \"\"\"\n",
    "    for label_file in tqdm(os.listdir(labels_dir)):\n",
    "       \n",
    "        if label_file.endswith('.txt'):  # Process only label files\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            img_path = os.path.join(img_dir, label_file.replace('.txt', '.jpg'))  # Assuming JPG images\n",
    "            if os.path.exists(img_path):\n",
    "                # Get image dimensions to normalize the labels\n",
    "                img_width, img_height = get_image_size(img_path)\n",
    "                # print(f\"Normalizing {label_file}...\")\n",
    "                normalize_label_file(label_path, img_width, img_height)\n",
    "            else:\n",
    "                print(f\"Warning: Image for label {label_file} not found!\")\n",
    "    print(\"Normalize Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f66204",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "59bfefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo(data_yaml=\"uavdt_yolo.yaml\", epochs=40, imgsz=640, batch=32, name=\"yolov8-uavdt\"):\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "    model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        show=True,\n",
    "        name=name,\n",
    "        project=\"runs_yolo/train\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee96142",
   "metadata": {},
   "source": [
    "### Finding best model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fb322ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_pt(base_dir='runs_yolo/'):\n",
    "    best_paths = list(Path(base_dir).rglob('best.pt'))\n",
    "    if not best_paths:\n",
    "        raise FileNotFoundError(\"No 'best.pt' file found in the 'runs/' directory.\")\n",
    "    \n",
    "    # Optionally, sort by latest modified time\n",
    "    best_paths.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    print(f\"✅ Found best.pt at: {best_paths[0]}\")\n",
    "    return str(best_paths[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69e3a88",
   "metadata": {},
   "source": [
    "### Prediciton on val images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1ea00c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# -------- Class ID to name and color -------- #\n",
    "class_id_to_name = {\n",
    "    0: ('road', [28, 42, 168]),\n",
    "    1: ('pool', [0, 50, 89]),\n",
    "    2: ('vegetation', [107, 142, 35]),\n",
    "    3: ('roof', [70, 70, 70]),\n",
    "    4: ('wall', [102, 102, 156]),\n",
    "    5: ('window', [254, 228, 12]),\n",
    "    6: ('person', [255, 22, 96]),\n",
    "    7: ('dog', [102, 51, 0]),\n",
    "    8: ('car', [9, 143, 150]),\n",
    "    9: ('bicycle', [119, 11, 32]),\n",
    "    10: ('tree', [51, 51, 0]),\n",
    "}\n",
    "\n",
    "# -------- Load YOLO Model -------- #\n",
    "def load_yolo_model(weight_path: str):\n",
    "    return YOLO(weight_path)\n",
    "\n",
    "# -------- Run YOLO Prediction -------- #\n",
    "def run_yolo(model, image: Image.Image):\n",
    "    temp_path = \"temp_yolo.jpg\"\n",
    "    image.save(temp_path)\n",
    "    result = model.predict(source=temp_path, conf=0.5, save=False, verbose=False)[0]\n",
    "    os.remove(temp_path)\n",
    "    return result\n",
    "\n",
    "# -------- Load Scalable Font -------- #\n",
    "def get_large_font(size=100):\n",
    "    try:\n",
    "        return ImageFont.truetype(\"arial.ttf\", size=size)\n",
    "    except:\n",
    "        try:\n",
    "            return ImageFont.truetype(\"DejaVuSans.ttf\", size=size)\n",
    "        except:\n",
    "            return ImageFont.load_default()\n",
    "\n",
    "# -------- Draw Bounding Boxes -------- #\n",
    "def draw_yolo_boxes(draw: ImageDraw.ImageDraw, result, font):\n",
    "    boxes = result.boxes\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes.xyxy[i].cpu().numpy()\n",
    "        cls_id = int(boxes.cls[i])\n",
    "        conf = float(boxes.conf[i])\n",
    "        name, color = class_id_to_name.get(cls_id, (f\"id_{cls_id}\", [255, 255, 255]))\n",
    "        label = f\"{name} {conf:.2f}\"\n",
    "\n",
    "        draw.rectangle(box.tolist(), outline=tuple(color), width=10)  # Thicker box\n",
    "        draw.text((box[0], box[1] - 120), label, fill=tuple(color), font=font)  # Adjust for bigger font\n",
    "\n",
    "# -------- Draw Embedded Legend -------- #\n",
    "def draw_legend_on_image(image: Image.Image, result, x_offset=20, y_offset=20, font=None):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    class_ids = result.boxes.cls.cpu().numpy().astype(int)\n",
    "    unique_ids = np.unique(class_ids)\n",
    "    spacing = 120  # Increased spacing\n",
    "    box_size = 80  # Larger color box\n",
    "\n",
    "    for idx, cls_id in enumerate(unique_ids):\n",
    "        name, color = class_id_to_name.get(cls_id, (f\"id_{cls_id}\", [255, 255, 255]))\n",
    "        y = y_offset + idx * spacing\n",
    "        draw.rectangle([x_offset, y, x_offset + box_size, y + box_size], fill=tuple(color))\n",
    "        draw.text((x_offset + box_size + 30, y), f\"{cls_id}: {name}\", fill=(255, 255, 255), font=font)\n",
    "\n",
    "# -------- Visualize Random Images -------- #\n",
    "def visualize_yolo_on_random_images(val_image_dir, yolo_model, num_images=5):\n",
    "    image_files = [f for f in os.listdir(val_image_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    random.shuffle(image_files)\n",
    "    selected_images = image_files[:num_images]\n",
    "\n",
    "    font = get_large_font(size=100)\n",
    "\n",
    "    for image_file in selected_images:\n",
    "        image_path = os.path.join(val_image_dir, image_file)\n",
    "        original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Run YOLO\n",
    "        yolo_result = run_yolo(yolo_model, original_image)\n",
    "\n",
    "        # Draw on image\n",
    "        yolo_image = original_image.copy()\n",
    "        draw = ImageDraw.Draw(yolo_image)\n",
    "        draw_yolo_boxes(draw, yolo_result, font=font)\n",
    "        draw_legend_on_image(yolo_image, yolo_result, font=font)\n",
    "\n",
    "        # Show\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(yolo_image)\n",
    "        plt.title(f\"YOLO Prediction: {image_file}\", fontsize=32)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0dc1d0",
   "metadata": {},
   "source": [
    "### Predcitions on videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ca55446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== FRAME PROCESSING ==========\n",
    "def process_frame(frame, yolo_model, w, h, class_id_to_name):\n",
    "    annotated = frame.copy()\n",
    "    results = yolo_model(annotated, verbose=False)[0]\n",
    "    boxes = results.boxes.xyxy.cpu().numpy()\n",
    "    class_ids = results.boxes.cls.cpu().numpy()\n",
    "\n",
    "    for box, cls_id in zip(boxes, class_ids):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        class_name, color = class_id_to_name[int(cls_id)]\n",
    "        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(annotated, class_name, (x1, max(y1 - 10, 10)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "    return annotated, boxes, class_ids\n",
    "\n",
    "# ========== VIDEO CAPTURE ==========\n",
    "def setup_video_capture(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    return cap, total_frames, fps, w, h\n",
    "\n",
    "# ========== MAIN FUNCTION ==========\n",
    "def process_all_videos(yolo_weights_path, class_id_to_name, video_dir='videos', output_base='opt', max_frames=None):\n",
    "    yolo_model = YOLO(yolo_weights_path)\n",
    "\n",
    "    image_out_dir = os.path.join(output_base, 'images')\n",
    "    label_out_dir = os.path.join(output_base, 'labels')\n",
    "    output_video_dir = os.path.join(output_base, 'output')\n",
    "\n",
    "    os.makedirs(image_out_dir, exist_ok=True)\n",
    "    os.makedirs(label_out_dir, exist_ok=True)\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    for video_file in tqdm(sorted(os.listdir(video_dir))):\n",
    "        if not video_file.lower().endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        video_id = os.path.splitext(video_file)[0]\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        output_video_path = os.path.join(output_video_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        print(f\"\\n========== STARTED: {video_id} ==========\")\n",
    "        cap, total_frames, fps, w, h = setup_video_capture(video_path)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(output_video_path, fourcc, fps, (w, h))\n",
    "\n",
    "        frame_count = 0\n",
    "        pbar = tqdm(total=max_frames if max_frames else total_frames, desc=video_id)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or (max_frames and frame_count >= max_frames):\n",
    "                break\n",
    "\n",
    "            annotated_bgr, boxes, class_ids = process_frame(frame, yolo_model, w, h, class_id_to_name)\n",
    "\n",
    "            # ✅ Save original image\n",
    "            img_filename = f'{video_id}_{frame_count:04d}.jpg'\n",
    "            img_path = os.path.join(image_out_dir, img_filename)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "\n",
    "            # ✅ Save YOLO-format label\n",
    "            label_filename = f'{video_id}_{frame_count:04d}.txt'\n",
    "            label_path = os.path.join(label_out_dir, label_filename)\n",
    "            with open(label_path, 'w') as f:\n",
    "                for box, cls_id in zip(boxes, class_ids):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    w_box = x2 - x1\n",
    "                    h_box = y2 - y1\n",
    "                    cx = x1 + w_box / 2\n",
    "                    cy = y1 + h_box / 2\n",
    "                    f.write(f\"{int(cls_id)} {cx/w:.6f} {cy/h:.6f} {w_box/w:.6f} {h_box/h:.6f}\\n\")\n",
    "\n",
    "            writer.write(annotated_bgr)\n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "        pbar.close()\n",
    "        print(f\"✔ DONE: {video_id} — Processed {frame_count} frames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6299d8c",
   "metadata": {},
   "source": [
    "### Print Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f1d4c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def find_results_csv(directory):\n",
    "    \"\"\"Find the results.csv file in the specified directory.\"\"\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if 'results.csv' in files:\n",
    "            return os.path.join(root, 'results.csv')\n",
    "    return None\n",
    "\n",
    "def load_results_csv(results_csv_path):\n",
    "    \"\"\"Load the results CSV into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(results_csv_path)\n",
    "\n",
    "def calculate_total_epochs(df):\n",
    "    \"\"\"Calculate the total number of epochs from the DataFrame.\"\"\"\n",
    "    return df['epoch'].max()\n",
    "\n",
    "def calculate_training_loss(epoch_data):\n",
    "    \"\"\"Calculate the total training loss from the given epoch data.\"\"\"\n",
    "    train_box_loss = epoch_data['train/box_loss']\n",
    "    train_cls_loss = epoch_data['train/cls_loss']\n",
    "    train_dfl_loss = epoch_data['train/dfl_loss']\n",
    "    return train_box_loss + train_cls_loss + train_dfl_loss\n",
    "\n",
    "def calculate_validation_loss(epoch_data):\n",
    "    \"\"\"Calculate the total validation loss from the given epoch data.\"\"\"\n",
    "    val_box_loss = epoch_data['val/box_loss']\n",
    "    val_cls_loss = epoch_data['val/cls_loss']\n",
    "    val_dfl_loss = epoch_data['val/dfl_loss']\n",
    "    return val_box_loss + val_cls_loss + val_dfl_loss\n",
    "\n",
    "def print_final_metrics(df):\n",
    "    \"\"\"Print the final metrics for the last epoch.\"\"\"\n",
    "    final_epoch_data = df.iloc[-1]\n",
    "\n",
    "    # Calculate total training and validation loss\n",
    "    train_loss = calculate_training_loss(final_epoch_data)\n",
    "    val_loss = calculate_validation_loss(final_epoch_data)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n========== Final Training Metrics ==========\")\n",
    "    print(f\"Training Loss: {train_loss:.6f}\")\n",
    "    print(f\"Precision: {final_epoch_data['metrics/precision(B)']:.6f}\")\n",
    "    print(f\"Recall: {final_epoch_data['metrics/recall(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5: {final_epoch_data['metrics/mAP50(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5:0.95: {final_epoch_data['metrics/mAP50-95(B)']:.6f}\")\n",
    "\n",
    "    print(\"\\n========== Final Validation Metrics ==========\")\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "def main(directory):\n",
    "    \"\"\"Main function to process and print final metrics.\"\"\"\n",
    "    # Find the results.csv file\n",
    "    results_csv_path = find_results_csv(directory)\n",
    "    \n",
    "    if not results_csv_path:\n",
    "        print(\"Error: 'results.csv' file not found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found results.csv at: {results_csv_path}\")\n",
    "\n",
    "    # Load results CSV\n",
    "    df = load_results_csv(results_csv_path)\n",
    "\n",
    "    # Get the total number of epochs\n",
    "    total_epochs = calculate_total_epochs(df)\n",
    "    print(f\"Total number of epochs: {total_epochs}\")\n",
    "\n",
    "    # Print columns in the CSV\n",
    "    print(\"\\n========== Columns in CSV ==========\")\n",
    "    print(df.columns)\n",
    "\n",
    "    # Print final metrics\n",
    "    print_final_metrics(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b357e76",
   "metadata": {},
   "source": [
    "### Retrain and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752af339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fea0de53",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237cff6",
   "metadata": {},
   "source": [
    "### Downaload and Convert Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "308c301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Class ID to name mapping with RGB values\n",
    "class_id_to_name = {\n",
    "    0: ('road', [28, 42, 168]),\n",
    "    1: ('pool', [0, 50, 89]),\n",
    "    2: ('vegetation', [107, 142, 35]),\n",
    "    3: ('roof', [70, 70, 70]),\n",
    "    4: ('wall', [102, 102, 156]),\n",
    "    5: ('window', [254, 228, 12]),\n",
    "    6: ('person', [255, 22, 96]),\n",
    "    7: ('dog', [102, 51, 0]),\n",
    "    8: ('car', [9, 143, 150]),\n",
    "    9: ('bicycle', [119, 11, 32]),\n",
    "    10: ('tree', [51, 51, 0]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a048bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gdrive_url = \"https://drive.google.com/file/d/1UppumYqYOi-kto6BWPfFxwJK2Eph46oY/view?usp=sharing\"\n",
    "# download_and_extract_from_gdrive(gdrive_url, extract_to=\"datasets\")\n",
    "\n",
    "# # Path to the dataset\n",
    "# dataset_path = \"./datasets/semantic_drone_dataset/training_set\"\n",
    "# output_dir = \"./datasets/new_dataset_yolo\"\n",
    "\n",
    "# convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22cd5c",
   "metadata": {},
   "source": [
    "### Visulaize the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d407833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"./datasets/new_dataset_yolo\"\n",
    "# image_dir = os.path.join(dataset_path, \"images\")\n",
    "# annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "\n",
    "# # Visualize 10 random images\n",
    "# image_ids = [f.split('.')[0] for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "# random_image_ids = random.sample(image_ids, min(10, len(image_ids)))\n",
    "\n",
    "# for image_id in random_image_ids:\n",
    "#     visualize_data(image_id, class_id_to_name, image_dir, annotations_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb37d1d",
   "metadata": {},
   "source": [
    "### Normalize and split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "440fb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set your paths\n",
    "# dataset_path = \"./datasets/new_dataset_yolo\"\n",
    "# image_dir = os.path.join(dataset_path, \"images\")\n",
    "# annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# normalize_all_labels(annotations_dir, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "660a3abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the split\n",
    "# split_and_move_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95130d28",
   "metadata": {},
   "source": [
    "### Training the datatsets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e1afbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# folder_path = './runs_yolo'\n",
    "\n",
    "# # Check if the folder exists before trying to delete\n",
    "# if os.path.exists(folder_path):\n",
    "#     shutil.rmtree(folder_path)\n",
    "#     print(f\"Deleted folder: {folder_path}\")\n",
    "# else:\n",
    "#     print(f\"Folder does not exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9bbab29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(\"[+] Training Start\")\n",
    "\n",
    "# # # Train YOLOv8\n",
    "# train_yolo(data_yaml=\"uavdt_yolo.yaml\",  epochs=50, imgsz=640, batch=32, name=\"yolov8-uavdt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0424e8",
   "metadata": {},
   "source": [
    "### Prediction on val images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "35bb74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# best_pt_path = find_best_pt()\n",
    "\n",
    "# val_image_dir = \"./datasets/new_dataset_yolo_split/val/images\"  # <<-- make sure this path exists\n",
    "    \n",
    "# yolo_model = load_yolo_model(best_pt_path)\n",
    "# # visualize_yolo_on_random_images(val_image_dir, yolo_model, num_images=10)  # Show only 10 images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581c05c",
   "metadata": {},
   "source": [
    "### Prediciton on videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4e278462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found best.pt at: runs_yolo\\train\\yolov8-uavdt\\weights\\best.pt\n"
     ]
    }
   ],
   "source": [
    "# # ================== CONFIG ==================\n",
    "# video_path = './video.mp4'\n",
    "# max_frames = None  # Set to None for full video\n",
    "best_pt_path = find_best_pt()\n",
    "# yolo_weights_path = best_pt_path  # <<-- change this\n",
    "# process_video(video_path, yolo_weights_path, class_id_to_name, max_frames=max_frames)  # Show only 10 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b2841c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found results.csv at: ./runs_yolo\\train\\yolov8-uavdt\\results.csv\n",
      "Total number of epochs: 200\n",
      "\n",
      "========== Columns in CSV ==========\n",
      "Index(['epoch', 'time', 'train/box_loss', 'train/cls_loss', 'train/dfl_loss',\n",
      "       'metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)',\n",
      "       'metrics/mAP50-95(B)', 'val/box_loss', 'val/cls_loss', 'val/dfl_loss',\n",
      "       'lr/pg0', 'lr/pg1', 'lr/pg2'],\n",
      "      dtype='object')\n",
      "\n",
      "========== Final Training Metrics ==========\n",
      "Training Loss: 2.568640\n",
      "Precision: 0.720460\n",
      "Recall: 0.589590\n",
      "mAP@0.5: 0.611510\n",
      "mAP@0.5:0.95: 0.405050\n",
      "\n",
      "========== Final Validation Metrics ==========\n",
      "Validation Loss: 4.009230\n"
     ]
    }
   ],
   "source": [
    "# Path to your results directory (where 'results.csv' should be located)\n",
    "directory = './runs_yolo'\n",
    "main(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5f1e3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_all_videos(yolo_weights_path=best_pt_path, class_id_to_name=class_id_to_name, video_dir='videos', output_base='opt', max_frames=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
