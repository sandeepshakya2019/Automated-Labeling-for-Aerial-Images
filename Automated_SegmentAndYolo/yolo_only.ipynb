{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc3ee2d",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "acb4ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id_to_name = {\n",
    "    0:  ('road', [28, 42, 168]),\n",
    "    1:  ('pool', [0, 50, 89]),\n",
    "    2:  ('vegetation', [107, 142, 35]),\n",
    "    3:  ('roof', [70, 70, 70]),\n",
    "    4:  ('wall', [102, 102, 156]),\n",
    "    5:  ('window', [254, 228, 12]),\n",
    "    6:  ('person', [255, 22, 96]),\n",
    "    7:  ('dog', [102, 51, 0]),\n",
    "    8:  ('car', [9, 143, 150]),\n",
    "    9:  ('bicycle', [119, 11, 32]),\n",
    "    10: ('tree', [51, 51, 0]),\n",
    "    11: ('truck', [160, 160, 60]),   # added truck\n",
    "    12: ('bus', [200, 80, 80]),      # added bus\n",
    "    13: ('vehicle', [200, 80, 80]),      # added bus\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62209f8",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f8ff18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install opencv-python\n",
    "# !pip install pillow\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch torchvision\n",
    "# !pip install ultralytics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "09e437de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch torchvision torchaudio\n",
    "# !pip cache purge  # clean out pip's install cache\n",
    "# !pip install torch torchvision torchaudio --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8431144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core packages\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "# Math and array handling\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Image and visualization\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep Learning Frameworks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.models.segmentation as segmentation\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Object Detection and Segmentation\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Automatically use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import gdown\n",
    "\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# save this as split_uavdt_train_val.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275651f4",
   "metadata": {},
   "source": [
    "### Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "42512998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_from_gdrive1(gdrive_url, extract_to=\"extracted\"):\n",
    "    # Convert shared drive URL to direct download URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create output folder\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[INFO] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[INFO] Extracting ZIP...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Optionally, remove the ZIP file after extraction\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[DONE] Extracted files to: {extract_to}\")\n",
    "\n",
    "def download_and_extract_from_gdrive2(gdrive_url, extract_to=\"extracted\"):\n",
    "    # Convert shared drive URL to direct download URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create output folder\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[INFO] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[INFO] Extracting ZIP...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Optionally, remove the ZIP file after extraction\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[DONE] Extracted files to: {extract_to}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1e94f",
   "metadata": {},
   "source": [
    "### Convert downloaded Dataset into yolo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7dbcea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Parse polygon and convert to YOLO bbox\n",
    "# ----------------------------\n",
    "# Semantic drone datasets \n",
    "def parse_yolo_style_bbox_from_xml(xml_path, class_id_to_name):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "        if class_name in [value[0] for value in class_id_to_name.values()]:\n",
    "            polygon = obj.find('polygon')\n",
    "            if polygon is not None:\n",
    "                points = polygon.findall('pt')\n",
    "                coords = [(float(pt.find('x').text), float(pt.find('y').text)) for pt in points]\n",
    "                x_min = min(coord[0] for coord in coords)\n",
    "                y_min = min(coord[1] for coord in coords)\n",
    "                x_max = max(coord[0] for coord in coords)\n",
    "                y_max = max(coord[1] for coord in coords)\n",
    "                bboxes.append(((x_min, y_min), (x_max, y_max), class_name))\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Save YOLO-format txt\n",
    "# ----------------------------\n",
    "def save_yolo_format(image_id, bboxes, image_width, image_height, output_path, class_id_to_name):\n",
    "    with open(output_path, 'w') as f:\n",
    "        for (x_min, y_min), (x_max, y_max), class_name in bboxes:\n",
    "            class_id = next(cid for cid, (name, _) in class_id_to_name.items() if name == class_name)\n",
    "            x_center = (x_min + x_max) / 2 / image_width\n",
    "            y_center = (y_min + y_max) / 2 / image_height\n",
    "            width = (x_max - x_min) / image_width\n",
    "            height = (y_max - y_min) / image_height\n",
    "            f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Convert dataset (YOLO only)\n",
    "# ----------------------------\n",
    "def convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name):\n",
    "    image_ids = [img.split('.')[0] for img in os.listdir(f\"{dataset_path}/images\") if img.endswith(\".jpg\")]\n",
    "\n",
    "    os.makedirs(f\"{output_dir}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/labels\", exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(image_ids, desc=\"Converting to YOLO\"):\n",
    "        img_path = f\"{dataset_path}/images/{image_id}.jpg\"\n",
    "        bbox_xml_path = f\"{dataset_path}/gt/bounding_box/label_me_xml/{image_id}.xml\"\n",
    "        semantic_xml_path = f\"{dataset_path}/gt/semantic/label_me_xml/{image_id}.xml\"\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"[WARNING] Image not found: {img_path}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            bboxes1 = parse_yolo_style_bbox_from_xml(bbox_xml_path, class_id_to_name)\n",
    "            bboxes2 = parse_yolo_style_bbox_from_xml(semantic_xml_path, class_id_to_name)\n",
    "            all_bboxes = bboxes1 + bboxes2\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Skipping image {image_id} due to parse error: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            image_np = np.array(image)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not load image {image_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save image\n",
    "        image.save(f\"{output_dir}/images/{image_id}.jpg\")\n",
    "\n",
    "        # Save YOLO labels\n",
    "        yolo_annotation_path = f\"{output_dir}/labels/{image_id}.txt\"\n",
    "        save_yolo_format(image_id, all_bboxes, image_np.shape[1], image_np.shape[0], yolo_annotation_path, class_id_to_name)\n",
    "\n",
    "    print(\"✅ YOLO-format annotation conversion complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f03a051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 🧠 Map UAVDT class to extended class_id_to_name\n",
    "uavdt_to_extended = {\n",
    "    0: 8,   # car\n",
    "    1: 11,  # truck\n",
    "    2: 12,  # bus\n",
    "    3: 13\n",
    "}\n",
    "\n",
    "# === Function to Convert Single Annotation to YOLO Format ===\n",
    "def convert_annotation(anno_path, label_path, image_path, stats):\n",
    "    if not os.path.exists(image_path):\n",
    "        stats[\"missing_image\"] += 1\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        height, width = img.shape[:2]\n",
    "    except:\n",
    "        stats[\"missing_image\"] += 1\n",
    "        return\n",
    "\n",
    "    with open(anno_path, 'r') as fin, open(label_path, 'w') as fout:\n",
    "        for line in fin:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) < 8:\n",
    "                stats[\"malformed\"] += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                x, y, w, h = map(float, parts[0:4])\n",
    "                original_cls = int(parts[5])\n",
    "\n",
    "                # 🔁 Convert original class to extended class\n",
    "                if original_cls not in uavdt_to_extended:\n",
    "                    stats[\"skipped\"][original_cls] += 1\n",
    "                    continue\n",
    "\n",
    "                cls = uavdt_to_extended[original_cls]\n",
    "\n",
    "                x_center = (x + w / 2) / width\n",
    "                y_center = (y + h / 2) / height\n",
    "                w /= width\n",
    "                h /= height\n",
    "\n",
    "                if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and w > 0 and h > 0):\n",
    "                    stats[\"skipped\"][cls] += 1\n",
    "                    continue\n",
    "\n",
    "                fout.write(f\"{cls} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "                stats[\"converted\"] += 1\n",
    "            except Exception:\n",
    "                stats[\"malformed\"] += 1\n",
    "                continue\n",
    "\n",
    "            stats[\"total\"] += 1\n",
    "\n",
    "# === Step 1: Convert UAVDT annotations to YOLO format ===\n",
    "def convert_dataset(root_dir):\n",
    "    annotation_paths = glob(os.path.join(root_dir, \"M*/annotations/*.txt\"))\n",
    "    total_files = len(annotation_paths)\n",
    "\n",
    "    stats = {\n",
    "        \"total\": 0,\n",
    "        \"converted\": 0,\n",
    "        \"malformed\": 0,\n",
    "        \"missing_image\": 0,\n",
    "        \"skipped\": defaultdict(int)\n",
    "    }\n",
    "\n",
    "    print(f\"🔄 Converting {total_files} annotation files to YOLO format...\")\n",
    "\n",
    "    for anno_path in tqdm(annotation_paths, desc=\"Converting\", unit=\"file\"):\n",
    "        sequence_dir = os.path.dirname(os.path.dirname(anno_path))  # Mxxxx\n",
    "        file_name = os.path.basename(anno_path)\n",
    "\n",
    "        label_dir = os.path.join(sequence_dir, \"labels\")\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "        label_path = os.path.join(label_dir, file_name)\n",
    "\n",
    "        # Construct image path\n",
    "        image_name = file_name.replace(\".txt\", \".jpg\")\n",
    "        image_path = os.path.join(sequence_dir, \"images\", image_name)\n",
    "\n",
    "        convert_annotation(anno_path, label_path, image_path, stats)\n",
    "\n",
    "    print(\"\\n✅ Conversion complete.\")\n",
    "    print(f\"📊 Total boxes:     {stats['total']}\")\n",
    "    print(f\"✅ Converted boxes: {stats['converted']}\")\n",
    "    print(f\"❌ Skipped boxes:   {sum(stats['skipped'].values())}\")\n",
    "    for cls, count in sorted(stats[\"skipped\"].items()):\n",
    "        print(f\"   - Skipped class {cls}: {count}\")\n",
    "    print(f\"⚠️ Malformed lines: {stats['malformed']}\")\n",
    "    print(f\"🖼️  Missing images: {stats['missing_image']}\")\n",
    "\n",
    "# === Step 2: Copy to train/val structure ===\n",
    "def copy_split_sequences(src_root, dst_root, train_ratio=0.8):\n",
    "    all_sequences = sorted(glob(os.path.join(src_root, \"M*\")))\n",
    "    train_seqs, val_seqs = train_test_split(all_sequences, train_size=train_ratio, random_state=42)\n",
    "\n",
    "    for split_name, split_list in zip(['train', 'val'], [train_seqs, val_seqs]):\n",
    "        for seq_path in tqdm(split_list, desc=f\"Copying {split_name}\"):\n",
    "            images_src = os.path.join(seq_path, \"images\")\n",
    "            labels_src = os.path.join(seq_path, \"labels\")\n",
    "\n",
    "            images_dst = os.path.join(dst_root, split_name, \"images\")\n",
    "            labels_dst = os.path.join(dst_root, split_name, \"labels\")\n",
    "\n",
    "            os.makedirs(images_dst, exist_ok=True)\n",
    "            os.makedirs(labels_dst, exist_ok=True)\n",
    "\n",
    "            for img_file in glob(os.path.join(images_src, \"*.jpg\")):\n",
    "                shutil.copy(img_file, os.path.join(images_dst, os.path.basename(img_file)))\n",
    "\n",
    "            for label_file in glob(os.path.join(labels_src, \"*.txt\")):\n",
    "                shutil.copy(label_file, os.path.join(labels_dst, os.path.basename(label_file)))\n",
    "\n",
    "    print(\"\\n✅ Dataset split into 'train/' and 'val/' with images and YOLO labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4683246",
   "metadata": {},
   "source": [
    "### Visualizing images with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8af9cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Load YOLO annotations\n",
    "def load_yolo_annotations(anno_file):\n",
    "    with open(anno_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    boxes = []\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "        boxes.append((class_id, x_center, y_center, width, height))\n",
    "    return boxes\n",
    "\n",
    "# Visualize image + boxes with dynamic font/thickness\n",
    "def visualize_data(image_id, class_id_to_name, image_dir, annotations_dir):\n",
    "    img_path = os.path.join(image_dir, f\"{image_id}.jpg\")\n",
    "    annotation_path = os.path.join(annotations_dir, f\"{image_id}.txt\")\n",
    "\n",
    "    if not os.path.exists(img_path) or not os.path.exists(annotation_path):\n",
    "        print(f\"[WARNING] Missing files for {image_id}, skipping...\")\n",
    "        return\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(img_path)\n",
    "    image = np.array(image)\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Dynamic scaling factors\n",
    "    scale = max(width, height) / 1000.0  # adjust divisor for tuning\n",
    "    rectangle_thickness = int(2 * scale)\n",
    "    font_scale = 0.7 * scale\n",
    "    font_thickness = int(2 * scale)\n",
    "    text_color = (255, 0, 0)\n",
    "\n",
    "    # Load annotations\n",
    "    boxes = load_yolo_annotations(annotation_path)\n",
    "\n",
    "    # Draw boxes\n",
    "    image_with_boxes = image.copy()\n",
    "    for box in boxes:\n",
    "        class_id, x_center, y_center, w, h = box\n",
    "        class_name, color = class_id_to_name[int(class_id)]\n",
    "\n",
    "        x_min = int((x_center - w / 2) * width)\n",
    "        y_min = int((y_center - h / 2) * height)\n",
    "        x_max = int((x_center + w / 2) * width)\n",
    "        y_max = int((y_center + h / 2) * height)\n",
    "\n",
    "        cv2.rectangle(image_with_boxes, (x_min, y_min), (x_max, y_max), color, rectangle_thickness)\n",
    "        cv2.putText(\n",
    "            image_with_boxes,\n",
    "            class_name,\n",
    "            (x_min, max(y_min - 10, 0)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            font_scale,\n",
    "            text_color,\n",
    "            font_thickness,\n",
    "            lineType=cv2.LINE_AA\n",
    "        )\n",
    "\n",
    "    # Plot with class legend\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(image_with_boxes)\n",
    "    plt.title(f\"Image ID: {image_id}\")\n",
    "    legend_handles = [\n",
    "        mpatches.Patch(color=np.array(rgb) / 255.0, label=f\"{name} ({cid})\")\n",
    "        for cid, (name, rgb) in class_id_to_name.items()\n",
    "    ]\n",
    "    plt.legend(handles=legend_handles, loc='upper right', fontsize=10)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ab554",
   "metadata": {},
   "source": [
    "### Convert into train set and val set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d7373103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Drone Datasets\n",
    "def move_files(file_list, \n",
    "               source_image_dir, \n",
    "               source_annotation_dir,\n",
    "               target_image_dir, \n",
    "               target_annotation_dir):\n",
    "    \n",
    "    os.makedirs(target_image_dir, exist_ok=True)\n",
    "    os.makedirs(target_annotation_dir, exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(file_list, desc=f\"Moving to {os.path.basename(os.path.dirname(target_image_dir))}\"):\n",
    "        image_path = os.path.join(source_image_dir, f\"{image_id}.jpg\")\n",
    "        annotation_path = os.path.join(source_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        target_image_path = os.path.join(target_image_dir, f\"{image_id}.jpg\")\n",
    "        target_annotation_path = os.path.join(target_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            shutil.copy(image_path, target_image_path)\n",
    "        else:\n",
    "            print(f\"[Warning] Missing image: {image_path}\")\n",
    "\n",
    "        if os.path.exists(annotation_path):\n",
    "            shutil.copy(annotation_path, target_annotation_path)\n",
    "        else:\n",
    "            print(f\"[Warning] Missing annotation: {annotation_path}\")\n",
    "\n",
    "def split_and_move_dataset(source_base_dir=\"./datasets/new_dataset_yolo\",\n",
    "                           target_base_dir=\"./datasets/new_dataset_yolo_split\",\n",
    "                           split_ratio=0.8,\n",
    "                           seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    image_dir = os.path.join(source_base_dir, \"images\")\n",
    "    label_dir = os.path.join(source_base_dir, \"labels\")\n",
    "\n",
    "    image_ids = [os.path.splitext(f)[0] for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "    random.shuffle(image_ids)\n",
    "\n",
    "    split_idx = int(len(image_ids) * split_ratio)\n",
    "    train_ids = image_ids[:split_idx]\n",
    "    val_ids = image_ids[split_idx:]\n",
    "\n",
    "    # Train\n",
    "    move_files(train_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"train/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"train/labels\"))\n",
    "\n",
    "    # Val\n",
    "    move_files(val_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"val/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"val/labels\"))\n",
    "\n",
    "    print(f\"\\n[✓] Dataset split completed: {len(train_ids)} train / {len(val_ids)} val samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c6d895",
   "metadata": {},
   "source": [
    "### Normalize labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "02cd8027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def normalize_label_file(label_file, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Normalize the label coordinates in a label file to ensure they are within [0, 1] range.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    with open(label_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, width, height = map(float, parts[1:])\n",
    "            \n",
    "            # Normalize coordinates to ensure they are within the range [0, 1]\n",
    "            x_center = min(1.0, max(0.0, x_center))\n",
    "            y_center = min(1.0, max(0.0, y_center))\n",
    "            width = min(1.0, max(0.0, width))\n",
    "            height = min(1.0, max(0.0, height))\n",
    "\n",
    "            # Write normalized values back to file\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "\n",
    "def get_image_size(img_path):\n",
    "    \"\"\"\n",
    "    Get the width and height of the image to normalize the coordinates properly.\n",
    "    \"\"\"\n",
    "    with Image.open(img_path) as img:\n",
    "        return img.size  # returns (width, height)\n",
    "\n",
    "\n",
    "def normalize_all_labels(labels_dir, img_dir):\n",
    "    \"\"\"\n",
    "    Normalize all label files in the specified directory.\n",
    "    \"\"\"\n",
    "    for label_file in tqdm(os.listdir(labels_dir)):\n",
    "       \n",
    "        if label_file.endswith('.txt'):  # Process only label files\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            img_path = os.path.join(img_dir, label_file.replace('.txt', '.jpg'))  # Assuming JPG images\n",
    "            if os.path.exists(img_path):\n",
    "                # Get image dimensions to normalize the labels\n",
    "                img_width, img_height = get_image_size(img_path)\n",
    "                # print(f\"Normalizing {label_file}...\")\n",
    "                normalize_label_file(label_path, img_width, img_height)\n",
    "            else:\n",
    "                print(f\"Warning: Image for label {label_file} not found!\")\n",
    "    print(\"Normalize Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f66204",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "59bfefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo(data_yaml=\"uavdt_yolo.yaml\", epochs=40, imgsz=720, batch=8, name=\"yolov8-uavdt\"):\n",
    "    model = YOLO(\"yolov8s.pt\")\n",
    "\n",
    "    model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        name=name,\n",
    "        project=\"runs_yolo/train\",\n",
    "        patience=20,  # Early stopping\n",
    "        augment=True,  # Apply augmentations\n",
    "        degrees=10,  # Image rotation\n",
    "        scale=0.5,  # Scale range\n",
    "        flipud=0.2,  # Vertical flip\n",
    "        fliplr=0.5,  # Horizontal flip\n",
    "        hsv_h=0.015,  # Hue augmentation\n",
    "        hsv_s=0.7,  # Saturation augmentation\n",
    "        hsv_v=0.4,  # Value augmentation\n",
    "        mosaic=1.0,  # Mosaic augmentation\n",
    "        mixup=0.2,  # Mixup augmentation\n",
    "        lr0=0.01,  # Initial learning rate (you can tune this)\n",
    "        lrf=0.01,  # Learning rate final factor (for cosine annealing)\n",
    "        verbose=True  # Print progress\n",
    "    )\n",
    "\n",
    "    metrics = model.val()\n",
    "    print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee96142",
   "metadata": {},
   "source": [
    "### Finding best model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fb322ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_pt(base_dir='runs_yolo/'):\n",
    "    best_paths = list(Path(base_dir).rglob('best.pt'))\n",
    "    if not best_paths:\n",
    "        raise FileNotFoundError(\"No 'best.pt' file found in the 'runs/' directory.\")\n",
    "    \n",
    "    # Optionally, sort by latest modified time\n",
    "    best_paths.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    print(f\"✅ Found best.pt at: {best_paths[0]}\")\n",
    "    return str(best_paths[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69e3a88",
   "metadata": {},
   "source": [
    "### Prediciton on val images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1ea00c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# -------- Load YOLO Model -------- #\n",
    "def load_yolo_model(weight_path: str):\n",
    "    return YOLO(weight_path)\n",
    "\n",
    "# -------- Run YOLO Prediction -------- #\n",
    "def run_yolo(model, image: Image.Image):\n",
    "    temp_path = \"temp_yolo.jpg\"\n",
    "    image.save(temp_path)\n",
    "    result = model.predict(source=temp_path, conf=0.5, save=False, verbose=False)[0]\n",
    "    os.remove(temp_path)\n",
    "    return result\n",
    "\n",
    "# -------- Load Scalable Font -------- #\n",
    "def get_large_font(size=100):\n",
    "    try:\n",
    "        return ImageFont.truetype(\"arial.ttf\", size=size)\n",
    "    except:\n",
    "        try:\n",
    "            return ImageFont.truetype(\"DejaVuSans.ttf\", size=size)\n",
    "        except:\n",
    "            return ImageFont.load_default()\n",
    "\n",
    "# -------- Draw Bounding Boxes -------- #\n",
    "def draw_yolo_boxes(draw: ImageDraw.ImageDraw, result, font):\n",
    "    boxes = result.boxes\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes.xyxy[i].cpu().numpy()\n",
    "        cls_id = int(boxes.cls[i])\n",
    "        conf = float(boxes.conf[i])\n",
    "        name, color = class_id_to_name.get(cls_id, (f\"id_{cls_id}\", [255, 255, 255]))\n",
    "        label = f\"{name} {conf:.2f}\"\n",
    "\n",
    "        draw.rectangle(box.tolist(), outline=tuple(color), width=10)  # Thicker box\n",
    "        draw.text((box[0], box[1] - 120), label, fill=tuple(color), font=font)  # Adjust for bigger font\n",
    "\n",
    "# -------- Draw Embedded Legend -------- #\n",
    "def draw_legend_on_image(image: Image.Image, result, x_offset=20, y_offset=20, font=None):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    class_ids = result.boxes.cls.cpu().numpy().astype(int)\n",
    "    unique_ids = np.unique(class_ids)\n",
    "    spacing = 120  # Increased spacing\n",
    "    box_size = 80  # Larger color box\n",
    "\n",
    "    for idx, cls_id in enumerate(unique_ids):\n",
    "        name, color = class_id_to_name.get(cls_id, (f\"id_{cls_id}\", [255, 255, 255]))\n",
    "        y = y_offset + idx * spacing\n",
    "        draw.rectangle([x_offset, y, x_offset + box_size, y + box_size], fill=tuple(color))\n",
    "        draw.text((x_offset + box_size + 30, y), f\"{cls_id}: {name}\", fill=(255, 255, 255), font=font)\n",
    "\n",
    "# -------- Visualize Random Images -------- #\n",
    "def visualize_yolo_on_random_images(val_image_dir, yolo_model, num_images=5):\n",
    "    image_files = [f for f in os.listdir(val_image_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    random.shuffle(image_files)\n",
    "    selected_images = image_files[:num_images]\n",
    "\n",
    "    font = get_large_font(size=100)\n",
    "\n",
    "    for image_file in selected_images:\n",
    "        image_path = os.path.join(val_image_dir, image_file)\n",
    "        original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Run YOLO\n",
    "        yolo_result = run_yolo(yolo_model, original_image)\n",
    "\n",
    "        # Draw on image\n",
    "        yolo_image = original_image.copy()\n",
    "        draw = ImageDraw.Draw(yolo_image)\n",
    "        draw_yolo_boxes(draw, yolo_result, font=font)\n",
    "        draw_legend_on_image(yolo_image, yolo_result, font=font)\n",
    "\n",
    "        # Show\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(yolo_image)\n",
    "        plt.title(f\"YOLO Prediction: {image_file}\", fontsize=32)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0dc1d0",
   "metadata": {},
   "source": [
    "### Predcitions on videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ca55446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== FRAME PROCESSING ==========\n",
    "def process_frame(frame, yolo_model, w, h, class_id_to_name):\n",
    "    annotated = frame.copy()\n",
    "    results = yolo_model(annotated, verbose=False)[0]\n",
    "    boxes = results.boxes.xyxy.cpu().numpy()\n",
    "    class_ids = results.boxes.cls.cpu().numpy()\n",
    "\n",
    "    for box, cls_id in zip(boxes, class_ids):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        class_name, color = class_id_to_name[int(cls_id)]\n",
    "        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(annotated, class_name, (x1, max(y1 - 10, 10)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "    return annotated, boxes, class_ids\n",
    "\n",
    "# ========== VIDEO CAPTURE ==========\n",
    "def setup_video_capture(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    return cap, total_frames, fps, w, h\n",
    "\n",
    "# ========== MAIN FUNCTION ==========\n",
    "def process_all_videos(yolo_weights_path, class_id_to_name, video_dir='videos', output_base='opt', max_frames=None):\n",
    "    yolo_model = YOLO(yolo_weights_path)\n",
    "\n",
    "    image_out_dir = os.path.join(output_base, 'images')\n",
    "    label_out_dir = os.path.join(output_base, 'labels')\n",
    "    output_video_dir = os.path.join(output_base, 'output')\n",
    "\n",
    "    os.makedirs(image_out_dir, exist_ok=True)\n",
    "    os.makedirs(label_out_dir, exist_ok=True)\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    for video_file in tqdm(sorted(os.listdir(video_dir))):\n",
    "        if not video_file.lower().endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        video_id = os.path.splitext(video_file)[0]\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        output_video_path = os.path.join(output_video_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        print(f\"\\n========== STARTED: {video_id} ==========\")\n",
    "        cap, total_frames, fps, w, h = setup_video_capture(video_path)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(output_video_path, fourcc, fps, (w, h))\n",
    "\n",
    "        frame_count = 0\n",
    "        pbar = tqdm(total=max_frames if max_frames else total_frames, desc=video_id)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or (max_frames and frame_count >= max_frames):\n",
    "                break\n",
    "\n",
    "            annotated_bgr, boxes, class_ids = process_frame(frame, yolo_model, w, h, class_id_to_name)\n",
    "\n",
    "            # ✅ Save original image\n",
    "            img_filename = f'{video_id}_{frame_count:04d}.jpg'\n",
    "            img_path = os.path.join(image_out_dir, img_filename)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "\n",
    "            # ✅ Save YOLO-format label\n",
    "            label_filename = f'{video_id}_{frame_count:04d}.txt'\n",
    "            label_path = os.path.join(label_out_dir, label_filename)\n",
    "            with open(label_path, 'w') as f:\n",
    "                for box, cls_id in zip(boxes, class_ids):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    w_box = x2 - x1\n",
    "                    h_box = y2 - y1\n",
    "                    cx = x1 + w_box / 2\n",
    "                    cy = y1 + h_box / 2\n",
    "                    f.write(f\"{int(cls_id)} {cx/w:.6f} {cy/h:.6f} {w_box/w:.6f} {h_box/h:.6f}\\n\")\n",
    "\n",
    "            writer.write(annotated_bgr)\n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "        pbar.close()\n",
    "        print(f\"✔ DONE: {video_id} — Processed {frame_count} frames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6299d8c",
   "metadata": {},
   "source": [
    "### Print Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f1d4c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def find_results_csv(directory):\n",
    "    \"\"\"Find the results.csv file in the specified directory.\"\"\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if 'results.csv' in files:\n",
    "            return os.path.join(root, 'results.csv')\n",
    "    return None\n",
    "\n",
    "def load_results_csv(results_csv_path):\n",
    "    \"\"\"Load the results CSV into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(results_csv_path)\n",
    "\n",
    "def calculate_total_epochs(df):\n",
    "    \"\"\"Calculate the total number of epochs from the DataFrame.\"\"\"\n",
    "    return df['epoch'].max()\n",
    "\n",
    "def calculate_training_loss(epoch_data):\n",
    "    \"\"\"Calculate the total training loss from the given epoch data.\"\"\"\n",
    "    train_box_loss = epoch_data['train/box_loss']\n",
    "    train_cls_loss = epoch_data['train/cls_loss']\n",
    "    train_dfl_loss = epoch_data['train/dfl_loss']\n",
    "    return train_box_loss + train_cls_loss + train_dfl_loss\n",
    "\n",
    "def calculate_validation_loss(epoch_data):\n",
    "    \"\"\"Calculate the total validation loss from the given epoch data.\"\"\"\n",
    "    val_box_loss = epoch_data['val/box_loss']\n",
    "    val_cls_loss = epoch_data['val/cls_loss']\n",
    "    val_dfl_loss = epoch_data['val/dfl_loss']\n",
    "    return val_box_loss + val_cls_loss + val_dfl_loss\n",
    "\n",
    "def print_final_metrics(df):\n",
    "    \"\"\"Print the final metrics for the last epoch.\"\"\"\n",
    "    final_epoch_data = df.iloc[-1]\n",
    "\n",
    "    # Calculate total training and validation loss\n",
    "    train_loss = calculate_training_loss(final_epoch_data)\n",
    "    val_loss = calculate_validation_loss(final_epoch_data)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n========== Final Training Metrics ==========\")\n",
    "    print(f\"Training Loss: {train_loss:.6f}\")\n",
    "    print(f\"Precision: {final_epoch_data['metrics/precision(B)']:.6f}\")\n",
    "    print(f\"Recall: {final_epoch_data['metrics/recall(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5: {final_epoch_data['metrics/mAP50(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5:0.95: {final_epoch_data['metrics/mAP50-95(B)']:.6f}\")\n",
    "\n",
    "    print(\"\\n========== Final Validation Metrics ==========\")\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "    print(f\"Validation Precision: {final_epoch_data['metrics/precision(B)']:.6f}\")  # Validation precision (use corresponding val column)\n",
    "    print(f\"Validation Recall: {final_epoch_data['metrics/recall(B)']:.6f}\")  # Validation recall (use corresponding val column)\n",
    "    print(f\"Validation mAP@0.5: {final_epoch_data['metrics/mAP50(B)']:.6f}\")  # Validation mAP@0.5 (use corresponding val column)\n",
    "    print(f\"Validation mAP@0.5:0.95: {final_epoch_data['metrics/mAP50-95(B)']:.6f}\")  # Validation mAP@0.5:0.95 (use corresponding val column)\n",
    "\n",
    "def main(directory):\n",
    "    \"\"\"Main function to process and print final metrics.\"\"\"\n",
    "    # Find the results.csv file\n",
    "    results_csv_path = find_results_csv(directory)\n",
    "    \n",
    "    if not results_csv_path:\n",
    "        print(\"Error: 'results.csv' file not found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found results.csv at: {results_csv_path}\")\n",
    "\n",
    "    # Load results CSV\n",
    "    df = load_results_csv(results_csv_path)\n",
    "    # Get the total number of epochs\n",
    "    total_epochs = calculate_total_epochs(df)\n",
    "    print(f\"Total number of epochs: {total_epochs}\")\n",
    "\n",
    "    # Print columns in the CSV\n",
    "    # print(\"\\n========== Columns in CSV ==========\")\n",
    "    # print(df.columns)\n",
    "\n",
    "    # Print final metrics\n",
    "    print_final_metrics(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b357e76",
   "metadata": {},
   "source": [
    "### Retrain and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752af339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fea0de53",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237cff6",
   "metadata": {},
   "source": [
    "### Downaload and Convert Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b9ef868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gdrive_url = \"https://drive.google.com/file/d/1UppumYqYOi-kto6BWPfFxwJK2Eph46oY/view?usp=sharing\"\n",
    "# download_and_extract_from_gdrive1(gdrive_url, extract_to=\"datasets\")\n",
    "\n",
    "# --------- Usage ----------\n",
    "# gdrive_url = \"https://drive.google.com/file/d/12cbrTaBAMIsuU-mwAA7IgDk9wSLC9cC-/view?usp=sharing\"\n",
    "# download_and_extract_from_gdrive2(gdrive_url, extract_to=\"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a048bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Path to the dataset\n",
    "# dataset_path = \"./datasets/semantic_drone_dataset/training_set\"\n",
    "# output_dir = \"./datasets/new_dataset_yolo\"\n",
    "\n",
    "# convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "660a3abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #UAVDT-2024\n",
    "\n",
    "# source_root = \"./datasets/UAVDT-2024\"\n",
    "# output_root = \"./datasets/new_dataset_yolo_split\"\n",
    "\n",
    "# convert_dataset(source_root)\n",
    "# copy_split_sequences(source_root, output_root, train_ratio=0.8)\n",
    "\n",
    "\n",
    "# # Semantic dorne datasets\n",
    "# split_and_move_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "567b1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set your paths\n",
    "# dataset_path = \"./datasets/new_dataset_yolo_split/train\"\n",
    "# image_dir = os.path.join(dataset_path, \"images\")\n",
    "# annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# normalize_all_labels(annotations_dir, image_dir)\n",
    "\n",
    "# dataset_path = \"./datasets/new_dataset_yolo_split/val\"\n",
    "# image_dir = os.path.join(dataset_path, \"images\")\n",
    "# annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# normalize_all_labels(annotations_dir, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ff47eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"./datasets/new_dataset_yolo_split/train\"\n",
    "# image_dir = os.path.join(dataset_path, \"images\")\n",
    "# annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "\n",
    "# # Visualize 10 random images\n",
    "# image_ids = [f.split('.')[0] for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "# random_image_ids = random.sample(image_ids, min(30, len(image_ids)))\n",
    "\n",
    "# for image_id in random_image_ids:\n",
    "#     visualize_data(image_id, class_id_to_name, image_dir, annotations_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95130d28",
   "metadata": {},
   "source": [
    "### Training the datatsets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e1afbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # List of folders to delete\n",
    "# folders_to_delete = ['./datasets/new', './datasets/new_dataset_yolo', './datasets/uavdt-processed', './runs_yolo']\n",
    "\n",
    "# for folder_path in folders_to_delete:\n",
    "#     if os.path.exists(folder_path):\n",
    "#         shutil.rmtree(folder_path)\n",
    "#         print(f\"✅ Deleted folder: {folder_path}\")\n",
    "#     else:\n",
    "#         print(f\"⚠️ Folder does not exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9bbab29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"[+] Training Start\")\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# # # Train YOLOv8\n",
    "# train_yolo(data_yaml=\"uavdt_yolo.yaml\",  epochs=200, imgsz=720, batch=8, name=\"yolov8-uavdt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0424e8",
   "metadata": {},
   "source": [
    "### Prediction on val images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "35bb74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# best_pt_path = find_best_pt()\n",
    "\n",
    "# val_image_dir = \"./datasets/new_dataset_yolo_split/val/images\"  # <<-- make sure this path exists\n",
    "    \n",
    "# yolo_model = load_yolo_model(best_pt_path)\n",
    "# visualize_yolo_on_random_images(val_image_dir, yolo_model, num_images=10)  # Show only 10 images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581c05c",
   "metadata": {},
   "source": [
    "### Prediciton on videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8d0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b2841c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found results.csv at: ./runs_yolo\\train\\yolov8-uavdt\\results.csv\n",
      "Total number of epochs: 97\n",
      "\n",
      "========== Final Training Metrics ==========\n",
      "Training Loss: 1.896580\n",
      "Precision: 0.482240\n",
      "Recall: 0.319210\n",
      "mAP@0.5: 0.353800\n",
      "mAP@0.5:0.95: 0.229580\n",
      "\n",
      "========== Final Validation Metrics ==========\n",
      "Validation Loss: 5.205170\n",
      "Validation Precision: 0.482240\n",
      "Validation Recall: 0.319210\n",
      "Validation mAP@0.5: 0.353800\n",
      "Validation mAP@0.5:0.95: 0.229580\n"
     ]
    }
   ],
   "source": [
    "# Path to your results directory (where 'results.csv' should be located)\n",
    "directory = './runs_yolo'\n",
    "main(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e3842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found best.pt at: runs_yolo\\train\\yolov8-uavdt\\weights\\best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13469da161c94ffaa2ce49cc2690725c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== STARTED: v1 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc5dcf4a8304e3bb37cec5dd401f593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v1:   0%|          | 0/897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ DONE: v1 — Processed 897 frames\n",
      "\n",
      "========== STARTED: v10 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eebed20b9ef47e498b2bc38e38a3b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v10:   0%|          | 0/415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ DONE: v10 — Processed 415 frames\n",
      "\n",
      "========== STARTED: v11 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffe2a4bb18f4bcaa5ca167fda0aec7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v11:   0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ DONE: v11 — Processed 1242 frames\n",
      "\n",
      "========== STARTED: v12 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d1c3a040d741ba98f812219e002b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v12:   0%|          | 0/1122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ DONE: v12 — Processed 1122 frames\n",
      "\n",
      "========== STARTED: v2 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26151f676ff74f5294718cfc7c213407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v2:   0%|          | 0/778 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_pt_path = find_best_pt()\n",
    "process_all_videos(yolo_weights_path=best_pt_path, class_id_to_name=class_id_to_name, video_dir='./videos', output_base='./opt', max_frames=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a009c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built CUDA Version: None\n",
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print the CUDA version PyTorch is built with\n",
    "print(\"Built CUDA Version:\", torch.version.cuda)\n",
    "\n",
    "# Print the CUDA version runtime (if CUDA is available)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Runtime Version:\", torch._C._cuda_getCompiledVersion())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b094c750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
