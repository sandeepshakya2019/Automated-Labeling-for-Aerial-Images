{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074d4a4f",
   "metadata": {},
   "source": [
    "## Builting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150d5af",
   "metadata": {},
   "source": [
    "### Cuda Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "59ec5504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built CUDA Version: 12.4\n",
      "CUDA Runtime Version: 12040\n",
      "GPU Name: NVIDIA GeForce RTX 3050 OEM\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print the CUDA version that PyTorch was built with\n",
    "print(\"Built CUDA Version:\", torch.version.cuda)\n",
    "\n",
    "# Check if CUDA is available on the current system\n",
    "if torch.cuda.is_available():\n",
    "    # Print the CUDA runtime version (compiled version)\n",
    "    print(\"CUDA Runtime Version:\", torch._C._cuda_getCompiledVersion())\n",
    "    \n",
    "    # Print the name of the first available GPU\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    # Inform the user if CUDA is not available\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38dfa47",
   "metadata": {},
   "source": [
    "### Mapping Classes and their colors with class id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "206e74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of class IDs to class names and their corresponding RGB color codes\n",
    "class_id_to_name = {\n",
    "    0:  ('unlabeled', [28, 42, 168]),         # Background or unclassified area\n",
    "    1:  ('pool', [0, 50, 89]),                # Swimming pool\n",
    "    2:  ('vegetation', [107, 142, 35]),       # Trees, grass, or bushes\n",
    "    3:  ('roof', [70, 70, 70]),               # Building rooftops\n",
    "    4:  ('wall', [102, 102, 156]),            # Building walls\n",
    "    5:  ('window', [254, 228, 12]),           # Windows\n",
    "    6:  ('person', [255, 22, 96]),            # People\n",
    "    7:  ('dog', [102, 51, 0]),                # Dogs\n",
    "    8:  ('car', [9, 143, 150]),               # Cars\n",
    "    9:  ('bicycle', [119, 11, 32]),           # Bicycles\n",
    "    10: ('tree', [51, 51, 0]),                # Trees\n",
    "    11: ('truck', [160, 160, 60]),            # Trucks (added)\n",
    "    12: ('bus', [200, 80, 80]),               # Buses (added)\n",
    "    13: ('vehicle', [20, 80, 80]),            # General vehicle category (added)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52746b61",
   "metadata": {},
   "source": [
    "### Install and Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f7b06f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NumPy - fundamental package for numerical computations\n",
    "# !pip install numpy\n",
    "\n",
    "# Install OpenCV - library for computer vision tasks\n",
    "# !pip install opencv-python\n",
    "\n",
    "# Install Pillow - image processing library\n",
    "# !pip install pillow\n",
    "\n",
    "# Install Matplotlib - plotting and visualization library\n",
    "# !pip install matplotlib\n",
    "\n",
    "# Install tqdm - progress bar utility\n",
    "# !pip install tqdm\n",
    "\n",
    "# Install scikit-learn - machine learning tools\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# Install PyTorch and TorchVision - deep learning framework and its vision tools\n",
    "# !pip install torch torchvision\n",
    "\n",
    "# Install Ultralytics - YOLO model implementation and training tools\n",
    "# !pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cdfb2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os                     # Operating system interfaces\n",
    "import gc                     # Garbage collection interface\n",
    "import json                   # Working with JSON data\n",
    "import shutil                 # File operations like copy, move, etc.\n",
    "import zipfile                # Extracting zip archives\n",
    "import random                 # Random number generation\n",
    "from glob import glob         # Pattern matching for file paths\n",
    "from pathlib import Path      # Object-oriented file path handling\n",
    "from collections import defaultdict  # Dictionary with default value support\n",
    "import xml.etree.ElementTree as ET  # Parsing XML files\n",
    "\n",
    "# Scientific computing and data manipulation\n",
    "import numpy as np            # Numerical operations\n",
    "import pandas as pd           # Data analysis and manipulation\n",
    "from sklearn.model_selection import train_test_split  # Train-test split\n",
    "\n",
    "# Image processing and visualization\n",
    "import cv2                    # OpenCV for computer vision\n",
    "from PIL import Image, ImageDraw, ImageFont  # PIL for image handling\n",
    "import matplotlib.pyplot as plt              # Plotting library\n",
    "import matplotlib.patches as mpatches        # Drawing patches on plots\n",
    "\n",
    "# Progress bar utility\n",
    "from tqdm.auto import tqdm    # Progress bars for loops\n",
    "\n",
    "# PyTorch and related imports\n",
    "import torch\n",
    "import torch.nn as nn         # Neural network modules\n",
    "from torch.utils.data import DataLoader       # Efficient data loading\n",
    "import torchvision.models as models           # Pretrained models\n",
    "import torchvision.transforms as transforms   # Image transformations\n",
    "import torchvision.models.segmentation as segmentation  # Segmentation models\n",
    "\n",
    "# YOLO from Ultralytics\n",
    "from ultralytics import YOLO  # YOLO object detection models\n",
    "\n",
    "# Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Google Drive downloader\n",
    "import gdown                  # Downloading files from Google Drive\n",
    "\n",
    "# Environment configuration\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"   # Avoids OpenMP duplicate library error\n",
    "\n",
    "# Set device for computation (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd42d4",
   "metadata": {},
   "source": [
    "### Dataset Download Functions\n",
    "\n",
    "These functions allow you to download and extract datasets from Google Drive shared URLs. They handle the download and extraction of ZIP files for the **Semantic Drone Dataset** and the **UAVDT Dataset**.\n",
    "\n",
    "#### `semantic_drone_dataset_download`\n",
    "\n",
    "This function downloads and extracts the **Semantic Drone Dataset** from a Google Drive URL.\n",
    "\n",
    "#### `uavdt_dataset_download`\n",
    "This function downloads and extracts the UAVDT Dataset from a Google Drive URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3f810e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_drone_dataset_download(gdrive_url, extract_to=\"extracted\"):\n",
    "    \"\"\"\n",
    "    Downloads and extracts the Semantic Drone Dataset from a Google Drive URL.\n",
    "    \n",
    "    Parameters:\n",
    "        gdrive_url (str): The shared Google Drive link to the ZIP file.\n",
    "        extract_to (str): Directory to extract contents into. Default is 'extracted'.\n",
    "    \"\"\"\n",
    "    # Extract the file ID from the Google Drive shareable URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[INFO] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[INFO] Extracting ZIP...\")\n",
    "    # Extract contents of the ZIP file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Delete the ZIP file after extraction to save space\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[DONE] Extracted files to: {extract_to}\")\n",
    "\n",
    "\n",
    "def uavdt_dataset_download(gdrive_url, extract_to=\"extracted\"):\n",
    "    \"\"\"\n",
    "    Downloads and extracts the UAVDT Dataset from a Google Drive URL.\n",
    "    \n",
    "    Parameters:\n",
    "        gdrive_url (str): The shared Google Drive link to the ZIP file.\n",
    "        extract_to (str): Directory to extract contents into. Default is 'extracted'.\n",
    "    \"\"\"\n",
    "    # Extract the file ID from the Google Drive shareable URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[+] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[+] Extracting ZIP...\")\n",
    "    # Extract contents of the ZIP file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Delete the ZIP file after extraction to save space\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[+] Extracted files to: {extract_to}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ff73f",
   "metadata": {},
   "source": [
    "### YOLO Dataset Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09011805",
   "metadata": {},
   "source": [
    "#### Semantic Dorne Datasets\n",
    "\n",
    "1. **`parse_yolo_style_bbox_from_xml`**: Parses XML annotations and converts polygon objects to YOLO-style bounding boxes.\n",
    "2. **`save_yolo_format`**: Saves bounding boxes in YOLO format (normalized coordinates: `<class_id> <x_center> <y_center> <width> <height>`).\n",
    "3. **`convert_fulldataset_yolo_only`**: Converts a full dataset of images and XML annotations to YOLO format and saves them to the specified output directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "30ca98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Parse polygon and convert to YOLO bbox\n",
    "# ----------------------------\n",
    "\n",
    "# Parses XML annotation and converts polygon objects to YOLO-style bounding boxes\n",
    "def parse_yolo_style_bbox_from_xml(xml_path, class_id_to_name):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "        if class_name in [value[0] for value in class_id_to_name.values()]:\n",
    "            polygon = obj.find('polygon')\n",
    "            if polygon is not None:\n",
    "                # Extract points from polygon\n",
    "                points = polygon.findall('pt')\n",
    "                coords = [(float(pt.find('x').text), float(pt.find('y').text)) for pt in points]\n",
    "                # Convert polygon to bounding box\n",
    "                x_min = min(coord[0] for coord in coords)\n",
    "                y_min = min(coord[1] for coord in coords)\n",
    "                x_max = max(coord[0] for coord in coords)\n",
    "                y_max = max(coord[1] for coord in coords)\n",
    "                bboxes.append(((x_min, y_min), (x_max, y_max), class_name))\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Save YOLO-format txt\n",
    "# ----------------------------\n",
    "\n",
    "# Saves the bounding boxes in YOLO format: <class_id> <x_center> <y_center> <width> <height>\n",
    "def save_yolo_format(image_id, bboxes, image_width, image_height, output_path, class_id_to_name):\n",
    "    with open(output_path, 'w') as f:\n",
    "        for (x_min, y_min), (x_max, y_max), class_name in bboxes:\n",
    "            class_id = next(cid for cid, (name, _) in class_id_to_name.items() if name == class_name)\n",
    "            x_center = (x_min + x_max) / 2 / image_width\n",
    "            y_center = (y_min + y_max) / 2 / image_height\n",
    "            width = (x_max - x_min) / image_width\n",
    "            height = (y_max - y_min) / image_height\n",
    "            # Write to file with six decimal precision\n",
    "            f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Convert dataset (YOLO only)\n",
    "# ----------------------------\n",
    "\n",
    "# Converts the full dataset by extracting YOLO-style annotations and saving them\n",
    "def convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name):\n",
    "    # Get list of image IDs (without extension)\n",
    "    image_ids = [img.split('.')[0] for img in os.listdir(f\"{dataset_path}/images\") if img.endswith(\".jpg\")]\n",
    "\n",
    "    # Create output folders\n",
    "    os.makedirs(f\"{output_dir}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/labels\", exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(image_ids, desc=\"Converting to YOLO\"):\n",
    "        img_path = f\"{dataset_path}/images/{image_id}.jpg\"\n",
    "        bbox_xml_path = f\"{dataset_path}/gt/bounding_box/label_me_xml/{image_id}.xml\"\n",
    "        semantic_xml_path = f\"{dataset_path}/gt/semantic/label_me_xml/{image_id}.xml\"\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"[WARNING] Image not found: {img_path}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Parse bounding box and semantic annotations\n",
    "            bboxes1 = parse_yolo_style_bbox_from_xml(bbox_xml_path, class_id_to_name)\n",
    "            bboxes2 = parse_yolo_style_bbox_from_xml(semantic_xml_path, class_id_to_name)\n",
    "            all_bboxes = bboxes1 + bboxes2\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Skipping image {image_id} due to parse error: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(img_path)\n",
    "            image_np = np.array(image)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not load image {image_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save image to output directory\n",
    "        image.save(f\"{output_dir}/images/{image_id}.jpg\")\n",
    "\n",
    "        # Save YOLO-format labels to output directory\n",
    "        yolo_annotation_path = f\"{output_dir}/labels/{image_id}.txt\"\n",
    "        save_yolo_format(image_id, all_bboxes, image_np.shape[1], image_np.shape[0], yolo_annotation_path, class_id_to_name)\n",
    "\n",
    "    print(\"[+] YOLO-format annotation conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47eb3c",
   "metadata": {},
   "source": [
    "#### UAVDT Datasets\n",
    "\n",
    "1. **`convert_dataset`**: Converts UAVDT annotation files to YOLO format, mapping original class IDs to extended IDs and saving them with normalized bounding box coordinates.\n",
    "2. **`copy_split_sequences`**: Splits the dataset into training and validation sets, copying the corresponding images and YOLO-format label files to separate directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3a94cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Mapping UAVDT class IDs to extended class IDs used in the combined dataset\n",
    "uavdt_to_extended = {\n",
    "    0: 8,   # car\n",
    "    1: 11,  # truck\n",
    "    2: 12,  # bus\n",
    "    3: 13   # other vehicle\n",
    "}\n",
    "\n",
    "# === Function to convert a single annotation file to YOLO format ===\n",
    "def convert_annotation(anno_path, label_path, image_path, stats):\n",
    "    if not os.path.exists(image_path):\n",
    "        stats[\"missing_image\"] += 1\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        height, width = img.shape[:2]\n",
    "    except:\n",
    "        stats[\"missing_image\"] += 1\n",
    "        return\n",
    "\n",
    "    with open(anno_path, 'r') as fin, open(label_path, 'w') as fout:\n",
    "        for line in fin:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) < 8:\n",
    "                stats[\"malformed\"] += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Parse bounding box and class info\n",
    "                x, y, w, h = map(float, parts[0:4])\n",
    "                original_cls = int(parts[5])\n",
    "\n",
    "                # Skip classes not in our mapping\n",
    "                if original_cls not in uavdt_to_extended:\n",
    "                    stats[\"skipped\"][original_cls] += 1\n",
    "                    continue\n",
    "\n",
    "                # Convert to new class ID\n",
    "                cls = uavdt_to_extended[original_cls]\n",
    "\n",
    "                # Convert to YOLO format (normalized center_x, center_y, width, height)\n",
    "                x_center = (x + w / 2) / width\n",
    "                y_center = (y + h / 2) / height\n",
    "                w /= width\n",
    "                h /= height\n",
    "\n",
    "                # Validate normalized coordinates\n",
    "                if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and w > 0 and h > 0):\n",
    "                    stats[\"skipped\"][cls] += 1\n",
    "                    continue\n",
    "\n",
    "                # Write label line\n",
    "                fout.write(f\"{cls} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "                stats[\"converted\"] += 1\n",
    "            except Exception:\n",
    "                stats[\"malformed\"] += 1\n",
    "                continue\n",
    "\n",
    "            stats[\"total\"] += 1\n",
    "\n",
    "# === Step 1: Convert all UAVDT annotations to YOLO format ===\n",
    "def convert_dataset(root_dir):\n",
    "    # Find all annotation files inside any Mxxxx/annotations/ directory\n",
    "    annotation_paths = glob(os.path.join(root_dir, \"M*/annotations/*.txt\"))\n",
    "    total_files = len(annotation_paths)\n",
    "\n",
    "    # Stats for tracking issues and progress\n",
    "    stats = {\n",
    "        \"total\": 0,\n",
    "        \"converted\": 0,\n",
    "        \"malformed\": 0,\n",
    "        \"missing_image\": 0,\n",
    "        \"skipped\": defaultdict(int)\n",
    "    }\n",
    "\n",
    "    print(f\"ðŸ”„ Converting {total_files} annotation files to YOLO format...\")\n",
    "\n",
    "    for anno_path in tqdm(annotation_paths, desc=\"Converting\", unit=\"file\"):\n",
    "        # Get sequence directory (e.g., M0101)\n",
    "        sequence_dir = os.path.dirname(os.path.dirname(anno_path))\n",
    "        file_name = os.path.basename(anno_path)\n",
    "\n",
    "        # Output label directory\n",
    "        label_dir = os.path.join(sequence_dir, \"labels\")\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "        # Output label file path\n",
    "        label_path = os.path.join(label_dir, file_name)\n",
    "\n",
    "        # Corresponding image path\n",
    "        image_name = file_name.replace(\".txt\", \".jpg\")\n",
    "        image_path = os.path.join(sequence_dir, \"images\", image_name)\n",
    "\n",
    "        # Perform the actual conversion\n",
    "        convert_annotation(anno_path, label_path, image_path, stats)\n",
    "\n",
    "    # Print summary of the conversion process\n",
    "    print(\"\\nConversion complete.\")\n",
    "    print(f\"Total boxes:     {stats['total']}\")\n",
    "    print(f\"Converted boxes: {stats['converted']}\")\n",
    "    print(f\"Skipped boxes:   {sum(stats['skipped'].values())}\")\n",
    "    for cls, count in sorted(stats[\"skipped\"].items()):\n",
    "        print(f\"   - Skipped class {cls}: {count}\")\n",
    "    print(f\"Malformed lines: {stats['malformed']}\")\n",
    "    print(f\"Missing images:  {stats['missing_image']}\")\n",
    "\n",
    "# === Step 2: Split dataset into train/val and copy files ===\n",
    "def copy_split_sequences(src_root, dst_root, train_ratio=0.8):\n",
    "    # Find all sequences (Mxxxx folders)\n",
    "    all_sequences = sorted(glob(os.path.join(src_root, \"M*\")))\n",
    "\n",
    "    # Split into training and validation sequences\n",
    "    train_seqs, val_seqs = train_test_split(all_sequences, train_size=train_ratio, random_state=42)\n",
    "\n",
    "    # Copy files into respective folders\n",
    "    for split_name, split_list in zip(['train', 'val'], [train_seqs, val_seqs]):\n",
    "        for seq_path in tqdm(split_list, desc=f\"Copying {split_name}\"):\n",
    "            images_src = os.path.join(seq_path, \"images\")\n",
    "            labels_src = os.path.join(seq_path, \"labels\")\n",
    "\n",
    "            images_dst = os.path.join(dst_root, split_name, \"images\")\n",
    "            labels_dst = os.path.join(dst_root, split_name, \"labels\")\n",
    "\n",
    "            os.makedirs(images_dst, exist_ok=True)\n",
    "            os.makedirs(labels_dst, exist_ok=True)\n",
    "\n",
    "            # Copy image files\n",
    "            for img_file in glob(os.path.join(images_src, \"*.jpg\")):\n",
    "                shutil.copy(img_file, os.path.join(images_dst, os.path.basename(img_file)))\n",
    "\n",
    "            # Copy label files\n",
    "            for label_file in glob(os.path.join(labels_src, \"*.txt\")):\n",
    "                shutil.copy(label_file, os.path.join(labels_dst, os.path.basename(label_file)))\n",
    "\n",
    "    print(\"\\n[+] Dataset split into 'train/' and 'val/' folders with images and YOLO-format labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b9227",
   "metadata": {},
   "source": [
    "#### Convert into train and Val Sets\n",
    "\n",
    "This script converts UAVDT annotations to YOLO format, maps class IDs, normalizes bounding boxes, and splits the dataset into training and validation sets, organizing images and label files into respective directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "36ec4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to move files from source directories to target directories\n",
    "def move_files(file_list, \n",
    "               source_image_dir, \n",
    "               source_annotation_dir,\n",
    "               target_image_dir, \n",
    "               target_annotation_dir):\n",
    "    \n",
    "    # Create target directories if they don't exist\n",
    "    os.makedirs(target_image_dir, exist_ok=True)\n",
    "    os.makedirs(target_annotation_dir, exist_ok=True)\n",
    "\n",
    "    # Loop through each file in the provided list and move the corresponding image and annotation\n",
    "    for image_id in tqdm(file_list, desc=f\"Moving to {os.path.basename(os.path.dirname(target_image_dir))}\"):\n",
    "        # Construct paths for the image and annotation\n",
    "        image_path = os.path.join(source_image_dir, f\"{image_id}.jpg\")\n",
    "        annotation_path = os.path.join(source_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        # Construct target paths for the image and annotation\n",
    "        target_image_path = os.path.join(target_image_dir, f\"{image_id}.jpg\")\n",
    "        target_annotation_path = os.path.join(target_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        # Check if both the image and annotation files exist, then copy them to target directories\n",
    "        if os.path.exists(image_path) and os.path.exists(annotation_path):\n",
    "            shutil.copy(image_path, target_image_path)\n",
    "            shutil.copy(annotation_path, target_annotation_path)\n",
    "\n",
    "# Function to split the dataset into training and validation sets, and move the files\n",
    "def split_and_move_dataset(source_base_dir=\"./datasets/semantic_yolo\",\n",
    "                           target_base_dir=\"./datasets/new_dataset_yolo_split\",\n",
    "                           split_ratio=0.8,\n",
    "                           seed=42):\n",
    "    \n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Define paths for the image and label directories\n",
    "    image_dir = os.path.join(source_base_dir, \"images\")\n",
    "    label_dir = os.path.join(source_base_dir, \"labels\")\n",
    "\n",
    "    # Get all image IDs (file names without extensions) from the image directory\n",
    "    image_ids = [os.path.splitext(f)[0] for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    # Shuffle the image IDs to randomize the split\n",
    "    random.shuffle(image_ids)\n",
    "\n",
    "    # Determine the split index based on the split ratio\n",
    "    split_idx = int(len(image_ids) * split_ratio)\n",
    "\n",
    "    # Split the image IDs into training and validation sets\n",
    "    train_ids = image_ids[:split_idx]\n",
    "    val_ids = image_ids[split_idx:]\n",
    "\n",
    "    # Move the training images and annotations to the target directories\n",
    "    move_files(train_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"train/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"train/labels\"))\n",
    "\n",
    "    # Move the validation images and annotations to the target directories\n",
    "    move_files(val_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"val/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"val/labels\"))\n",
    "\n",
    "    # Print the summary of the dataset split\n",
    "    print(f\"\\n[âœ“] Dataset split completed: {len(train_ids)} train / {len(val_ids)} val samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110fe96d",
   "metadata": {},
   "source": [
    "#### Normalize Labels\n",
    "\n",
    "This script normalizes bounding box coordinates in YOLO label files to the [0, 1] range based on the corresponding image dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "541e4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_label_file(label_file, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Normalize the label coordinates in a label file to ensure they are within [0, 1] range.\n",
    "    The label file is updated with the normalized values.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    with open(label_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            # Split the line by spaces to get the class and coordinates\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, width, height = map(float, parts[1:])\n",
    "            \n",
    "            # Normalize the coordinates to be within the range [0, 1]\n",
    "            x_center = min(1.0, max(0.0, x_center))\n",
    "            y_center = min(1.0, max(0.0, y_center))\n",
    "            width = min(1.0, max(0.0, width))\n",
    "            height = min(1.0, max(0.0, height))\n",
    "\n",
    "            # Write the normalized values back to the file\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "\n",
    "def get_image_size(img_path):\n",
    "    \"\"\"\n",
    "    Get the width and height of the image to normalize the coordinates properly.\n",
    "    This function uses PIL (Pillow) to open the image and return its dimensions.\n",
    "    \"\"\"\n",
    "    with Image.open(img_path) as img:\n",
    "        return img.size  # returns (width, height)\n",
    "\n",
    "\n",
    "def normalize_all_labels(labels_dir, img_dir):\n",
    "    \"\"\"\n",
    "    Normalize all label files in the specified directory.\n",
    "    It reads each label file, gets the corresponding image size, and normalizes the label coordinates.\n",
    "    \"\"\"\n",
    "    for label_file in tqdm(os.listdir(labels_dir)):  # Iterate over all files in the labels directory\n",
    "       \n",
    "        if label_file.endswith('.txt'):  # Process only label files\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            img_path = os.path.join(img_dir, label_file.replace('.txt', '.jpg'))  # Assuming JPG images\n",
    "            \n",
    "            if os.path.exists(img_path):\n",
    "                # Get the image dimensions to normalize the labels\n",
    "                img_width, img_height = get_image_size(img_path)\n",
    "                normalize_label_file(label_path, img_width, img_height)\n",
    "            else:\n",
    "                # Warning if the corresponding image is missing\n",
    "                print(f\"Warning: Image for label {label_file} not found!\")\n",
    "    \n",
    "    print(\"Normalization Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce6fe5",
   "metadata": {},
   "source": [
    "### Training v8 model functions\n",
    "\n",
    "This function trains a YOLO model from scratch using a specified base model variant and dataset configuration, with various data augmentation techniques and hyperparameters for effective training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ae8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "def train_yolo_from_scratch(data_yaml, epochs, imgsz, batch, name, model_variant=\"yolov8n.pt\"):\n",
    "    \"\"\"\n",
    "    Train YOLO model from scratch using a specified base model variant.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_yaml (str): Path to the YAML file containing dataset configuration.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - imgsz (int): Image size for training.\n",
    "    - batch (int): Batch size.\n",
    "    - name (str): Name for the training session, which will be used for saving results.\n",
    "    - model_variant (str): Base model to start from (default is 'yolov8n.pt').\n",
    "    \n",
    "    The function initializes a YOLO model from the specified variant, \n",
    "    then trains it on the given dataset configuration with data augmentation \n",
    "    and hyperparameters tailored for the task.\n",
    "    \"\"\"\n",
    "    print(f\"[+] Training from scratch using base model: {model_variant}\")\n",
    "    \n",
    "    # Initialize the model with the specified base model variant\n",
    "    model = YOLO(model_variant)\n",
    "\n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Start the training process with the provided parameters\n",
    "    model.train(\n",
    "        data=data_yaml,            # Path to dataset YAML configuration\n",
    "        epochs=epochs,             # Number of epochs for training\n",
    "        imgsz=imgsz,               # Image size to resize input images during training\n",
    "        batch=batch,               # Batch size\n",
    "        name=name,                 # Name for the experiment (saved in 'runs/train')\n",
    "        project=\"runs/train\",      # Directory where training results will be saved\n",
    "        augment=True,              # Whether to apply data augmentation\n",
    "        degrees=10,                # Rotation degrees for augmentation\n",
    "        scale=0.5,                 # Scale factor for augmentation\n",
    "        flipud=0.2,                # Probability of flipping the image upside down\n",
    "        fliplr=0.5,                # Probability of flipping the image left-right\n",
    "        hsv_h=0.015,               # Hue shift for HSV augmentation\n",
    "        hsv_s=0.7,                 # Saturation shift for HSV augmentation\n",
    "        hsv_v=0.4,                 # Value shift for HSV augmentation\n",
    "        mosaic=1.0,                # Mosaic augmentation probability\n",
    "        mixup=0.2,                 # Mixup augmentation probability\n",
    "        lr0=0.01,                  # Initial learning rate\n",
    "        lrf=0.01,                  # Final learning rate (multiplied by lr0)\n",
    "        verbose=True,              # Whether to print training logs\n",
    "        patience=15                # Number of epochs with no improvement before stopping\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2408215",
   "metadata": {},
   "source": [
    "### Fine-Tuning YOLO Model\n",
    "\n",
    "This function fine-tunes a pre-trained YOLO model on a new dataset, applying data augmentation and adjusting hyperparameters like learning rate and weight decay to optimize performance for the new task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2925090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def fine_tune_yolo(data_yaml, epochs, imgsz, batch, name, base_model_path):\n",
    "    \"\"\"\n",
    "    Fine-tune a pre-trained YOLO model on a new dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data_yaml (str): Path to the dataset YAML file containing the configuration.\n",
    "    - epochs (int): Number of epochs for fine-tuning.\n",
    "    - imgsz (int): Image size for training.\n",
    "    - batch (int): Batch size for training.\n",
    "    - name (str): The name of the fine-tuning experiment, which will be used for saving.\n",
    "    - base_model_path (str): Path to the pre-trained YOLO model that will be fine-tuned.\n",
    "    \n",
    "    The function loads the pre-trained YOLO model from the specified path, performs garbage \n",
    "    collection and memory clearing to avoid CUDA memory issues, and then starts the fine-tuning process \n",
    "    on the new dataset with specific settings, such as a lower learning rate and data augmentations.\n",
    "    \"\"\"\n",
    "    print(f\"[+] Fine-tuning model from: {base_model_path}\")\n",
    "    \n",
    "    # Load the pre-trained model from the specified base model path\n",
    "    model = YOLO(base_model_path)\n",
    "\n",
    "    # Perform garbage collection and clear CUDA memory to avoid out-of-memory errors\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Fine-tune the model with the specified parameters\n",
    "    model.train(\n",
    "        data=data_yaml,           # Path to dataset YAML configuration\n",
    "        epochs=epochs,            # Number of epochs for fine-tuning\n",
    "        imgsz=imgsz,              # Image size for resizing input images\n",
    "        batch=batch,              # Batch size for training\n",
    "        name=name,                # Name for the fine-tuning experiment (saved in 'runs/train')\n",
    "        project=\"runs/train\",     # Directory where the results of the fine-tuning will be stored\n",
    "        weight_decay=0.0005,      # Weight decay parameter to prevent overfitting\n",
    "        augment=True,             # Enable data augmentation\n",
    "        degrees=10,               # Rotate images by up to 10 degrees\n",
    "        scale=0.5,                # Scale images by 50% during augmentation\n",
    "        flipud=0.2,               # 20% chance of flipping images vertically\n",
    "        fliplr=0.5,               # 50% chance of flipping images horizontally\n",
    "        hsv_h=0.015,              # Adjust hue by +/- 1.5% during augmentation\n",
    "        hsv_s=0.7,                # Adjust saturation by +/- 70% during augmentation\n",
    "        hsv_v=0.4,                # Adjust brightness by +/- 40% during augmentation\n",
    "        mosaic=1.0,               # Apply mosaic augmentation with 100% probability\n",
    "        mixup=0.2,                # Mixup augmentation with 20% probability\n",
    "        patience=20,              # Early stopping patience (wait 10 epochs without improvement)\n",
    "        verbose=True,             # Display detailed training logs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ff318",
   "metadata": {},
   "source": [
    "### YOLO Model Evaluation and Metrics Extraction\n",
    "\n",
    "This script provides functions for evaluating a YOLO model on a dataset, extracting per-class mAP@0.5:0.95 metrics, and saving them to a JSON file. Below are the key functions:\n",
    "\n",
    "- **load_yolo_model**: Loads a pre-trained YOLO model from the specified path.\n",
    "- **run_model_validation**: Runs the validation for the loaded YOLO model and returns the results.\n",
    "- **extract_per_class_metrics**: Extracts the mAP@0.5:0.95 metrics for each class from the model validation results.\n",
    "- **save_metrics_to_json**: Saves the extracted metrics in a JSON file for easy access and further analysis.\n",
    "- **evaluate_and_save_metrics**: A high-level function that loads the model, validates it, extracts metrics, and saves them to a JSON file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bf85d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def load_yolo_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained YOLO model from the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path (str): Path to the pre-trained YOLO model file.\n",
    "\n",
    "    Returns:\n",
    "    - YOLO model object.\n",
    "    \"\"\"\n",
    "    return YOLO(model_path)\n",
    "\n",
    "def run_model_validation(model):\n",
    "    \"\"\"\n",
    "    Runs the model validation and returns the results.\n",
    "\n",
    "    Parameters:\n",
    "    - model (YOLO): The YOLO model to be validated.\n",
    "\n",
    "    Returns:\n",
    "    - results (Result object): Validation results from the model.\n",
    "    \"\"\"\n",
    "    return model.val()\n",
    "\n",
    "def extract_per_class_metrics(results):\n",
    "    \"\"\"\n",
    "    Extracts mAP@0.5:0.95 for each class from the validation results.\n",
    "\n",
    "    Parameters:\n",
    "    - results (Result object): Validation results from the YOLO model.\n",
    "\n",
    "    Returns:\n",
    "    - per_class_metrics (dict): Dictionary containing per-class mAP@0.5:0.95 values.\n",
    "    \"\"\"\n",
    "    per_class_metrics = {}\n",
    "    if hasattr(results.box, 'maps') and results.box.maps is not None:\n",
    "        maps = results.box.maps  # NumPy array of mAP@0.5:0.95 for each class\n",
    "        for i, name in results.names.items():\n",
    "            per_class_metrics[name] = {\n",
    "                \"class_id\": i,\n",
    "                \"mAP@0.5:0.95\": round(float(maps[i]), 4)  # Round to 4 decimal places\n",
    "            }\n",
    "    else:\n",
    "        print(\"[-] No per-class mAP@0.5:0.95 data found.\")\n",
    "    return per_class_metrics\n",
    "\n",
    "def save_metrics_to_json(metrics, output_path):\n",
    "    \"\"\"\n",
    "    Saves the per-class metrics to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics (dict): The metrics to be saved, typically containing per-class mAP values.\n",
    "    - output_path (str): Path to the output JSON file where the metrics will be saved.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)  # Save as JSON with indents for readability\n",
    "    print(f\"[+] Saved per-class metrics to {output_path}\")\n",
    "\n",
    "def evaluate_and_save_metrics(model_path, output_json_path=\"per_class_metrics.json\"):\n",
    "    \"\"\"\n",
    "    Evaluates the YOLO model and saves the per-class mAP metrics to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path (str): Path to the pre-trained YOLO model.\n",
    "    - output_json_path (str): Path to save the output JSON file with per-class mAP values.\n",
    "    \"\"\"\n",
    "    model = load_yolo_model(model_path)  # Load the YOLO model\n",
    "    results = run_model_validation(model)  # Run validation\n",
    "    metrics = extract_per_class_metrics(results)  # Extract per-class metrics\n",
    "    save_metrics_to_json(metrics, output_json_path)  # Save metrics to JSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a2343",
   "metadata": {},
   "source": [
    "### Print Per-Class mAP Metrics\n",
    "\n",
    "This function loads the per-class mAP@0.5:0.95 metrics from a JSON file and prints them in a tabular format.\n",
    "\n",
    "- **print_per_class_metrics**: \n",
    "    - **Parameters**: \n",
    "        - `json_path` (str): The path to the JSON file containing the per-class metrics (default is `\"per_class_metrics.json\"`).\n",
    "    - **Functionality**:\n",
    "        - Loads the metrics from the provided JSON file.\n",
    "        - Prints a header and iterates through each class to display the class name, class ID, and corresponding mAP value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "65ca8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_per_class_metrics(json_path=\"per_class_metrics.json\"):\n",
    "    \"\"\"\n",
    "    Prints the per-class mAP@0.5:0.95 metrics from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - json_path (str): Path to the JSON file containing the per-class metrics.\n",
    "    \"\"\"\n",
    "    # Load the metrics from the specified JSON file\n",
    "    with open(json_path, \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    # Print the header for the table\n",
    "    print(\"Per-Class mAP@0.5:0.95 Metrics:\\n\")\n",
    "    print(f\"{'Class Name':<15} {'Class ID':<10} {'mAP@0.5:0.95':<15}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Loop through each class in the metrics and print the results\n",
    "    for name, data in metrics.items():\n",
    "        print(f\"{name:<15} {data['class_id']:<10} {data['mAP@0.5:0.95']:<15}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22b1a8",
   "metadata": {},
   "source": [
    "### Find Best YOLO Model\n",
    "\n",
    "- **find_best_model(base_dir='runs_yolo/')**: \n",
    "    - Searches for the most recently modified `best.pt` file in the given directory and subdirectories.\n",
    "    - Returns the path to the latest `best.pt` file.\n",
    "    - Raises `FileNotFoundError` if no `best.pt` file is found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d0b390e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_best_model(base_dir='runs_yolo/'):\n",
    "    \"\"\"\n",
    "    Searches for the 'best.pt' model file in the given directory and its subdirectories.\n",
    "\n",
    "    Parameters:\n",
    "    - base_dir (str): The directory where the search will start. Defaults to 'runs_yolo/'.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the 'best.pt' model file.\n",
    "    \n",
    "    Raises:\n",
    "    - FileNotFoundError: If no 'best.pt' file is found in the directory.\n",
    "    \"\"\"\n",
    "    # Use Path.rglob to recursively search for all 'best.pt' files in the directory\n",
    "    best_paths = list(Path(base_dir).rglob('best.pt'))\n",
    "    \n",
    "    # Check if any 'best.pt' file was found\n",
    "    if not best_paths:\n",
    "        raise FileNotFoundError(\"No 'best.pt' file found in the 'runs/' directory.\")\n",
    "    \n",
    "    # Optionally, sort the found files by their last modified time (descending)\n",
    "    best_paths.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    # Print the path of the most recently modified 'best.pt' file\n",
    "    print(f\"[+] Found best.pt at: {best_paths[0]}\")\n",
    "    \n",
    "    # Return the path to the 'best.pt' file\n",
    "    return str(best_paths[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378bc85",
   "metadata": {},
   "source": [
    "### Prediction on Videos\n",
    "\n",
    "1. **Frame Processing**  \n",
    "\n",
    "2. **Video Loop & Saving**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a013ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FRAME PROCESSING ==========\n",
    "\n",
    "import cv2\n",
    "\n",
    "def process_frame_1(frame, yolo_model, w, h, class_id_to_name, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Process a single frame from a video, run YOLO model inference, and annotate the frame.\n",
    "\n",
    "    Parameters:\n",
    "    - frame: The video frame to process.\n",
    "    - yolo_model: The YOLO model used for object detection.\n",
    "    - w, h: The width and height of the frame (image size).\n",
    "    - class_id_to_name: A dictionary mapping class IDs to class names and colors.\n",
    "    - conf_threshold: The confidence threshold for filtering detections.\n",
    "\n",
    "    Returns:\n",
    "    - annotated: The annotated frame with bounding boxes and labels.\n",
    "    - boxes: The bounding box coordinates for each detected object.\n",
    "    - class_ids: The class IDs for each detected object.\n",
    "    \"\"\"\n",
    "    annotated = frame.copy()  # Make a copy of the original frame for annotation\n",
    "    results = yolo_model(annotated, verbose=False)[0]  # Run YOLO inference on the frame\n",
    "    boxes = results.boxes.xyxy.cpu().numpy()  # Get bounding box coordinates (x1, y1, x2, y2)\n",
    "    class_ids = results.boxes.cls.cpu().numpy()  # Get class IDs for detected objects\n",
    "    confidences = results.boxes.conf.cpu().numpy()  # Confidence scores for each box\n",
    "\n",
    "    for box, cls_id, confidence in zip(boxes, class_ids, confidences):\n",
    "        if confidence > conf_threshold:  # Filter detections based on confidence\n",
    "            x1, y1, x2, y2 = map(int, box)  # Convert box coordinates to integers\n",
    "            class_name, color = class_id_to_name[int(cls_id)]  # Get class name and color based on class ID\n",
    "            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)  # Draw bounding box\n",
    "            cv2.putText(annotated, f\"{class_name} {confidence:.2f}\", (x1, max(y1 - 10, 10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)  # Add label and confidence text\n",
    "\n",
    "    return annotated, boxes, class_ids\n",
    "\n",
    "\n",
    "# ========== VIDEO CAPTURE ==========\n",
    "\n",
    "def setup_video_capture_1(video_path):\n",
    "    \"\"\"\n",
    "    Set up video capture for a given video file and return video properties.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path: Path to the video file.\n",
    "\n",
    "    Returns:\n",
    "    - cap: OpenCV video capture object.\n",
    "    - total_frames: Total number of frames in the video.\n",
    "    - fps: Frames per second of the video.\n",
    "    - w, h: Width and height of the video frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)  # Open the video file\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Get total number of frames\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Get frames per second\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Get frame width\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Get frame height\n",
    "    return cap, total_frames, fps, w, h\n",
    "\n",
    "\n",
    "# ========== MAIN FUNCTION ==========\n",
    "\n",
    "def videos_predictions(yolo_weights_path, class_id_to_name, video_dir='videos', output_base='./datatsets/opt', max_frames=None):\n",
    "    \"\"\"\n",
    "    Process multiple video files, run YOLO inference, and save the results.\n",
    "\n",
    "    Parameters:\n",
    "    - yolo_weights_path: Path to the YOLO weights file.\n",
    "    - class_id_to_name: A dictionary mapping class IDs to class names and colors.\n",
    "    - video_dir: Directory containing the input video files.\n",
    "    - output_base: Base directory where output images, labels, and videos will be saved.\n",
    "    - max_frames: The maximum number of frames to process per video. If None, all frames will be processed.\n",
    "    \"\"\"\n",
    "    yolo_model = YOLO(yolo_weights_path)  # Load the YOLO model\n",
    "\n",
    "    # Set up output directories for images, labels, and output video\n",
    "    image_out_dir = os.path.join(output_base, 'images')\n",
    "    label_out_dir = os.path.join(output_base, 'labels')\n",
    "    output_video_dir = os.path.join(output_base, 'output')\n",
    "\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(image_out_dir, exist_ok=True)\n",
    "    os.makedirs(label_out_dir, exist_ok=True)\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over video files in the video directory\n",
    "    for video_file in tqdm(sorted(os.listdir(video_dir))):\n",
    "        if not video_file.lower().endswith(\".mp4\"):\n",
    "            continue  # Skip non-video files\n",
    "\n",
    "        video_id = os.path.splitext(video_file)[0]  # Extract video ID (filename without extension)\n",
    "        video_path = os.path.join(video_dir, video_file)  # Full path to the video file\n",
    "        output_video_path = os.path.join(output_video_dir, f\"{video_id}.mp4\")  # Path for output video\n",
    "\n",
    "        print(f\"========== STARTED: {video_id} ==========\")\n",
    "        cap, total_frames, fps, w, h = setup_video_capture_1(video_path)  # Set up video capture\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Video writer codec\n",
    "        writer = cv2.VideoWriter(output_video_path, fourcc, fps, (w, h))  # Video writer setup\n",
    "\n",
    "        frame_count = 0\n",
    "        pbar = tqdm(total=max_frames if max_frames else total_frames, desc=video_id)  # Progress bar\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or (max_frames and frame_count >= max_frames):\n",
    "                break  # Exit if no frame is read or max_frames is reached\n",
    "\n",
    "            # Process the current frame\n",
    "            annotated_bgr, boxes, class_ids = process_frame_1(frame, yolo_model, w, h, class_id_to_name)\n",
    "\n",
    "            # Save original frame as image\n",
    "            img_filename = f'{video_id}_{frame_count:04d}.jpg'\n",
    "            img_path = os.path.join(image_out_dir, img_filename)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "\n",
    "            # Save YOLO-format label file\n",
    "            label_filename = f'{video_id}_{frame_count:04d}.txt'\n",
    "            label_path = os.path.join(label_out_dir, label_filename)\n",
    "            with open(label_path, 'w') as f:\n",
    "                for box, cls_id in zip(boxes, class_ids):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    w_box = x2 - x1\n",
    "                    h_box = y2 - y1\n",
    "                    cx = x1 + w_box / 2\n",
    "                    cy = y1 + h_box / 2\n",
    "                    f.write(f\"{int(cls_id)} {cx/w:.6f} {cy/h:.6f} {w_box/w:.6f} {h_box/h:.6f}\\n\")\n",
    "\n",
    "            writer.write(annotated_bgr)  # Write the annotated frame to the output video\n",
    "            frame_count += 1\n",
    "            pbar.update(1)  # Update progress bar\n",
    "\n",
    "        cap.release()  # Release the video capture object\n",
    "        writer.release()  # Release the video writer object\n",
    "        pbar.close()  # Close the progress bar\n",
    "        print(f\"DONE: {video_id} â€” Processed {frame_count} frames\")  # Print processing summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc84910d",
   "metadata": {},
   "source": [
    "### YOLO Video Inference \n",
    "\n",
    "1. **Frame Processing with Filtering**  \n",
    "   `process_frame()` runs YOLO inference on each frame, filters by confidence and specific class IDs (`underrepresented_class_ids`), and draws bounding boxes with labels.\n",
    "\n",
    "2. **Batch Video Processing & Saving**  \n",
    "   `process_all_videos()` loops through videos, processes each frame, and saves:\n",
    "   - Original frame (`.jpg`)\n",
    "   - YOLO label (`.txt`)\n",
    "   - Annotated output video (`.mp4`)  \n",
    "   It also tracks total frames, bounding boxes, and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4d5d77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# ========== FRAME PROCESSING ==========\n",
    "def process_frame(frame, yolo_model, w, h, class_id_to_name, valid_class_ids, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Process each frame, run inference, and annotate with bounding boxes and class labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - frame: Input video frame\n",
    "    - yolo_model: YOLO model for inference\n",
    "    - w, h: Width and height of the frame\n",
    "    - class_id_to_name: Mapping of class IDs to class names and colors\n",
    "    - valid_class_ids: List of valid class IDs to filter\n",
    "    - conf_threshold: Confidence threshold for valid detections\n",
    "    \n",
    "    Returns:\n",
    "    - annotated: Annotated frame with bounding boxes and labels\n",
    "    - filtered_boxes: List of bounding boxes for valid detections\n",
    "    - filtered_ids: List of class IDs for valid detections\n",
    "    \"\"\"\n",
    "    annotated = frame.copy()\n",
    "    results = yolo_model(annotated, verbose=False)[0]\n",
    "\n",
    "    # Filter detections based on confidence\n",
    "    mask = results.boxes.conf > conf_threshold\n",
    "    boxes = results.boxes.xyxy[mask].cpu().numpy()\n",
    "    class_ids = results.boxes.cls[mask].cpu().numpy()\n",
    "    confs = results.boxes.conf[mask].cpu().numpy()\n",
    "\n",
    "    filtered_boxes, filtered_ids = [], []\n",
    "\n",
    "    # Process each detection\n",
    "    for box, cls_id, conf in zip(boxes, class_ids, confs):\n",
    "        if int(cls_id) in valid_class_ids:\n",
    "            filtered_boxes.append(box)\n",
    "            filtered_ids.append(cls_id)\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            class_name, color = class_id_to_name[int(cls_id)]\n",
    "            label = f\"{class_name} {conf:.2f}\"\n",
    "            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(annotated, label, (x1, max(y1 - 10, 10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "    return annotated, filtered_boxes, filtered_ids\n",
    "\n",
    "# ========== VIDEO CAPTURE ==========\n",
    "def setup_video_capture(video_path):\n",
    "    \"\"\"\n",
    "    Setup video capture and retrieve video metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - video_path: Path to the video file\n",
    "    \n",
    "    Returns:\n",
    "    - cap: VideoCapture object\n",
    "    - total_frames: Total number of frames in the video\n",
    "    - fps: Frames per second\n",
    "    - w, h: Width and height of the video frames\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    return cap, total_frames, fps, w, h\n",
    "\n",
    "# ========== MAIN FUNCTION ==========\n",
    "def process_all_videos(yolo_weights_path, class_id_to_name, underrepresented_class_ids,\n",
    "                       video_dir='videos', output_base='./datasets/opt', conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Process all videos in the specified directory, running YOLO model inference and saving\n",
    "    frames, bounding boxes, and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - yolo_weights_path: Path to the YOLO weights file\n",
    "    - class_id_to_name: Mapping of class IDs to class names and colors\n",
    "    - underrepresented_class_ids: List of class IDs that need special attention\n",
    "    - video_dir: Directory containing the video files\n",
    "    - output_base: Base directory for saving outputs\n",
    "    - conf_threshold: Confidence threshold for valid detections\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"[+] Using Model\", yolo_weights_path)\n",
    "    yolo_model = YOLO(yolo_weights_path)\n",
    "\n",
    "    # Output directories for images, labels, and video\n",
    "    image_out_dir = os.path.join(output_base, 'images')\n",
    "    label_out_dir = os.path.join(output_base, 'labels')\n",
    "    output_video_dir = os.path.join(output_base, 'output')\n",
    "\n",
    "    os.makedirs(image_out_dir, exist_ok=True)\n",
    "    os.makedirs(label_out_dir, exist_ok=True)\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    total_frame_count = 0\n",
    "    total_bounding_boxes = 0\n",
    "    total_label_files = 0\n",
    "\n",
    "    # Process each video\n",
    "    for video_file in tqdm(sorted(os.listdir(video_dir))):\n",
    "        if not video_file.lower().endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        video_id = os.path.splitext(video_file)[0]\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        output_video_path = os.path.join(output_video_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        print(f\"\\n========== STARTED: {video_id} ==========\")\n",
    "        cap, total_frames, fps, w, h = setup_video_capture(video_path)\n",
    "        writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "\n",
    "        frame_count = 0\n",
    "        current_index = 0\n",
    "\n",
    "        pbar = tqdm(total=total_frames, desc=f\"{video_id} (all frames)\")\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Annotate the frame\n",
    "            annotated_bgr, boxes, class_ids = process_frame(\n",
    "                frame, yolo_model, w, h, class_id_to_name, underrepresented_class_ids, conf_threshold=conf_threshold)\n",
    "\n",
    "            if boxes:  # Only save if there are valid detections\n",
    "                img_filename = f'{video_id}_{frame_count:04d}.jpg'\n",
    "                img_path = os.path.join(image_out_dir, img_filename)\n",
    "                cv2.imwrite(img_path, frame)\n",
    "\n",
    "                # Save YOLO-format label\n",
    "                label_filename = f'{video_id}_{frame_count:04d}.txt'\n",
    "                label_path = os.path.join(label_out_dir, label_filename)\n",
    "                with open(label_path, 'w') as f:\n",
    "                    for box, cls_id in zip(boxes, class_ids):\n",
    "                        x1, y1, x2, y2 = box\n",
    "                        w_box = x2 - x1\n",
    "                        h_box = y2 - y1\n",
    "                        cx = x1 + w_box / 2\n",
    "                        cy = y1 + h_box / 2\n",
    "                        f.write(f\"{int(cls_id)} {cx/w:.6f} {cy/h:.6f} {w_box/w:.6f} {h_box/h:.6f}\\n\")\n",
    "\n",
    "                total_label_files += 1\n",
    "                total_bounding_boxes += len(boxes)\n",
    "\n",
    "                writer.write(annotated_bgr)\n",
    "                frame_count += 1\n",
    "\n",
    "            current_index += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "        pbar.close()\n",
    "        print(f\"DONE: {video_id} â€” Processed {frame_count} meaningful frames\")\n",
    "        total_frame_count += frame_count\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\n[+] DONE: Total Processed {total_frame_count} meaningful frames\")\n",
    "    print(f\"[+] Total Bounding Boxes Detected: {total_bounding_boxes}\")\n",
    "    print(f\"[+] Total Label Files Created: {total_label_files}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70f244",
   "metadata": {},
   "source": [
    "### Identify Rare Classes from YOLO Labels\n",
    "\n",
    "1. **Class Frequency Counting**  \n",
    "   The function scans all `.txt` label files in `label_dir` to count the occurrences of each class ID.\n",
    "\n",
    "2. **Rare Class Detection**  \n",
    "   Class IDs with instance counts below `rare_threshold` are identified as rare and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bcf3c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_rare_class_ids(label_dir, class_id_to_name, rare_threshold=1000):\n",
    "    \"\"\"\n",
    "    Identify rare class IDs based on the number of instances in the label files.\n",
    "    \n",
    "    Parameters:\n",
    "    - label_dir: Directory containing the label files\n",
    "    - class_id_to_name: Mapping of class IDs to class names\n",
    "    - rare_threshold: Threshold for class instances to be considered rare\n",
    "    \n",
    "    Returns:\n",
    "    - rare_class_ids: Set of class IDs that have fewer instances than the threshold\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to count occurrences of each class\n",
    "    class_counts = defaultdict(int)\n",
    "\n",
    "    # Iterate through all label files in the specified directory\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        if not label_file.endswith('.txt'):  # Only consider .txt label files\n",
    "            continue\n",
    "        with open(os.path.join(label_dir, label_file), 'r') as f:\n",
    "            # Count occurrences of each class in the label file\n",
    "            for line in f:\n",
    "                parts = line.strip().split()  # Split each line by spaces\n",
    "                if len(parts) >= 1:  # Ensure there's at least one part (class ID)\n",
    "                    cls_id = int(parts[0])  # Extract class ID\n",
    "                    class_counts[cls_id] += 1  # Increment the count for this class\n",
    "\n",
    "    # Print the count for each class\n",
    "    print(\"Class-wise instance counts:\")\n",
    "    total_count = 0  # Variable to track the total count of instances\n",
    "\n",
    "    # Iterate over the class IDs and print the count for each class\n",
    "    for cls_id in sorted(class_counts.keys()):\n",
    "        name = class_id_to_name.get(cls_id, (\"Unknown\", []))[0]  # Get the class name\n",
    "        count = class_counts[cls_id]\n",
    "        print(f\"Class {cls_id:2d} ({name:10s}): {count} instances\")\n",
    "        total_count += count  # Update the total count of instances\n",
    "\n",
    "    print(\"Total Count:\", total_count)\n",
    "\n",
    "    # Identify classes with fewer instances than the threshold\n",
    "    rare_class_ids = {cls_id for cls_id, count in class_counts.items() if count < rare_threshold}\n",
    "    print(f\"\\nRare class IDs (threshold < {rare_threshold}): {rare_class_ids}\")\n",
    "\n",
    "    return rare_class_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c030e",
   "metadata": {},
   "source": [
    "### Merge Two YOLO Datasets\n",
    "\n",
    "1. **Dataset Copy with Prefix Renaming**  \n",
    "   Images and labels from two YOLO datasets (`source1`, `source2`) are copied to a `destination` directory, each renamed with a unique prefix (`orig_`, `pred_`) to avoid filename clashes.\n",
    "\n",
    "2. **Folder Structure Preserved**  \n",
    "   Ensures YOLO folder structure (`images/`, `labels/`) is maintained in the destination and handles missing label files with warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0bdc3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def merge_yolo_datasets(source1, source2, destination):\n",
    "    \"\"\"\n",
    "    Merges two YOLO datasets by copying the images and label files from two sources\n",
    "    into a destination directory, with appropriate renaming and handling of duplicate labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - source1: Path to the first source dataset\n",
    "    - source2: Path to the second source dataset\n",
    "    - destination: Path to the destination directory where merged dataset will be saved\n",
    "    \"\"\"\n",
    "    # Define subfolders for images and labels in both source datasets\n",
    "    img1_dir = os.path.join(source1, 'images')\n",
    "    lbl1_dir = os.path.join(source1, 'labels')\n",
    "    img2_dir = os.path.join(source2, 'images')\n",
    "    lbl2_dir = os.path.join(source2, 'labels')\n",
    "    dst_img_dir = os.path.join(destination, 'images')\n",
    "    dst_lbl_dir = os.path.join(destination, 'labels')\n",
    "\n",
    "    # Create destination folders for images and labels if they don't exist\n",
    "    os.makedirs(dst_img_dir, exist_ok=True)\n",
    "    os.makedirs(dst_lbl_dir, exist_ok=True)\n",
    "\n",
    "    def copy_files(src_img_dir, src_lbl_dir, prefix):\n",
    "        \"\"\"\n",
    "        Copies image and label files from source directories to destination,\n",
    "        renaming them with a given prefix to distinguish the datasets.\n",
    "        \n",
    "        Parameters:\n",
    "        - src_img_dir: Source directory containing image files\n",
    "        - src_lbl_dir: Source directory containing label files\n",
    "        - prefix: Prefix to append to filenames to distinguish the source dataset\n",
    "        \"\"\"\n",
    "        # Iterate through the image files in the source directory\n",
    "        for filename in sorted(os.listdir(src_img_dir)):\n",
    "            if not filename.lower().endswith('.jpg'):\n",
    "                continue\n",
    "            base = os.path.splitext(filename)[0]  # Get base filename without extension\n",
    "\n",
    "            # Copy image file with the prefix\n",
    "            new_img_name = f\"{prefix}_{base}.jpg\"\n",
    "            shutil.copy(os.path.join(src_img_dir, filename),\n",
    "                        os.path.join(dst_img_dir, new_img_name))\n",
    "\n",
    "            # Copy corresponding label file, if it exists\n",
    "            label_file = base + \".txt\"\n",
    "            if os.path.exists(os.path.join(src_lbl_dir, label_file)):\n",
    "                new_lbl_name = f\"{prefix}_{base}.txt\"\n",
    "                shutil.copy(os.path.join(src_lbl_dir, label_file),\n",
    "                            os.path.join(dst_lbl_dir, new_lbl_name))\n",
    "            else:\n",
    "                print(f\"Warning: No label for {filename}\")\n",
    "\n",
    "    # Merge the first (original) dataset\n",
    "    print(\"[+] Merging original dataset...\")\n",
    "    copy_files(img1_dir, lbl1_dir, prefix=\"orig\")\n",
    "\n",
    "    # Merge the second (predicted) dataset\n",
    "    print(\"[+] Merging predicted video dataset...\")\n",
    "    copy_files(img2_dir, lbl2_dir, prefix=\"pred\")\n",
    "\n",
    "    # Print completion message\n",
    "    print(f\"\\n[+] Merge complete! Merged dataset saved at: {destination}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adccd711",
   "metadata": {},
   "source": [
    "### Print YOLOv8 Training Metrics from `results.csv`\n",
    "\n",
    "1. **Automatic CSV Discovery & Analysis**  \n",
    "   Recursively searches for `results.csv` in the given directory and loads it into a DataFrame to analyze training progress and outcomes.\n",
    "\n",
    "2. **Final Epoch Summary**  \n",
    "   Calculates total training and validation loss and prints key metrics like Precision, Recall, and mAP from the last epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fbbb0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def find_results_csv(directory):\n",
    "    \"\"\"Find the results.csv file in the specified directory.\"\"\"\n",
    "    # Traverse the directory to find 'results.csv'\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if 'results.csv' in files:\n",
    "            return os.path.join(root, 'results.csv')  # Return the full path if found\n",
    "    return None  # Return None if 'results.csv' is not found\n",
    "\n",
    "def load_results_csv(results_csv_path):\n",
    "    \"\"\"Load the results CSV into a pandas DataFrame.\"\"\"\n",
    "    # Load the CSV file into a DataFrame and return it\n",
    "    return pd.read_csv(results_csv_path)\n",
    "\n",
    "def calculate_total_epochs(df):\n",
    "    \"\"\"Calculate the total number of epochs from the DataFrame.\"\"\"\n",
    "    # Return the maximum epoch value from the DataFrame\n",
    "    return df['epoch'].max()\n",
    "\n",
    "def calculate_training_loss(epoch_data):\n",
    "    \"\"\"Calculate the total training loss from the given epoch data.\"\"\"\n",
    "    # Extract training loss components from the epoch data\n",
    "    train_box_loss = epoch_data['train/box_loss']\n",
    "    train_cls_loss = epoch_data['train/cls_loss']\n",
    "    train_dfl_loss = epoch_data['train/dfl_loss']\n",
    "    # Return the sum of the training losses\n",
    "    return train_box_loss + train_cls_loss + train_dfl_loss\n",
    "\n",
    "def calculate_validation_loss(epoch_data):\n",
    "    \"\"\"Calculate the total validation loss from the given epoch data.\"\"\"\n",
    "    # Extract validation loss components from the epoch data\n",
    "    val_box_loss = epoch_data['val/box_loss']\n",
    "    val_cls_loss = epoch_data['val/cls_loss']\n",
    "    val_dfl_loss = epoch_data['val/dfl_loss']\n",
    "    # Return the sum of the validation losses\n",
    "    return val_box_loss + val_cls_loss + val_dfl_loss\n",
    "\n",
    "def print_final_metrics(df):\n",
    "    \"\"\"Print the final metrics for the last epoch.\"\"\"\n",
    "    # Extract the data for the last epoch\n",
    "    final_epoch_data = df.iloc[-1]\n",
    "\n",
    "    # Calculate total training and validation loss\n",
    "    train_loss = calculate_training_loss(final_epoch_data)\n",
    "    val_loss = calculate_validation_loss(final_epoch_data)\n",
    "\n",
    "    # Print the training metrics for the last epoch\n",
    "    print(\"\\n========== Final Training Metrics ==========\")\n",
    "    print(f\"Training Loss: {train_loss:.6f}\")\n",
    "    print(f\"Precision: {final_epoch_data['metrics/precision(B)']:.6f}\")\n",
    "    print(f\"Recall: {final_epoch_data['metrics/recall(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5: {final_epoch_data['metrics/mAP50(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5:0.95: {final_epoch_data['metrics/mAP50-95(B)']:.6f}\")\n",
    "\n",
    "    # Print the validation metrics for the last epoch\n",
    "    print(\"\\n========== Final Validation Metrics ==========\")\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "def print_csv_metrics(directory):\n",
    "    \"\"\"Main function to process and print final metrics.\"\"\"\n",
    "    # Find the 'results.csv' file in the given directory\n",
    "    results_csv_path = find_results_csv(directory)\n",
    "    \n",
    "    if not results_csv_path:\n",
    "        # If no 'results.csv' is found, print an error and return\n",
    "        print(\"Error: 'results.csv' file not found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    # Print the path to the found 'results.csv' file\n",
    "    print(f\"Found results.csv at: {results_csv_path}\")\n",
    "\n",
    "    # Load the results from the CSV file into a DataFrame\n",
    "    df = load_results_csv(results_csv_path)\n",
    "\n",
    "    # Get the total number of epochs from the DataFrame\n",
    "    total_epochs = calculate_total_epochs(df)\n",
    "    print(f\"Total number of epochs: {total_epochs}\")\n",
    "\n",
    "    # Print the final training and validation metrics for the last epoch\n",
    "    print_final_metrics(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1557b",
   "metadata": {},
   "source": [
    "### Compare Class-wise mAP@0.5:0.95 Before vs After Retraining\n",
    "\n",
    "1. **Side-by-Side Metric Comparison**  \n",
    "   Reads two JSON files containing class-wise mAP@0.5:0.95 values (before & after retraining) and compares them.\n",
    "\n",
    "2. **Visual Trend Highlighting**  \n",
    "   Displays changes in mAP with colored indicators:\n",
    "   - ðŸŸ¢ **Increase** for performance gain\n",
    "   - ðŸ”´ **Decrease** for performance drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c83bce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def compare_maps(json_path1, json_path2):\n",
    "    with open(json_path1, 'r') as f1, open(json_path2, 'r') as f2:\n",
    "        metrics1 = json.load(f1)\n",
    "        metrics2 = json.load(f2)\n",
    "\n",
    "    print(\"\\nmAP@0.5:0.95 Differences Before and After Retraning:\\n\")\n",
    "    print(f\"{'Class':<15} {'Before':<10} {'After':<10} {'Diff':<10} {'Trend'}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for class_name in metrics1:\n",
    "        map1 = metrics1[class_name].get(\"mAP@0.5:0.95\", 0)\n",
    "        map2 = metrics2.get(class_name, {}).get(\"mAP@0.5:0.95\", 0)\n",
    "\n",
    "        diff = map2 - map1\n",
    "        if abs(diff) > 1e-6:\n",
    "            if diff > 0:\n",
    "                trend = f\"\\033[92m Increase\\033[0m\"  # Green for increase\n",
    "            else:\n",
    "                trend = f\"\\033[91m Decrease\\033[0m\"  # Red for decrease\n",
    "\n",
    "            # Printing with colors\n",
    "            print(f\"{class_name:<15} {map1:<10.4f} {map2:<10.4f} {diff:<10.4f} {trend}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "070ae4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_final_metrics(csv1_path, csv2_path):\n",
    "    df1 = pd.read_csv(csv1_path)\n",
    "    df2 = pd.read_csv(csv2_path)\n",
    "\n",
    "    last1 = df1.iloc[-1]\n",
    "    last2 = df2.iloc[-1]\n",
    "\n",
    "    metrics_to_compare = {\n",
    "        \"train/box_loss\": \"Box Loss (Train)\",\n",
    "        \"train/cls_loss\": \"Cls Loss (Train)\",\n",
    "        \"train/dfl_loss\": \"DFL Loss (Train)\",\n",
    "        \"metrics/precision(B)\": \"Precision\",\n",
    "        \"metrics/recall(B)\": \"Recall\",\n",
    "        \"metrics/mAP50(B)\": \"mAP@0.5\",\n",
    "        \"metrics/mAP50-95(B)\": \"mAP@0.5:0.95\",\n",
    "        \"val/box_loss\": \"Box Loss (Val)\",\n",
    "        \"val/cls_loss\": \"Cls Loss (Val)\",\n",
    "        \"val/dfl_loss\": \"DFL Loss (Val)\"\n",
    "    }\n",
    "\n",
    "    print(\"Changes in Metrics Before and After Retraning:\\n\")\n",
    "    print(f\"{'Metric':<25} {'Before':<10} {'After':<10} {'Diff':<10} {'Trend'}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    for key, label in metrics_to_compare.items():\n",
    "        val1 = last1[key]\n",
    "        val2 = last2[key]\n",
    "        diff = val2 - val1\n",
    "        if abs(diff) > 1e-6:\n",
    "            # If increase, color green; if decrease, color red\n",
    "            if diff > 0:\n",
    "                trend = f\"\\033[92m Increase\\033[0m\"  # Green\n",
    "            else:\n",
    "                trend = f\"\\033[91m Decrease\\033[0m\"  # Red\n",
    "\n",
    "            # Printing with colors\n",
    "            print(f\"{label:<25} {val1:<10.5f} {val2:<10.5f} {diff:<10.5f} {trend}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d7bab",
   "metadata": {},
   "source": [
    "## Calling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f592f31",
   "metadata": {},
   "source": [
    "### Download and Extract the Semantic Drone Dataset and UAVDT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "54ca66d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading ZIP from Google Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1UppumYqYOi-kto6BWPfFxwJK2Eph46oY\n",
      "From (redirected): https://drive.google.com/uc?id=1UppumYqYOi-kto6BWPfFxwJK2Eph46oY&confirm=t&uuid=7b45470b-0565-4adc-8d16-76e8e9282feb\n",
      "To: /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/downloaded.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.14G/4.14G [01:36<00:00, 42.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting ZIP...\n",
      "[DONE] Extracted files to: datasets\n",
      "[+] Downloading ZIP from Google Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=12cbrTaBAMIsuU-mwAA7IgDk9wSLC9cC-\n",
      "From (redirected): https://drive.google.com/uc?id=12cbrTaBAMIsuU-mwAA7IgDk9wSLC9cC-&confirm=t&uuid=18b98c08-4e97-4c47-825e-3577518f9045\n",
      "To: /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/downloaded.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.21G/5.21G [04:36<00:00, 18.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Extracting ZIP...\n",
      "[+] Extracted files to: datasets\n"
     ]
    }
   ],
   "source": [
    "# Google Drive URL for the Semantic Drone Dataset\n",
    "gdrive_url = \"https://drive.google.com/file/d/1UppumYqYOi-kto6BWPfFxwJK2Eph46oY/view?usp=sharing\"\n",
    "# Call the function to download and extract the Semantic Drone Dataset\n",
    "semantic_drone_dataset_download(gdrive_url, extract_to=\"datasets\")\n",
    "\n",
    "# Google Drive URL for the UAVDT Dataset\n",
    "gdrive_url = \"https://drive.google.com/file/d/12cbrTaBAMIsuU-mwAA7IgDk9wSLC9cC-/view?usp=sharing\"\n",
    "# Call the function to download and extract the UAVDT Dataset\n",
    "uavdt_dataset_download(gdrive_url, extract_to=\"datasets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148a50a",
   "metadata": {},
   "source": [
    "### Semantic Drone Dataset to YOLO Format Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "558387bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to YOLO:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 301/400 [01:04<00:20,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Skipping image 389 due to parse error: not well-formed (invalid token): line 1, column 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to YOLO: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [01:25<00:00,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] YOLO-format annotation conversion complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the Semantic Drone Dataset training set\n",
    "dataset_path = \"./datasets/semantic_drone_dataset/training_set\" \n",
    "\n",
    "# Output directory where the YOLO formatted dataset will be saved\n",
    "output_dir = \"./datasets/semantic_yolo\"\n",
    "\n",
    "# Call the function to convert the full dataset into YOLO format\n",
    "# The function converts annotations and images from the Semantic Drone Dataset into YOLO format\n",
    "convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09d5280",
   "metadata": {},
   "source": [
    "### UAVDT-2024 and Semnatic Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "94505830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Converting 30946 annotation files to YOLO format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30946/30946 [01:13<00:00, 422.79file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversion complete.\n",
      "Total boxes:     868139\n",
      "Converted boxes: 868139\n",
      "Skipped boxes:   0\n",
      "Malformed lines: 0\n",
      "Missing images:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:03<00:00, 10.22it/s]\n",
      "Copying val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[+] Dataset split into 'train/' and 'val/' folders with images and YOLO-format labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving to train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 319/319 [00:00<00:00, 721.30it/s]\n",
      "Moving to val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:00<00:00, 719.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[âœ“] Dataset split completed: 319 train / 80 val samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# UAVDT-2024 Dataset Processing\n",
    "\n",
    "# Path to the source UAVDT-2024 dataset\n",
    "source_root = \"./datasets/UAVDT-2024\"\n",
    "\n",
    "# Output directory where the new YOLO format dataset will be saved\n",
    "output_root = \"./datasets/new_dataset_yolo_split\"\n",
    "\n",
    "# Convert the UAVDT-2024 dataset into YOLO format\n",
    "# The function processes the dataset and converts annotations and images into YOLO format\n",
    "convert_dataset(source_root)\n",
    "\n",
    "# Split the dataset into training and validation sets with a ratio of 80:20\n",
    "# This function copies the relevant sequences into the respective directories for training and validation\n",
    "copy_split_sequences(source_root, output_root, train_ratio=0.8)\n",
    "\n",
    "\n",
    "# Semantic Drone Datasets Processing\n",
    "\n",
    "# Split and move the Semantic Drone dataset into training and validation sets\n",
    "# The function handles the splitting of the dataset and moves the images and annotations into separate directories\n",
    "split_and_move_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239667ff",
   "metadata": {},
   "source": [
    "### Dataset Label Normalization for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ec08031d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2354/2354 [00:00<00:00, 8007.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1181/1181 [00:00<00:00, 5780.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set your paths for the training dataset\n",
    "dataset_path = \"./datasets/new_dataset_yolo_split/train\"\n",
    "\n",
    "# Directory where the images are stored in the training dataset\n",
    "image_dir = os.path.join(dataset_path, \"images\")\n",
    "\n",
    "# Directory where the label files are stored in the training dataset\n",
    "annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# Normalize all label files in the training dataset by adjusting coordinates\n",
    "# This function ensures that the labels follow the expected YOLO format (normalized coordinates)\n",
    "normalize_all_labels(annotations_dir, image_dir)\n",
    "\n",
    "# Set your paths for the validation dataset\n",
    "dataset_path = \"./datasets/new_dataset_yolo_split/val\"\n",
    "\n",
    "# Directory where the images are stored in the validation dataset\n",
    "image_dir = os.path.join(dataset_path, \"images\")\n",
    "\n",
    "# Directory where the label files are stored in the validation dataset\n",
    "annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# Normalize all label files in the validation dataset\n",
    "normalize_all_labels(annotations_dir, image_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88e922",
   "metadata": {},
   "source": [
    "### Identifying Rare Classes in the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "eeb923ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-wise instance counts:\n",
      "Class  1 (pool      ): 30 instances\n",
      "Class  2 (vegetation): 5528 instances\n",
      "Class  3 (roof      ): 280 instances\n",
      "Class  4 (wall      ): 954 instances\n",
      "Class  5 (window    ): 376 instances\n",
      "Class  6 (person    ): 2475 instances\n",
      "Class  7 (dog       ): 25 instances\n",
      "Class  8 (car       ): 35045 instances\n",
      "Class  9 (bicycle   ): 222 instances\n",
      "Class 10 (tree      ): 417 instances\n",
      "Class 11 (truck     ): 129 instances\n",
      "Class 12 (bus       ): 86 instances\n",
      "Class 13 (vehicle   ): 577 instances\n",
      "Total Count: 46144\n",
      "\n",
      "Rare class IDs (threshold < 3000): {1, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13}\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the training labels directory\n",
    "labels_dir = './datasets/new_dataset_yolo_split/train/labels'\n",
    "\n",
    "# Get the list of rare class IDs by analyzing the label files in the specified directory\n",
    "# The function `get_rare_class_ids` will count the number of occurrences of each class\n",
    "# and return those with occurrences below the specified threshold (in this case, 3000)\n",
    "rare_class_ids = get_rare_class_ids(label_dir=labels_dir, class_id_to_name=class_id_to_name, rare_threshold=3000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63556279",
   "metadata": {},
   "source": [
    "### Clean Up Directories and Train YOLOv8 from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "56985371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Deleted folder: ./datasets/semantic_yolo\n",
      "âš ï¸ Folder does not exist: ./datasets/new_dataset_yolo\n",
      "âš ï¸ Folder does not exist: ./datasets/uavdt-processed\n",
      "âš ï¸ Folder does not exist: ./runs\n",
      "âš ï¸ Folder does not exist: ./metrics\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# List of folders to delete\n",
    "folders_to_delete = ['./datasets/semantic_yolo', './datasets/new_dataset_yolo', './datasets/uavdt-processed', './runs', \"./metrics\"]\n",
    "\n",
    "# Iterate over each folder path in the list\n",
    "for folder_path in folders_to_delete:\n",
    "    # Check if the folder exists\n",
    "    if os.path.exists(folder_path):\n",
    "        # Delete the folder and its contents\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"âœ… Deleted folder: {folder_path}\")\n",
    "    else:\n",
    "        # If the folder doesn't exist, print a warning message\n",
    "        print(f\"âš ï¸ Folder does not exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f81a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Training from scratch using base model: yolov8n.pt\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.25M/6.25M [00:00<00:00, 36.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.109 ðŸš€ Python-3.10.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 OEM, 7957MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=yolov8.yaml, epochs=1, time=None, patience=30, batch=8, imgsz=720, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/train, name=yolov8, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=10, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.2, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.2, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/train/yolov8\n",
      "Overriding model.yaml nc=80 with nc=14\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    754042  ultralytics.nn.modules.head.Detect           [14, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,013,578 parameters, 3,013,562 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 38.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "WARNING âš ï¸ imgsz=[720] must be multiple of max stride 32, updating to [736]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/new_dataset_yolo_split/train/labels... 2354 images, 3 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2354/2354 [00:00<00:00, 5196.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/new_dataset_yolo_split/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/new_dataset_yolo_split/val/labels... 1181 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1181/1181 [00:00<00:00, 2073.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/new_dataset_yolo_split/val/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/train/yolov8/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000556, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 736 train, 736 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/yolov8\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      2.61G      1.721       2.26      1.118        123        736: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [00:34<00:00,  8.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:11<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.601      0.107     0.0824     0.0402\n",
      "\n",
      "1 epochs completed in 0.013 hours.\n",
      "Optimizer stripped from runs/train/yolov8/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/train/yolov8/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/train/yolov8/weights/best.pt...\n",
      "Ultralytics 8.3.109 ðŸš€ Python-3.10.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 OEM, 7957MiB)\n",
      "Model summary (fused): 72 layers, 3,008,378 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:15<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.489      0.116     0.0761     0.0364\n",
      "                  pool          9          9          1          0          0          0\n",
      "            vegetation         75       1780      0.232      0.185      0.112     0.0499\n",
      "                  roof         42         79     0.0709      0.582      0.276      0.162\n",
      "                  wall         62        255     0.0696     0.0627     0.0207    0.00968\n",
      "                window         38        139          1          0          0          0\n",
      "                person         75        637      0.223     0.0706     0.0694     0.0338\n",
      "                   dog          6         12          1          0          0          0\n",
      "                   car       1115      53669      0.322      0.434      0.277     0.0994\n",
      "               bicycle         32         53        0.2     0.0566     0.0583     0.0197\n",
      "                  tree         38        108      0.236       0.12     0.0774     0.0368\n",
      "                 truck        907       3904          1          0          0          0\n",
      "                   bus        169        169          0          0    0.00886    0.00241\n",
      "               vehicle       1079       9579          1          0     0.0898     0.0594\n",
      "Speed: 0.2ms preprocess, 5.3ms inference, 0.0ms loss, 3.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/train/yolov8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv8 from scratch with the specified parameters\n",
    "train_yolo_from_scratch(\n",
    "    data_yaml=\"yolov8.yaml\",  # Path to the YAML file that contains dataset and class configuration\n",
    "    epochs=50,               # Number of epochs to train the model\n",
    "    imgsz=720,                # Image size (height and width) for training\n",
    "    batch=8,                  # Batch size for training\n",
    "    name=\"yolov8\",            # Name of the training run (used for saving checkpoints, logs, etc.)\n",
    "    model_variant=\"yolov8n.pt\"  # The base YOLOv8 model variant to start training (options: yolov8n.pt, yolov8s.pt, yolov8m.pt, etc.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c39cf0",
   "metadata": {},
   "source": [
    "### Evaluate YOLOv8 Model and Print Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "830c11fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Found best.pt at: runs/train/yolov8/weights/best.pt\n",
      "Ultralytics 8.3.109 ðŸš€ Python-3.10.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 OEM, 7957MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 72 layers, 3,008,378 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/new_dataset_yolo_split/val/labels.cache... 1181 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1181/1181 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:10<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.601      0.107     0.0824     0.0401\n",
      "                  pool          9          9          1          0     0.0307     0.0213\n",
      "            vegetation         75       1780      0.299      0.152      0.105     0.0456\n",
      "                  roof         42         79     0.0973      0.456      0.267      0.158\n",
      "                  wall         62        255      0.169     0.0392     0.0216    0.00904\n",
      "                window         38        139          1          0          0          0\n",
      "                person         75        637      0.363      0.118      0.112     0.0482\n",
      "                   dog          6         12          1          0          0          0\n",
      "                   car       1115      53669      0.447      0.395      0.316      0.116\n",
      "               bicycle         32         53       0.15     0.0566     0.0422     0.0106\n",
      "                  tree         38        108      0.284      0.176      0.102     0.0596\n",
      "                 truck        907       3904          1          0          0          0\n",
      "                   bus        169        169          1          0     0.0022    0.00131\n",
      "               vehicle       1079       9579          1          0     0.0724     0.0519\n",
      "Speed: 0.2ms preprocess, 2.9ms inference, 0.0ms loss, 2.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
      "[+] Saved per-class metrics to per_class_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the YOLOv8 training run directory\n",
    "yolov8 = './runs/train/yolov8'\n",
    "\n",
    "# Find the best model based on the training run (usually the best model is the one with the lowest validation loss)\n",
    "best_pt_path = find_best_model(yolov8)\n",
    "\n",
    "# Evaluate the best model and save the performance metrics\n",
    "evaluate_and_save_metrics(best_pt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "44ab66bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Class mAP@0.5:0.95 Metrics:\n",
      "\n",
      "Class Name      Class ID   mAP@0.5:0.95   \n",
      "----------------------------------------\n",
      "unlabeled       0          0.0401         \n",
      "pool            1          0.0213         \n",
      "vegetation      2          0.0456         \n",
      "roof            3          0.158          \n",
      "wall            4          0.009          \n",
      "window          5          0.0            \n",
      "person          6          0.0482         \n",
      "dog             7          0.0            \n",
      "car             8          0.1155         \n",
      "bicycle         9          0.0106         \n",
      "tree            10         0.0596         \n",
      "truck           11         0.0            \n",
      "bus             12         0.0013         \n",
      "vehicle         13         0.0519         \n"
     ]
    }
   ],
   "source": [
    "# Print per-class metrics from the specified JSON file\n",
    "print_per_class_metrics(\"per_class_metrics.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d15b47f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found results.csv at: ./runs/train/yolov8/results.csv\n",
      "Total number of epochs: 1\n",
      "\n",
      "========== Final Training Metrics ==========\n",
      "Training Loss: 5.099240\n",
      "Precision: 0.601400\n",
      "Recall: 0.106930\n",
      "mAP@0.5: 0.082410\n",
      "mAP@0.5:0.95: 0.040160\n",
      "\n",
      "========== Final Validation Metrics ==========\n",
      "Validation Loss: 5.742000\n"
     ]
    }
   ],
   "source": [
    "# Print the metrics from the CSV file found in the specified directory\n",
    "print_csv_metrics(yolov8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230e616",
   "metadata": {},
   "source": [
    "### Clean Up Folders and Process Videos for YOLO Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "01fe5528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Folder does not exist: ./datasets/new-videos-predicted-yolo\n",
      "âš ï¸ Folder does not exist: ./datasets/merged_yolo_dataset\n",
      "âš ï¸ Folder does not exist: ./datasets/split_videos_dataset\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# List of folders to delete\n",
    "folders_to_delete = ['./datasets/new-videos-predicted-yolo', \"./datasets/merged_yolo_dataset\", \"./datasets/split_videos_dataset\"]\n",
    "\n",
    "# Loop through each folder path in the list\n",
    "for folder_path in folders_to_delete:\n",
    "    # Check if the folder exists\n",
    "    if os.path.exists(folder_path):\n",
    "        # If it exists, delete the folder and its contents\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"âœ… Deleted folder: {folder_path}\")\n",
    "    else:\n",
    "        # If the folder does not exist, print a warning message\n",
    "        print(f\"âš ï¸ Folder does not exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9cc5a553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Using Model runs/train/yolov8/weights/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== STARTED: v1 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v1 (all frames): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 642/642 [00:32<00:00, 19.50it/s]\n",
      " 12%|â–ˆâ–Ž        | 1/8 [00:33<03:53, 33.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v1 â€” Processed 599 meaningful frames\n",
      "\n",
      "========== STARTED: v12 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v12 (all frames): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 897/897 [00:10<00:00, 83.00it/s]\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:44<02:01, 20.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v12 â€” Processed 17 meaningful frames\n",
      "\n",
      "========== STARTED: v2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v2 (all frames): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175/175 [00:10<00:00, 17.42it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:54<01:18, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v2 â€” Processed 175 meaningful frames\n",
      "\n",
      "========== STARTED: v3 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v3 (all frames): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:01<00:00, 172.01it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:55<00:39,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v3 â€” Processed 25 meaningful frames\n",
      "\n",
      "========== STARTED: v4 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v4 (all frames): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [00:03<00:00, 68.13it/s] \n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:59<00:23,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v4 â€” Processed 154 meaningful frames\n",
      "\n",
      "========== STARTED: v5 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v5 (all frames): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:12<00:00, 90.00it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [01:12<00:18,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v5 â€” Processed 0 meaningful frames\n",
      "\n",
      "========== STARTED: v6 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v6 (all frames): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:12<00:00, 38.86it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [01:24<00:10, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v6 â€” Processed 142 meaningful frames\n",
      "\n",
      "========== STARTED: v8 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v8 (all frames): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 949/949 [00:07<00:00, 120.28it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:32<00:00, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v8 â€” Processed 11 meaningful frames\n",
      "\n",
      "[+] DONE: Total Processed 1123 meaningful frames\n",
      "[+] Total Bounding Boxes Detected: 1590\n",
      "[+] Total Label Files Created: 1123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all videos and generate predictions\n",
    "process_all_videos(best_pt_path, class_id_to_name, list(rare_class_ids),\n",
    "                   video_dir='videos', output_base='./datasets/new-videos-predicted-yolo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f4fce1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-wise instance counts:\n",
      "Class  3 (roof      ): 62 instances\n",
      "Class  4 (wall      ): 6 instances\n",
      "Class  6 (person    ): 1520 instances\n",
      "Class 10 (tree      ): 2 instances\n",
      "Total Count: 1590\n",
      "\n",
      "Rare class IDs (threshold < 0): set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Folder with YOLO label files\n",
    "label_dir = './datasets/new-videos-predicted-yolo/labels'\n",
    "\n",
    "# Get the rare class IDs from the label files based on the threshold\n",
    "get_rare_class_ids(label_dir=label_dir, class_id_to_name=class_id_to_name ,rare_threshold=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ff204",
   "metadata": {},
   "source": [
    "### Split and Merge new Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bc6a5518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving to train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 561/561 [00:00<00:00, 772.04it/s]\n",
      "Moving to val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 562/562 [00:00<00:00, 758.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[âœ“] Dataset split completed: 561 train / 562 val samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "split_and_move_dataset(source_base_dir=\"./datasets/new-videos-predicted-yolo\",\n",
    "                           target_base_dir=\"./datasets/split_videos_dataset\",\n",
    "                           split_ratio=0.5,\n",
    "                           seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3807d000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Merging original dataset...\n",
      "[+] Merging predicted video dataset...\n",
      "\n",
      "[+] Merge complete! Merged dataset saved at: ./datasets/merged_yolo_dataset\n"
     ]
    }
   ],
   "source": [
    "merge_yolo_datasets(\n",
    "    source1='./datasets/new_dataset_yolo_split/train',  # Path to the first source dataset (YOLO format)\n",
    "    source2='./datasets/split_videos_dataset/train',    # Path to the second source dataset (YOLO format)\n",
    "    destination='./datasets/merged_yolo_dataset'        # Path where the merged dataset will be saved\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd73f7",
   "metadata": {},
   "source": [
    "### Identify Rare Classes in the Merged YOLO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1eeb6de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-wise instance counts:\n",
      "Class  1 (pool      ): 30 instances\n",
      "Class  2 (vegetation): 5528 instances\n",
      "Class  3 (roof      ): 309 instances\n",
      "Class  4 (wall      ): 956 instances\n",
      "Class  5 (window    ): 376 instances\n",
      "Class  6 (person    ): 3242 instances\n",
      "Class  7 (dog       ): 25 instances\n",
      "Class  8 (car       ): 35045 instances\n",
      "Class  9 (bicycle   ): 222 instances\n",
      "Class 10 (tree      ): 418 instances\n",
      "Class 11 (truck     ): 129 instances\n",
      "Class 12 (bus       ): 86 instances\n",
      "Class 13 (vehicle   ): 577 instances\n",
      "Total Count: 46943\n",
      "\n",
      "Rare class IDs (threshold < 0): set()\n"
     ]
    }
   ],
   "source": [
    "# Folder with YOLO label files\n",
    "label_dir = './datasets/merged_yolo_dataset/labels'  # Path to the directory containing YOLO label files\n",
    "\n",
    "# Get rare class IDs in the dataset based on the given threshold\n",
    "rare_class_ids = get_rare_class_ids(\n",
    "    label_dir=label_dir,                # Path to the label directory\n",
    "    class_id_to_name=class_id_to_name,  # Mapping of class IDs to class names\n",
    "    rare_threshold=0                    # Set the threshold for class frequency; here, we are considering all classes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2ca7c",
   "metadata": {},
   "source": [
    "### Clean Up Fine-Tune YOLOv8 Folders and Fine-Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d7f1df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Match all folders starting with 'fine-tune-yolov8' inside './runs/train/'\n",
    "folders_to_delete = glob.glob('./runs/train/fine-tune-yolov8*')  # Find all folders starting with 'fine-tune-yolov8'\n",
    "\n",
    "# Iterate through the matched folders\n",
    "for folder_path in folders_to_delete:\n",
    "    if os.path.isdir(folder_path):  # Check if the path is a valid directory\n",
    "        shutil.rmtree(folder_path)  # Delete the folder and its contents\n",
    "        print(f\"âœ… Deleted folder: {folder_path}\")  # Print a success message\n",
    "    else:\n",
    "        print(f\"âš ï¸ Not a directory or doesn't exist: {folder_path}\")  # Print a warning if the folder doesn't exist or isn't a directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "103f6cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Fine-tuning model from: runs/train/yolov8/weights/best.pt\n",
      "Ultralytics 8.3.109 ðŸš€ Python-3.10.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 OEM, 7957MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=runs/train/yolov8/weights/best.pt, data=yolo_retrain.yaml, epochs=1, time=None, patience=10, batch=16, imgsz=720, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/train, name=fine-tune-yolov8, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=10, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.2, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.2, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/train/fine-tune-yolov8\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    754042  ultralytics.nn.modules.head.Detect           [14, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,013,578 parameters, 3,013,562 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "WARNING âš ï¸ imgsz=[720] must be multiple of max stride 32, updating to [736]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/merged_yolo_dataset/labels... 2915 images, 3 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2915/2915 [00:00<00:00, 4033.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/merged_yolo_dataset/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/split_videos_dataset/val/labels... 562 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 562/562 [00:00<00:00, 3495.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/split_videos_dataset/val/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/train/fine-tune-yolov8/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000556, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 736 train, 736 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/fine-tune-yolov8\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      5.37G       1.46      1.223      1.028        121        736: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 183/183 [00:40<00:00,  4.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:02<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        562        791      0.359      0.731      0.494      0.368\n",
      "\n",
      "1 epochs completed in 0.012 hours.\n",
      "Optimizer stripped from runs/train/fine-tune-yolov8/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/train/fine-tune-yolov8/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/train/fine-tune-yolov8/weights/best.pt...\n",
      "Ultralytics 8.3.109 ðŸš€ Python-3.10.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 OEM, 7957MiB)\n",
      "Model summary (fused): 72 layers, 3,008,378 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:04<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        562        791      0.834      0.355      0.354      0.237\n",
      "                  roof         30         33      0.387      0.842      0.525      0.438\n",
      "                  wall          4          4          1          0     0.0211    0.00917\n",
      "                person        549        753       0.95      0.579      0.863      0.495\n",
      "                  tree          1          1          1          0    0.00737    0.00663\n",
      "Speed: 0.1ms preprocess, 5.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/train/fine-tune-yolov8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "fine_tune_yolo(\n",
    "    data_yaml=\"yolo_retrain.yaml\",        # Path to the updated dataset YAML file containing information like class names, train/val paths, etc.\n",
    "    epochs=100,                             # Number of epochs for fine-tuning\n",
    "    imgsz=720,                             # Image size to be used for training (720x720 pixels in this case)\n",
    "    batch=16,                              # Batch size used during training (16 images per batch)\n",
    "    name=\"fine-tune-yolov8\",               # Name for this fine-tuning experiment\n",
    "    base_model_path=best_pt_path          # Path to the pre-trained YOLO model (the best model from previous training)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f9715",
   "metadata": {},
   "source": [
    "### Process and Compare Metrics for Fine-Tuned YOLOv8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "28391f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found results.csv at: ./runs/train/fine-tune-yolov8/results.csv\n",
      "Total number of epochs: 1\n",
      "\n",
      "========== Final Training Metrics ==========\n",
      "Training Loss: 3.711750\n",
      "Precision: 0.358670\n",
      "Recall: 0.730740\n",
      "mAP@0.5: 0.494490\n",
      "mAP@0.5:0.95: 0.367860\n",
      "\n",
      "========== Final Validation Metrics ==========\n",
      "Validation Loss: 3.572730\n"
     ]
    }
   ],
   "source": [
    "new_path = './runs/train/fine-tune-yolov8'  # Path to the folder where the fine-tuned YOLO model's training results are stored\n",
    "print_csv_metrics(new_path)  # This will process and print the final metrics from the 'results.csv' file in the specified folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "33362264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Found best.pt at: runs/train/fine-tune-yolov8/weights/best.pt\n",
      "Ultralytics 8.3.109 ðŸš€ Python-3.10.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 OEM, 7957MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 72 layers, 3,008,378 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/split_videos_dataset/val/labels.cache... 562 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 562/562 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:03<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        562        791       0.36      0.738      0.495      0.369\n",
      "                  roof         30         33      0.267      0.909      0.519      0.456\n",
      "                  wall          4          4     0.0347       0.25     0.0618     0.0421\n",
      "                person        549        753      0.858      0.792      0.901       0.53\n",
      "                  tree          1          1      0.279          1      0.497      0.448\n",
      "Speed: 0.3ms preprocess, 2.8ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n",
      "[+] Saved per-class metrics to per_class_metrics_retrain.json\n"
     ]
    }
   ],
   "source": [
    "# Find the best model (the one with the best performance) from the fine-tuned YOLOv8 training results\n",
    "best_pt_path = find_best_model(new_path)\n",
    "\n",
    "# Evaluate the best model and save the performance metrics in a JSON file\n",
    "evaluate_and_save_metrics(best_pt_path, output_json_path=\"per_class_metrics_retrain.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "edceacbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Class mAP@0.5:0.95 Metrics:\n",
      "\n",
      "Class Name      Class ID   mAP@0.5:0.95   \n",
      "----------------------------------------\n",
      "unlabeled       0          0.369          \n",
      "pool            1          0.369          \n",
      "vegetation      2          0.369          \n",
      "roof            3          0.4562         \n",
      "wall            4          0.0421         \n",
      "window          5          0.369          \n",
      "person          6          0.53           \n",
      "dog             7          0.369          \n",
      "car             8          0.369          \n",
      "bicycle         9          0.369          \n",
      "tree            10         0.4477         \n",
      "truck           11         0.369          \n",
      "bus             12         0.369          \n",
      "vehicle         13         0.369          \n"
     ]
    }
   ],
   "source": [
    "# Print the per-class metrics from the saved JSON file\n",
    "print_per_class_metrics(\"per_class_metrics_retrain.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7b1915b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes in Metrics Before and After Retraning:\n",
      "\n",
      "Metric                    Before     After      Diff       Trend\n",
      "-----------------------------------------------------------------\n",
      "Box Loss (Train)          1.72107    1.46032    -0.26075   \u001b[91m Decrease\u001b[0m\n",
      "Cls Loss (Train)          2.26020    1.22311    -1.03709   \u001b[91m Decrease\u001b[0m\n",
      "DFL Loss (Train)          1.11797    1.02832    -0.08965   \u001b[91m Decrease\u001b[0m\n",
      "Precision                 0.60140    0.35867    -0.24273   \u001b[91m Decrease\u001b[0m\n",
      "Recall                    0.10693    0.73074    0.62381    \u001b[92m Increase\u001b[0m\n",
      "mAP@0.5                   0.08241    0.49449    0.41208    \u001b[92m Increase\u001b[0m\n",
      "mAP@0.5:0.95              0.04016    0.36786    0.32770    \u001b[92m Increase\u001b[0m\n",
      "Box Loss (Val)            2.42401    1.33874    -1.08527   \u001b[91m Decrease\u001b[0m\n",
      "Cls Loss (Val)            2.18918    1.21991    -0.96927   \u001b[91m Decrease\u001b[0m\n",
      "DFL Loss (Val)            1.12881    1.01408    -0.11473   \u001b[91m Decrease\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define paths for the fine-tuned and original YOLOv8 model results\n",
    "new_path = './runs/train/fine-tune-yolov8'\n",
    "old_path = './runs/train/yolov8'\n",
    "\n",
    "# Find the results.csv files for both models\n",
    "results_csv_path = find_results_csv(new_path)  # Fine-tuned YOLOv8\n",
    "results_csv_path_1 = find_results_csv(old_path)  # Original YOLOv8\n",
    "\n",
    "# Compare the final epoch metrics between the two models\n",
    "compare_final_metrics(results_csv_path_1, results_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "97c1a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mAP@0.5:0.95 Differences Before and After Retraning:\n",
      "\n",
      "Class           Before     After      Diff       Trend\n",
      "------------------------------------------------------------\n",
      "unlabeled       0.0401     0.3690     0.3289     \u001b[92m Increase\u001b[0m\n",
      "pool            0.0213     0.3690     0.3477     \u001b[92m Increase\u001b[0m\n",
      "vegetation      0.0456     0.3690     0.3234     \u001b[92m Increase\u001b[0m\n",
      "roof            0.1580     0.4562     0.2982     \u001b[92m Increase\u001b[0m\n",
      "wall            0.0090     0.0421     0.0331     \u001b[92m Increase\u001b[0m\n",
      "window          0.0000     0.3690     0.3690     \u001b[92m Increase\u001b[0m\n",
      "person          0.0482     0.5300     0.4818     \u001b[92m Increase\u001b[0m\n",
      "dog             0.0000     0.3690     0.3690     \u001b[92m Increase\u001b[0m\n",
      "car             0.1155     0.3690     0.2535     \u001b[92m Increase\u001b[0m\n",
      "bicycle         0.0106     0.3690     0.3584     \u001b[92m Increase\u001b[0m\n",
      "tree            0.0596     0.4477     0.3881     \u001b[92m Increase\u001b[0m\n",
      "truck           0.0000     0.3690     0.3690     \u001b[92m Increase\u001b[0m\n",
      "bus             0.0013     0.3690     0.3677     \u001b[92m Increase\u001b[0m\n",
      "vehicle         0.0519     0.3690     0.3171     \u001b[92m Increase\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "compare_maps(\"per_class_metrics.json\", \"per_class_metrics_retrain.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759278b8",
   "metadata": {},
   "source": [
    "### Generate Predictions for Fine-Tuned and Original YOLOv8 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "db73f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Found best.pt at: runs/train/fine-tune-yolov8/weights/best.pt\n",
      "[+] Found best.pt at: runs/train/yolov8/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "# Set paths for the fine-tuned and original YOLOv8 models\n",
    "new_path = './runs/train/fine-tune-yolov8'\n",
    "old_path = './runs/train/yolov8'\n",
    "\n",
    "# Find the best model (checkpoint) from the fine-tuned YOLOv8 run\n",
    "best_pt_path_retrain = find_best_model(new_path)\n",
    "\n",
    "# Find the best model (checkpoint) from the original YOLOv8 run\n",
    "best_pt_path = find_best_model(old_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933909df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== STARTED: v1 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 642/642 [00:35<00:00, 17.88it/s]\n",
      " 12%|â–ˆâ–Ž        | 1/8 [00:35<04:11, 35.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v1 â€” Processed 642 frames\n",
      "========== STARTED: v12 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–Ž        | 1/8 [01:19<09:16, 79.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvideos_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_pt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_id_to_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./datasets/final_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[116], line 128\u001b[0m, in \u001b[0;36mvideos_predictions\u001b[0;34m(yolo_weights_path, class_id_to_name, video_dir, output_base, max_frames)\u001b[0m\n\u001b[1;32m    125\u001b[0m         cy \u001b[38;5;241m=\u001b[39m y1 \u001b[38;5;241m+\u001b[39m h_box \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    126\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(cls_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcx\u001b[38;5;241m/\u001b[39mw\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcy\u001b[38;5;241m/\u001b[39mh\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw_box\u001b[38;5;241m/\u001b[39mw\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh_box\u001b[38;5;241m/\u001b[39mh\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotated_bgr\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Write the annotated frame to the output video\u001b[39;00m\n\u001b[1;32m    129\u001b[0m frame_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    130\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Update progress bar\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "videos_predictions(best_pt_path, class_id_to_name, video_dir='videos', output_base='./datasets/final_output', max_frames=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872e10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== STARTED: v1 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 642/642 [00:37<00:00, 17.13it/s]\n",
      " 11%|â–ˆ         | 1/9 [00:37<05:00, 37.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v1 â€” Processed 642 frames\n",
      "\n",
      "========== STARTED: v10 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 259/259 [00:15<00:00, 16.41it/s]\n",
      " 22%|â–ˆâ–ˆâ–       | 2/9 [00:53<02:53, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v10 â€” Processed 259 frames\n",
      "\n",
      "========== STARTED: v2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175/175 [00:11<00:00, 15.84it/s]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [01:04<01:51, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v2 â€” Processed 175 frames\n",
      "\n",
      "========== STARTED: v3 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:01<00:00, 111.12it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [01:06<00:59, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v3 â€” Processed 176 frames\n",
      "\n",
      "========== STARTED: v4 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [00:05<00:00, 48.19it/s]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [01:11<00:37,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v4 â€” Processed 253 frames\n",
      "\n",
      "========== STARTED: v5 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [01:12<00:00, 15.39it/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [02:23<01:32, 30.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v5 â€” Processed 1114 frames\n",
      "\n",
      "========== STARTED: v6 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:28<00:00, 16.95it/s]\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [02:52<01:00, 30.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v6 â€” Processed 483 frames\n",
      "\n",
      "========== STARTED: v8 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 949/949 [00:29<00:00, 32.42it/s]\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [03:21<00:29, 29.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v8 â€” Processed 949 frames\n",
      "\n",
      "========== STARTED: v9 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2752/2752 [03:18<00:00, 13.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [06:40<00:00, 44.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v9 â€” Processed 2752 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "videos_predictions(best_pt_path_retrain, class_id_to_name, video_dir='videos', output_base='./datasets/final_output_retrain', max_frames=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
