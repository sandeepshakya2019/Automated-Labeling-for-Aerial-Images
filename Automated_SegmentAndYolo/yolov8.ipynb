{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074d4a4f",
   "metadata": {},
   "source": [
    "## Builting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150d5af",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "59ec5504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built CUDA Version: None\n",
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print the CUDA version PyTorch is built with\n",
    "print(\"Built CUDA Version:\", torch.version.cuda)\n",
    "\n",
    "# Print the CUDA version runtime (if CUDA is available)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Runtime Version:\", torch._C._cuda_getCompiledVersion())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "206e74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id_to_name = {\n",
    "    0:  ('road', [28, 42, 168]),\n",
    "    1:  ('pool', [0, 50, 89]),\n",
    "    2:  ('vegetation', [107, 142, 35]),\n",
    "    3:  ('roof', [70, 70, 70]),\n",
    "    4:  ('wall', [102, 102, 156]),\n",
    "    5:  ('window', [254, 228, 12]),\n",
    "    6:  ('person', [255, 22, 96]),\n",
    "    7:  ('dog', [102, 51, 0]),\n",
    "    8:  ('car', [9, 143, 150]),\n",
    "    9:  ('bicycle', [119, 11, 32]),\n",
    "    10: ('tree', [51, 51, 0]),\n",
    "    11: ('truck', [160, 160, 60]),   # added truck\n",
    "    12: ('bus', [200, 80, 80]),      # added bus\n",
    "    13: ('vehicle', [200, 80, 80]),      # added bus\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52746b61",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f7b06f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install opencv-python\n",
    "# !pip install pillow\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch torchvision\n",
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "892fe670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch torchvision torchaudio\n",
    "# !pip cache purge  # clean out pip's install cache\n",
    "# !pip install torch torchvision torchaudio --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cdfb2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core packages\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "# Math and array handling\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Image and visualization\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep Learning Frameworks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.models.segmentation as segmentation\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Object Detection and Segmentation\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Automatically use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import gdown\n",
    "\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# save this as split_uavdt_train_val.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd42d4",
   "metadata": {},
   "source": [
    "### Download Datsets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3f810e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_drone_dataset_download(gdrive_url, extract_to=\"extracted\"):\n",
    "    # Convert shared drive URL to direct download URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create output folder\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[INFO] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[INFO] Extracting ZIP...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Optionally, remove the ZIP file after extraction\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[DONE] Extracted files to: {extract_to}\")\n",
    "\n",
    "def uavdt_dataset_download(gdrive_url, extract_to=\"extracted\"):\n",
    "    # Convert shared drive URL to direct download URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create output folder\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[INFO] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[INFO] Extracting ZIP...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Optionally, remove the ZIP file after extraction\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[DONE] Extracted files to: {extract_to}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09011805",
   "metadata": {},
   "source": [
    "### Convert Two Datsets into yolo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "30ca98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Parse polygon and convert to YOLO bbox\n",
    "# ----------------------------\n",
    "# Semantic drone datasets \n",
    "def parse_yolo_style_bbox_from_xml(xml_path, class_id_to_name):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "        if class_name in [value[0] for value in class_id_to_name.values()]:\n",
    "            polygon = obj.find('polygon')\n",
    "            if polygon is not None:\n",
    "                points = polygon.findall('pt')\n",
    "                coords = [(float(pt.find('x').text), float(pt.find('y').text)) for pt in points]\n",
    "                x_min = min(coord[0] for coord in coords)\n",
    "                y_min = min(coord[1] for coord in coords)\n",
    "                x_max = max(coord[0] for coord in coords)\n",
    "                y_max = max(coord[1] for coord in coords)\n",
    "                bboxes.append(((x_min, y_min), (x_max, y_max), class_name))\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Save YOLO-format txt\n",
    "# ----------------------------\n",
    "def save_yolo_format(image_id, bboxes, image_width, image_height, output_path, class_id_to_name):\n",
    "    with open(output_path, 'w') as f:\n",
    "        for (x_min, y_min), (x_max, y_max), class_name in bboxes:\n",
    "            class_id = next(cid for cid, (name, _) in class_id_to_name.items() if name == class_name)\n",
    "            x_center = (x_min + x_max) / 2 / image_width\n",
    "            y_center = (y_min + y_max) / 2 / image_height\n",
    "            width = (x_max - x_min) / image_width\n",
    "            height = (y_max - y_min) / image_height\n",
    "            f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Convert dataset (YOLO only)\n",
    "# ----------------------------\n",
    "def convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name):\n",
    "    image_ids = [img.split('.')[0] for img in os.listdir(f\"{dataset_path}/images\") if img.endswith(\".jpg\")]\n",
    "\n",
    "    os.makedirs(f\"{output_dir}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/labels\", exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(image_ids, desc=\"Converting to YOLO\"):\n",
    "        img_path = f\"{dataset_path}/images/{image_id}.jpg\"\n",
    "        bbox_xml_path = f\"{dataset_path}/gt/bounding_box/label_me_xml/{image_id}.xml\"\n",
    "        semantic_xml_path = f\"{dataset_path}/gt/semantic/label_me_xml/{image_id}.xml\"\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"[WARNING] Image not found: {img_path}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            bboxes1 = parse_yolo_style_bbox_from_xml(bbox_xml_path, class_id_to_name)\n",
    "            bboxes2 = parse_yolo_style_bbox_from_xml(semantic_xml_path, class_id_to_name)\n",
    "            all_bboxes = bboxes1 + bboxes2\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Skipping image {image_id} due to parse error: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            image_np = np.array(image)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not load image {image_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save image\n",
    "        image.save(f\"{output_dir}/images/{image_id}.jpg\")\n",
    "\n",
    "        # Save YOLO labels\n",
    "        yolo_annotation_path = f\"{output_dir}/labels/{image_id}.txt\"\n",
    "        save_yolo_format(image_id, all_bboxes, image_np.shape[1], image_np.shape[0], yolo_annotation_path, class_id_to_name)\n",
    "\n",
    "    print(\"✅ YOLO-format annotation conversion complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3a94cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 🧠 Map UAVDT class to extended class_id_to_name\n",
    "uavdt_to_extended = {\n",
    "    0: 8,   # car\n",
    "    1: 11,  # truck\n",
    "    2: 12,  # bus\n",
    "    3: 13\n",
    "}\n",
    "\n",
    "# === Function to Convert Single Annotation to YOLO Format ===\n",
    "def convert_annotation(anno_path, label_path, image_path, stats):\n",
    "    if not os.path.exists(image_path):\n",
    "        stats[\"missing_image\"] += 1\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        height, width = img.shape[:2]\n",
    "    except:\n",
    "        stats[\"missing_image\"] += 1\n",
    "        return\n",
    "\n",
    "    with open(anno_path, 'r') as fin, open(label_path, 'w') as fout:\n",
    "        for line in fin:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) < 8:\n",
    "                stats[\"malformed\"] += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                x, y, w, h = map(float, parts[0:4])\n",
    "                original_cls = int(parts[5])\n",
    "\n",
    "                # 🔁 Convert original class to extended class\n",
    "                if original_cls not in uavdt_to_extended:\n",
    "                    stats[\"skipped\"][original_cls] += 1\n",
    "                    continue\n",
    "\n",
    "                cls = uavdt_to_extended[original_cls]\n",
    "\n",
    "                x_center = (x + w / 2) / width\n",
    "                y_center = (y + h / 2) / height\n",
    "                w /= width\n",
    "                h /= height\n",
    "\n",
    "                if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and w > 0 and h > 0):\n",
    "                    stats[\"skipped\"][cls] += 1\n",
    "                    continue\n",
    "\n",
    "                fout.write(f\"{cls} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "                stats[\"converted\"] += 1\n",
    "            except Exception:\n",
    "                stats[\"malformed\"] += 1\n",
    "                continue\n",
    "\n",
    "            stats[\"total\"] += 1\n",
    "\n",
    "# === Step 1: Convert UAVDT annotations to YOLO format ===\n",
    "def convert_dataset(root_dir):\n",
    "    annotation_paths = glob(os.path.join(root_dir, \"M*/annotations/*.txt\"))\n",
    "    total_files = len(annotation_paths)\n",
    "\n",
    "    stats = {\n",
    "        \"total\": 0,\n",
    "        \"converted\": 0,\n",
    "        \"malformed\": 0,\n",
    "        \"missing_image\": 0,\n",
    "        \"skipped\": defaultdict(int)\n",
    "    }\n",
    "\n",
    "    print(f\"🔄 Converting {total_files} annotation files to YOLO format...\")\n",
    "\n",
    "    for anno_path in tqdm(annotation_paths, desc=\"Converting\", unit=\"file\"):\n",
    "        sequence_dir = os.path.dirname(os.path.dirname(anno_path))  # Mxxxx\n",
    "        file_name = os.path.basename(anno_path)\n",
    "\n",
    "        label_dir = os.path.join(sequence_dir, \"labels\")\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "        label_path = os.path.join(label_dir, file_name)\n",
    "\n",
    "        # Construct image path\n",
    "        image_name = file_name.replace(\".txt\", \".jpg\")\n",
    "        image_path = os.path.join(sequence_dir, \"images\", image_name)\n",
    "\n",
    "        convert_annotation(anno_path, label_path, image_path, stats)\n",
    "\n",
    "    print(\"\\nConversion complete.\")\n",
    "    print(f\"Total boxes:     {stats['total']}\")\n",
    "    print(f\"Converted boxes: {stats['converted']}\")\n",
    "    print(f\"Skipped boxes:   {sum(stats['skipped'].values())}\")\n",
    "    for cls, count in sorted(stats[\"skipped\"].items()):\n",
    "        print(f\"   - Skipped class {cls}: {count}\")\n",
    "    print(f\"Malformed lines: {stats['malformed']}\")\n",
    "    print(f\"Missing images: {stats['missing_image']}\")\n",
    "\n",
    "# === Step 2: Copy to train/val structure ===\n",
    "def copy_split_sequences(src_root, dst_root, train_ratio=0.8):\n",
    "    all_sequences = sorted(glob(os.path.join(src_root, \"M*\")))\n",
    "    train_seqs, val_seqs = train_test_split(all_sequences, train_size=train_ratio, random_state=42)\n",
    "\n",
    "    for split_name, split_list in zip(['train', 'val'], [train_seqs, val_seqs]):\n",
    "        for seq_path in tqdm(split_list, desc=f\"Copying {split_name}\"):\n",
    "            images_src = os.path.join(seq_path, \"images\")\n",
    "            labels_src = os.path.join(seq_path, \"labels\")\n",
    "\n",
    "            images_dst = os.path.join(dst_root, split_name, \"images\")\n",
    "            labels_dst = os.path.join(dst_root, split_name, \"labels\")\n",
    "\n",
    "            os.makedirs(images_dst, exist_ok=True)\n",
    "            os.makedirs(labels_dst, exist_ok=True)\n",
    "\n",
    "            for img_file in glob(os.path.join(images_src, \"*.jpg\")):\n",
    "                shutil.copy(img_file, os.path.join(images_dst, os.path.basename(img_file)))\n",
    "\n",
    "            for label_file in glob(os.path.join(labels_src, \"*.txt\")):\n",
    "                shutil.copy(label_file, os.path.join(labels_dst, os.path.basename(label_file)))\n",
    "\n",
    "    print(\"\\nDataset split into 'train/' and 'val/' with images and YOLO labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b9227",
   "metadata": {},
   "source": [
    "### Convert into train and Val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "36ec4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Drone Datasets\n",
    "def move_files(file_list, \n",
    "               source_image_dir, \n",
    "               source_annotation_dir,\n",
    "               target_image_dir, \n",
    "               target_annotation_dir):\n",
    "    \n",
    "    os.makedirs(target_image_dir, exist_ok=True)\n",
    "    os.makedirs(target_annotation_dir, exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(file_list, desc=f\"Moving to {os.path.basename(os.path.dirname(target_image_dir))}\"):\n",
    "        image_path = os.path.join(source_image_dir, f\"{image_id}.jpg\")\n",
    "        annotation_path = os.path.join(source_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        target_image_path = os.path.join(target_image_dir, f\"{image_id}.jpg\")\n",
    "        target_annotation_path = os.path.join(target_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            shutil.copy(image_path, target_image_path)\n",
    "        else:\n",
    "            print(f\"[Warning] Missing image: {image_path}\")\n",
    "\n",
    "        if os.path.exists(annotation_path):\n",
    "            shutil.copy(annotation_path, target_annotation_path)\n",
    "        else:\n",
    "            print(f\"[Warning] Missing annotation: {annotation_path}\")\n",
    "\n",
    "def split_and_move_dataset(source_base_dir=\"./datasets/semantic_yolo\",\n",
    "                           target_base_dir=\"./datasets/new_dataset_yolo_split\",\n",
    "                           split_ratio=0.8,\n",
    "                           seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    image_dir = os.path.join(source_base_dir, \"images\")\n",
    "    label_dir = os.path.join(source_base_dir, \"labels\")\n",
    "\n",
    "    image_ids = [os.path.splitext(f)[0] for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "    random.shuffle(image_ids)\n",
    "\n",
    "    split_idx = int(len(image_ids) * split_ratio)\n",
    "    train_ids = image_ids[:split_idx]\n",
    "    val_ids = image_ids[split_idx:]\n",
    "\n",
    "    # Train\n",
    "    move_files(train_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"train/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"train/labels\"))\n",
    "\n",
    "    # Val\n",
    "    move_files(val_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"val/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"val/labels\"))\n",
    "\n",
    "    print(f\"\\n[✓] Dataset split completed: {len(train_ids)} train / {len(val_ids)} val samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110fe96d",
   "metadata": {},
   "source": [
    "### Normalize Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "541e4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label_file(label_file, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Normalize the label coordinates in a label file to ensure they are within [0, 1] range.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    with open(label_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, width, height = map(float, parts[1:])\n",
    "            \n",
    "            # Normalize coordinates to ensure they are within the range [0, 1]\n",
    "            x_center = min(1.0, max(0.0, x_center))\n",
    "            y_center = min(1.0, max(0.0, y_center))\n",
    "            width = min(1.0, max(0.0, width))\n",
    "            height = min(1.0, max(0.0, height))\n",
    "\n",
    "            # Write normalized values back to file\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "\n",
    "def get_image_size(img_path):\n",
    "    \"\"\"\n",
    "    Get the width and height of the image to normalize the coordinates properly.\n",
    "    \"\"\"\n",
    "    with Image.open(img_path) as img:\n",
    "        return img.size  # returns (width, height)\n",
    "\n",
    "\n",
    "def normalize_all_labels(labels_dir, img_dir):\n",
    "    \"\"\"\n",
    "    Normalize all label files in the specified directory.\n",
    "    \"\"\"\n",
    "    for label_file in tqdm(os.listdir(labels_dir)):\n",
    "       \n",
    "        if label_file.endswith('.txt'):  # Process only label files\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            img_path = os.path.join(img_dir, label_file.replace('.txt', '.jpg'))  # Assuming JPG images\n",
    "            if os.path.exists(img_path):\n",
    "                # Get image dimensions to normalize the labels\n",
    "                img_width, img_height = get_image_size(img_path)\n",
    "                # print(f\"Normalizing {label_file}...\")\n",
    "                normalize_label_file(label_path, img_width, img_height)\n",
    "            else:\n",
    "                print(f\"Warning: Image for label {label_file} not found!\")\n",
    "    print(\"Normalize Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce6fe5",
   "metadata": {},
   "source": [
    "### Training v8 model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f2925090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo(data_yaml, epochs, imgsz, batch, name, model, dir):\n",
    "    if(model):\n",
    "        model = YOLO(dir)\n",
    "    else:\n",
    "        model = YOLO(\"yolov8m.pt\")\n",
    "\n",
    "    model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        name=name,\n",
    "        project=\"runs/train\",\n",
    "        patience=30,  # Early stopping\n",
    "        augment=True,  # Apply augmentations\n",
    "        degrees=10,  # Image rotation\n",
    "        scale=0.5,  # Scale range\n",
    "        flipud=0.2,  # Vertical flip\n",
    "        fliplr=0.5,  # Horizontal flip\n",
    "        hsv_h=0.015,  # Hue augmentation\n",
    "        hsv_s=0.7,  # Saturation augmentation\n",
    "        hsv_v=0.4,  # Value augmentation\n",
    "        mosaic=1.0,  # Mosaic augmentation\n",
    "        mixup=0.2,  # Mixup augmentation\n",
    "        lr0=0.01,  # Initial learning rate (you can tune this)\n",
    "        lrf=0.01,  # Learning rate final factor (for cosine annealing)\n",
    "        verbose=True  # Print progress\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ff318",
   "metadata": {},
   "source": [
    "### Print val metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bf85d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def load_yolo_model(model_path):\n",
    "    return YOLO(model_path)\n",
    "\n",
    "def run_model_validation(model):\n",
    "    return model.val()\n",
    "\n",
    "def extract_per_class_metrics(results):\n",
    "    \"\"\"\n",
    "    Extracts mAP@0.5:0.95 per class from results.\n",
    "    NOTE: Only mAP@0.5:0.95 is available via `results.box.maps`\n",
    "    \"\"\"\n",
    "    per_class_metrics = {}\n",
    "    if hasattr(results.box, 'maps') and results.box.maps is not None:\n",
    "        maps = results.box.maps  # This is a NumPy array [num_classes]\n",
    "        for i, name in results.names.items():\n",
    "            per_class_metrics[name] = {\n",
    "                \"class_id\": i,\n",
    "                \"mAP@0.5:0.95\": round(float(maps[i]), 4)\n",
    "            }\n",
    "    else:\n",
    "        print(\"⚠️ No per-class mAP@0.5:0.95 data found.\")\n",
    "    return per_class_metrics\n",
    "\n",
    "def save_metrics_to_json(metrics, output_path):\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(f\"✅ Saved per-class metrics to {output_path}\")\n",
    "\n",
    "def evaluate_and_save_metrics(model_path, output_json_path=\"per_class_metrics.json\"):\n",
    "    model = load_yolo_model(model_path)\n",
    "    results = run_model_validation(model)\n",
    "    metrics = extract_per_class_metrics(results)\n",
    "    save_metrics_to_json(metrics, output_json_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "65ca8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def print_per_class_metrics(json_path=\"per_class_metrics.json\"):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(\"📊 Per-Class mAP@0.5:0.95 Metrics:\\n\")\n",
    "    print(f\"{'Class Name':<15} {'Class ID':<10} {'mAP@0.5:0.95':<15}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for name, data in metrics.items():\n",
    "        print(f\"{name:<15} {data['class_id']:<10} {data['mAP@0.5:0.95']:<15}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22b1a8",
   "metadata": {},
   "source": [
    "### Find best model path after training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d0b390e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(base_dir='runs_yolo/'):\n",
    "    best_paths = list(Path(base_dir).rglob('best.pt'))\n",
    "    if not best_paths:\n",
    "        raise FileNotFoundError(\"No 'best.pt' file found in the 'runs/' directory.\")\n",
    "    \n",
    "    # Optionally, sort by latest modified time\n",
    "    best_paths.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    print(f\"✅ Found best.pt at: {best_paths[0]}\")\n",
    "    return str(best_paths[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378bc85",
   "metadata": {},
   "source": [
    "### Prediction on vdieos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a013ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== FRAME PROCESSING ==========\n",
    "def process_frame_1(frame, yolo_model, w, h, class_id_to_name):\n",
    "    annotated = frame.copy()\n",
    "    results = yolo_model(annotated, verbose=False)[0]\n",
    "    boxes = results.boxes.xyxy.cpu().numpy()\n",
    "    class_ids = results.boxes.cls.cpu().numpy()\n",
    "\n",
    "    for box, cls_id in zip(boxes, class_ids):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        class_name, color = class_id_to_name[int(cls_id)]\n",
    "        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(annotated, class_name, (x1, max(y1 - 10, 10)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "    return annotated, boxes, class_ids\n",
    "\n",
    "# ========== VIDEO CAPTURE ==========\n",
    "def setup_video_capture_1(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    return cap, total_frames, fps, w, h\n",
    "\n",
    "# ========== MAIN FUNCTION ==========\n",
    "def videos_predictions(yolo_weights_path, class_id_to_name, video_dir='videos', output_base='./datatsets/opt', max_frames=None):\n",
    "    yolo_model = YOLO(yolo_weights_path)\n",
    "\n",
    "    image_out_dir = os.path.join(output_base, 'images')\n",
    "    label_out_dir = os.path.join(output_base, 'labels')\n",
    "    output_video_dir = os.path.join(output_base, 'output')\n",
    "\n",
    "    os.makedirs(image_out_dir, exist_ok=True)\n",
    "    os.makedirs(label_out_dir, exist_ok=True)\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    for video_file in tqdm(sorted(os.listdir(video_dir))):\n",
    "        if not video_file.lower().endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        video_id = os.path.splitext(video_file)[0]\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        output_video_path = os.path.join(output_video_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        print(f\"\\n========== STARTED: {video_id} ==========\")\n",
    "        cap, total_frames, fps, w, h = setup_video_capture_1(video_path)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(output_video_path, fourcc, fps, (w, h))\n",
    "\n",
    "        frame_count = 0\n",
    "        pbar = tqdm(total=max_frames if max_frames else total_frames, desc=video_id)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or (max_frames and frame_count >= max_frames):\n",
    "                break\n",
    "\n",
    "            annotated_bgr, boxes, class_ids = process_frame_1(frame, yolo_model, w, h, class_id_to_name)\n",
    "\n",
    "            # ✅ Save original image\n",
    "            img_filename = f'{video_id}_{frame_count:04d}.jpg'\n",
    "            img_path = os.path.join(image_out_dir, img_filename)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "\n",
    "            # ✅ Save YOLO-format label\n",
    "            label_filename = f'{video_id}_{frame_count:04d}.txt'\n",
    "            label_path = os.path.join(label_out_dir, label_filename)\n",
    "            with open(label_path, 'w') as f:\n",
    "                for box, cls_id in zip(boxes, class_ids):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    w_box = x2 - x1\n",
    "                    h_box = y2 - y1\n",
    "                    cx = x1 + w_box / 2\n",
    "                    cy = y1 + h_box / 2\n",
    "                    f.write(f\"{int(cls_id)} {cx/w:.6f} {cy/h:.6f} {w_box/w:.6f} {h_box/h:.6f}\\n\")\n",
    "\n",
    "            writer.write(annotated_bgr)\n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "        pbar.close()\n",
    "        print(f\"DONE: {video_id} — Processed {frame_count} frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ce3db596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ========== FRAME PROCESSING ==========\n",
    "def process_frame(frame, yolo_model, w, h, class_id_to_name, valid_class_ids, conf_threshold=0.5):\n",
    "    annotated = frame.copy()\n",
    "    results = yolo_model(annotated, verbose=False)[0]\n",
    "\n",
    "    # Filter by confidence\n",
    "    mask = results.boxes.conf > conf_threshold\n",
    "    boxes = results.boxes.xyxy[mask].cpu().numpy()\n",
    "    class_ids = results.boxes.cls[mask].cpu().numpy()\n",
    "    confs = results.boxes.conf[mask].cpu().numpy()\n",
    "\n",
    "    filtered_boxes, filtered_ids, filtered_confs = [], [], []\n",
    "\n",
    "    for box, cls_id, conf in zip(boxes, class_ids, confs):\n",
    "        if int(cls_id) in valid_class_ids:\n",
    "            filtered_boxes.append(box)\n",
    "            filtered_ids.append(cls_id)\n",
    "            filtered_confs.append(conf)\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            class_name, color = class_id_to_name[int(cls_id)]\n",
    "            label = f\"{class_name} {conf:.2f}\"\n",
    "            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(annotated, label, (x1, max(y1 - 10, 10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "    return annotated, filtered_boxes, filtered_ids\n",
    "\n",
    "# ========== VIDEO CAPTURE ==========\n",
    "def setup_video_capture(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    return cap, total_frames, fps, w, h\n",
    "\n",
    "# ========== MAIN FUNCTION ==========\n",
    "def process_all_videos(yolo_weights_path, class_id_to_name, underrepresented_class_ids,\n",
    "                       video_dir='videos', output_base='./datasets/opt', max_frames=None):\n",
    "    yolo_model = YOLO(yolo_weights_path)\n",
    "\n",
    "    image_out_dir = os.path.join(output_base, 'images')\n",
    "    label_out_dir = os.path.join(output_base, 'labels')\n",
    "    output_video_dir = os.path.join(output_base, 'output')\n",
    "\n",
    "    os.makedirs(image_out_dir, exist_ok=True)\n",
    "    os.makedirs(label_out_dir, exist_ok=True)\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    for video_file in tqdm(sorted(os.listdir(video_dir))):\n",
    "        if not video_file.lower().endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        video_id = os.path.splitext(video_file)[0]\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        output_video_path = os.path.join(output_video_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        print(f\"\\n========== STARTED: {video_id} ==========\")\n",
    "        cap, total_frames, fps, w, h = setup_video_capture(video_path)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(output_video_path, fourcc, fps, (w, h))\n",
    "\n",
    "        frame_count = 0\n",
    "        pbar = tqdm(total=max_frames if max_frames else total_frames, desc=video_id)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or (max_frames and frame_count >= max_frames):\n",
    "                break\n",
    "\n",
    "            annotated_bgr, boxes, class_ids = process_frame(\n",
    "                frame, yolo_model, w, h, class_id_to_name, underrepresented_class_ids)\n",
    "\n",
    "            # ✅ Save original image\n",
    "            img_filename = f'{video_id}_{frame_count:04d}.jpg'\n",
    "            img_path = os.path.join(image_out_dir, img_filename)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "\n",
    "            # ✅ Save YOLO-format label (only if boxes found)\n",
    "            if boxes:\n",
    "                label_filename = f'{video_id}_{frame_count:04d}.txt'\n",
    "                label_path = os.path.join(label_out_dir, label_filename)\n",
    "                with open(label_path, 'w') as f:\n",
    "                    for box, cls_id in zip(boxes, class_ids):\n",
    "                        x1, y1, x2, y2 = box\n",
    "                        w_box = x2 - x1\n",
    "                        h_box = y2 - y1\n",
    "                        cx = x1 + w_box / 2\n",
    "                        cy = y1 + h_box / 2\n",
    "                        f.write(f\"{int(cls_id)} {cx/w:.6f} {cy/h:.6f} {w_box/w:.6f} {h_box/h:.6f}\\n\")\n",
    "\n",
    "            writer.write(annotated_bgr)\n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "        pbar.close()\n",
    "        print(f\"DONE: {video_id} — Processed {frame_count} frames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70f244",
   "metadata": {},
   "source": [
    "### Get Class Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bcf3c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_rare_class_ids(label_dir, class_id_to_name, rare_threshold=1000):\n",
    "    class_counts = defaultdict(int)\n",
    "\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        if not label_file.endswith('.txt'):\n",
    "            continue\n",
    "        with open(os.path.join(label_dir, label_file), 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 1:\n",
    "                    cls_id = int(parts[0])\n",
    "                    class_counts[cls_id] += 1\n",
    "\n",
    "    print(\"🔍 Class-wise instance counts:\")\n",
    "    for cls_id in sorted(class_counts.keys()):\n",
    "        name = class_id_to_name.get(cls_id, (\"Unknown\", []))[0]\n",
    "        count = class_counts[cls_id]\n",
    "        print(f\"Class {cls_id:2d} ({name:10s}): {count} instances\")\n",
    "\n",
    "    rare_class_ids = {cls_id for cls_id, count in class_counts.items() if count < rare_threshold}\n",
    "    print(f\"\\n✅ Rare class IDs (threshold < {rare_threshold}): {rare_class_ids}\")\n",
    "\n",
    "    return rare_class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c030e",
   "metadata": {},
   "source": [
    "### Merge predciton and previous datatsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0bdc3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def merge_yolo_datasets(source1, source2, destination):\n",
    "    # Define subfolders\n",
    "    img1_dir = os.path.join(source1, 'images')\n",
    "    lbl1_dir = os.path.join(source1, 'labels')\n",
    "    img2_dir = os.path.join(source2, 'images')\n",
    "    lbl2_dir = os.path.join(source2, 'labels')\n",
    "    dst_img_dir = os.path.join(destination, 'images')\n",
    "    dst_lbl_dir = os.path.join(destination, 'labels')\n",
    "\n",
    "    # Create destination folders\n",
    "    os.makedirs(dst_img_dir, exist_ok=True)\n",
    "    os.makedirs(dst_lbl_dir, exist_ok=True)\n",
    "\n",
    "    def copy_files(src_img_dir, src_lbl_dir, prefix):\n",
    "        for filename in sorted(os.listdir(src_img_dir)):\n",
    "            if not filename.lower().endswith('.jpg'):\n",
    "                continue\n",
    "            base = os.path.splitext(filename)[0]\n",
    "\n",
    "            # Image\n",
    "            new_img_name = f\"{prefix}_{base}.jpg\"\n",
    "            shutil.copy(os.path.join(src_img_dir, filename),\n",
    "                        os.path.join(dst_img_dir, new_img_name))\n",
    "\n",
    "            # Label\n",
    "            label_file = base + \".txt\"\n",
    "            if os.path.exists(os.path.join(src_lbl_dir, label_file)):\n",
    "                new_lbl_name = f\"{prefix}_{base}.txt\"\n",
    "                shutil.copy(os.path.join(src_lbl_dir, label_file),\n",
    "                            os.path.join(dst_lbl_dir, new_lbl_name))\n",
    "            else:\n",
    "                print(f\"⚠️ No label for {filename}\")\n",
    "\n",
    "    print(\"🔁 Merging original dataset...\")\n",
    "    copy_files(img1_dir, lbl1_dir, prefix=\"orig\")\n",
    "\n",
    "    print(\"➕ Merging predicted video dataset...\")\n",
    "    copy_files(img2_dir, lbl2_dir, prefix=\"pred\")\n",
    "\n",
    "    print(f\"\\n✅ Merge complete! Merged dataset at: {destination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adccd711",
   "metadata": {},
   "source": [
    "### Print Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fbbb0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_results_csv(directory):\n",
    "    \"\"\"Find the results.csv file in the specified directory.\"\"\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if 'results.csv' in files:\n",
    "            return os.path.join(root, 'results.csv')\n",
    "    return None\n",
    "\n",
    "def load_results_csv(results_csv_path):\n",
    "    \"\"\"Load the results CSV into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(results_csv_path)\n",
    "\n",
    "def calculate_total_epochs(df):\n",
    "    \"\"\"Calculate the total number of epochs from the DataFrame.\"\"\"\n",
    "    return df['epoch'].max()\n",
    "\n",
    "def calculate_training_loss(epoch_data):\n",
    "    \"\"\"Calculate the total training loss from the given epoch data.\"\"\"\n",
    "    train_box_loss = epoch_data['train/box_loss']\n",
    "    train_cls_loss = epoch_data['train/cls_loss']\n",
    "    train_dfl_loss = epoch_data['train/dfl_loss']\n",
    "    return train_box_loss + train_cls_loss + train_dfl_loss\n",
    "\n",
    "def calculate_validation_loss(epoch_data):\n",
    "    \"\"\"Calculate the total validation loss from the given epoch data.\"\"\"\n",
    "    val_box_loss = epoch_data['val/box_loss']\n",
    "    val_cls_loss = epoch_data['val/cls_loss']\n",
    "    val_dfl_loss = epoch_data['val/dfl_loss']\n",
    "    return val_box_loss + val_cls_loss + val_dfl_loss\n",
    "\n",
    "def print_final_metrics(df):\n",
    "    \"\"\"Print the final metrics for the last epoch.\"\"\"\n",
    "    final_epoch_data = df.iloc[-1]\n",
    "\n",
    "    # Calculate total training and validation loss\n",
    "    train_loss = calculate_training_loss(final_epoch_data)\n",
    "    val_loss = calculate_validation_loss(final_epoch_data)\n",
    "\n",
    "    # Print overall metrics\n",
    "    print(\"\\n========== Final Training Metrics ==========\")\n",
    "    print(f\"Training Loss: {train_loss:.6f}\")\n",
    "    print(f\"Precision: {final_epoch_data['metrics/precision(B)']:.6f}\")\n",
    "    print(f\"Recall: {final_epoch_data['metrics/recall(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5: {final_epoch_data['metrics/mAP50(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5:0.95: {final_epoch_data['metrics/mAP50-95(B)']:.6f}\")\n",
    "\n",
    "    print(\"\\n========== Final Validation Metrics ==========\")\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "\n",
    "def print_csv_metrics(directory):\n",
    "    \"\"\"Main function to process and print final metrics.\"\"\"\n",
    "    # Find the results.csv file\n",
    "    results_csv_path = find_results_csv(directory)\n",
    "    \n",
    "    if not results_csv_path:\n",
    "        print(\"Error: 'results.csv' file not found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found results.csv at: {results_csv_path}\")\n",
    "\n",
    "    # Load results CSV\n",
    "    df = load_results_csv(results_csv_path)\n",
    "\n",
    "    # Get the total number of epochs\n",
    "    total_epochs = calculate_total_epochs(df)\n",
    "    print(f\"Total number of epochs: {total_epochs}\")\n",
    "\n",
    "    # Print columns in the CSV\n",
    "    # print(\"\\n========== Columns in CSV ==========\")\n",
    "    # print(df.columns)\n",
    "\n",
    "    # Print final metrics\n",
    "    print_final_metrics(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b82a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b0324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "366d7bab",
   "metadata": {},
   "source": [
    "## Calling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f592f31",
   "metadata": {},
   "source": [
    "### Download, convert, split and normalize datsets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "54ca66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdrive_url = \"https://drive.google.com/file/d/1UppumYqYOi-kto6BWPfFxwJK2Eph46oY/view?usp=sharing\"\n",
    "# semantic_drone_dataset_download(gdrive_url, extract_to=\"datasets\")\n",
    "\n",
    "# gdrive_url = \"https://drive.google.com/file/d/12cbrTaBAMIsuU-mwAA7IgDk9wSLC9cC-/view?usp=sharing\"\n",
    "# uavdt_dataset_download(gdrive_url, extract_to=\"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "558387bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"./datasets/semantic_drone_dataset/training_set\"\n",
    "# output_dir = \"./datasets/semantic_yolo\"\n",
    "\n",
    "# convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "94505830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #UAVDT-2024\n",
    "\n",
    "# source_root = \"./datasets/UAVDT-2024\"\n",
    "# output_root = \"./datasets/new_dataset_yolo_split\"\n",
    "\n",
    "# convert_dataset(source_root)\n",
    "# copy_split_sequences(source_root, output_root, train_ratio=0.8)\n",
    "\n",
    "\n",
    "# # Semantic dorne datasets\n",
    "# split_and_move_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ec08031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set your paths\n",
    "# dataset_path = \"./datasets/new_dataset_yolo_split/train\"\n",
    "# image_dir = os.path.join(dataset_path, \"images\")\n",
    "# annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# normalize_all_labels(annotations_dir, image_dir)\n",
    "\n",
    "# dataset_path = \"./datasets/new_dataset_yolo_split/val\"\n",
    "# image_dir = os.path.join(dataset_path, \"images\")\n",
    "# annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# normalize_all_labels(annotations_dir, image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88e922",
   "metadata": {},
   "source": [
    "### Checking how many classes have how much instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "eeb923ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_dir = './datasets/new_dataset_yolo_split/train/labels'\n",
    "\n",
    "# rare_class_ids = get_rare_class_ids(label_dir=labels_dir, class_id_to_name=class_id_to_name ,rare_threshold=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63556279",
   "metadata": {},
   "source": [
    "### Training v8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "56985371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # List of folders to delete\n",
    "# folders_to_delete = ['./datasets/semantic_yolo', './datasets/new_dataset_yolo', './datasets/uavdt-processed', './runs', \"./metrics\"]\n",
    "\n",
    "# for folder_path in folders_to_delete:\n",
    "#     if os.path.exists(folder_path):\n",
    "#         shutil.rmtree(folder_path)\n",
    "#         print(f\"✅ Deleted folder: {folder_path}\")\n",
    "#     else:\n",
    "#         print(f\"⚠️ Folder does not exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "735f81a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"[+] Training Start\")\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# train_yolo(\"yolov8.yaml\", 80, 720, 8, \"yolov8\", False, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c39cf0",
   "metadata": {},
   "source": [
    "### Print metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "830c11fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found best.pt at: runs\\train\\yolov8\\weights\\best.pt\n"
     ]
    }
   ],
   "source": [
    "yolov8 = './runs/train/yolov8'\n",
    "best_pt_path = find_best_model(yolov8)\n",
    "# evaluate_and_save_metrics(best_pt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "44ab66bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Per-Class mAP@0.5:0.95 Metrics:\n",
      "\n",
      "Class Name      Class ID   mAP@0.5:0.95   \n",
      "----------------------------------------\n",
      "unlabeled       0          0.3008         \n",
      "pool            1          0.8458         \n",
      "vegetation      2          0.1329         \n",
      "roof            3          0.5681         \n",
      "wall            4          0.1462         \n",
      "window          5          0.2316         \n",
      "person          6          0.2769         \n",
      "dog             7          0.4108         \n",
      "car             8          0.3159         \n",
      "bicycle         9          0.3551         \n",
      "tree            10         0.4516         \n",
      "truck           11         0.0336         \n",
      "bus             12         0.0            \n",
      "vehicle         13         0.1412         \n"
     ]
    }
   ],
   "source": [
    "print_per_class_metrics(\"per_class_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d15b47f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found results.csv at: ./runs/train/yolov8\\results.csv\n",
      "Total number of epochs: 80\n",
      "\n",
      "========== Final Training Metrics ==========\n",
      "Training Loss: 1.535840\n",
      "Precision: 0.535960\n",
      "Recall: 0.359830\n",
      "mAP@0.5: 0.411080\n",
      "mAP@0.5:0.95: 0.289710\n",
      "\n",
      "========== Final Validation Metrics ==========\n",
      "Validation Loss: 5.173490\n"
     ]
    }
   ],
   "source": [
    "print_csv_metrics(yolov8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230e616",
   "metadata": {},
   "source": [
    "### Prediciton videso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "01fe5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # List of folders to delete\n",
    "# folders_to_delete = ['./datasets/new-videos-predicted-yolo', \"./datasets/merged_yolo_dataset\"]\n",
    "\n",
    "# for folder_path in folders_to_delete:\n",
    "#     if os.path.exists(folder_path):\n",
    "#         shutil.rmtree(folder_path)\n",
    "#         print(f\"✅ Deleted folder: {folder_path}\")\n",
    "#     else:\n",
    "#         print(f\"⚠️ Folder does not exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9cc5a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_all_videos(best_pt_path, class_id_to_name, rare_class_ids, video_dir='videos', output_base='./datasets/new-videos-predicted-yolo', max_frames=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f4fce1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Folder with YOLO label files\n",
    "# label_dir = './datasets/new-videos-predicted-yolo/labels'\n",
    "\n",
    "# get_rare_class_ids(label_dir=label_dir, class_id_to_name=class_id_to_name ,rare_threshold=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3013569",
   "metadata": {},
   "source": [
    "### Merge previous and new prediction Ddatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3807d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge_yolo_datasets(\n",
    "#     source1='./datasets/new_dataset_yolo_split/train',\n",
    "#     source2='./datasets/new-videos-predicted-yolo',\n",
    "#     destination='./datasets/merged_yolo_dataset'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1eeb6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Folder with YOLO label files\n",
    "# label_dir = './datasets/merged_yolo_dataset/labels'\n",
    "\n",
    "# rare_class_ids = get_rare_class_ids(label_dir=label_dir, class_id_to_name=class_id_to_name ,rare_threshold=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2ca7c",
   "metadata": {},
   "source": [
    "### Retrain Model on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d7f1df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# import glob\n",
    "\n",
    "# # Match all folders starting with 'fine-tune-yolov8' inside './runs_yolo/train/'\n",
    "# folders_to_delete = glob.glob('./runs/train/fine-tune-yolov8*')\n",
    "\n",
    "# for folder_path in folders_to_delete:\n",
    "#     if os.path.isdir(folder_path):\n",
    "#         shutil.rmtree(folder_path)\n",
    "#         print(f\"✅ Deleted folder: {folder_path}\")\n",
    "#     else:\n",
    "#         print(f\"⚠️ Not a directory or doesn't exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "103f6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_yolo(\"yolo_retrain.yaml\", 10, 720, 8, \"fine-tune-yolov8\", True, dir=best_pt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6a5f6",
   "metadata": {},
   "source": [
    "### Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "28391f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found results.csv at: ./runs/train/fine-tune-yolov8\\results.csv\n",
      "Total number of epochs: 10\n",
      "\n",
      "========== Final Training Metrics ==========\n",
      "Training Loss: 2.290150\n",
      "Precision: 0.632960\n",
      "Recall: 0.548170\n",
      "mAP@0.5: 0.621000\n",
      "mAP@0.5:0.95: 0.526450\n",
      "\n",
      "========== Final Validation Metrics ==========\n",
      "Validation Loss: 2.578870\n"
     ]
    }
   ],
   "source": [
    "new_path = './runs/train/fine-tune-yolov8'\n",
    "print_csv_metrics(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b1ac9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_final_metrics(csv1_path, csv2_path):\n",
    "    # Load both result CSVs\n",
    "    df1 = pd.read_csv(csv1_path)\n",
    "    df2 = pd.read_csv(csv2_path)\n",
    "    # print(df1.head())\n",
    "    # Use the final row (last epoch)\n",
    "    last1 = df1.iloc[-1]\n",
    "    last2 = df2.iloc[-1]\n",
    "\n",
    "    metrics_to_compare = {\n",
    "        \"train/box_loss\": \"Box Loss (Train)\",\n",
    "        \"train/cls_loss\": \"Cls Loss (Train)\",\n",
    "        \"train/dfl_loss\": \"DFL Loss (Train)\",\n",
    "        \"metrics/precision(B)\": \"Precision\",\n",
    "        \"metrics/recall(B)\": \"Recall\",\n",
    "        \"metrics/mAP50(B)\": \"mAP@0.5\",\n",
    "        \"metrics/mAP50-95(B)\": \"mAP@0.5:0.95\",\n",
    "        \"val/box_loss\": \"Box Loss (Val)\",\n",
    "        \"val/cls_loss\": \"Cls Loss (Val)\",\n",
    "        \"val/dfl_loss\": \"DFL Loss (Val)\"\n",
    "    }\n",
    "\n",
    "    print(\"🔍 Comparison of Final Epoch Metrics:\\n\")\n",
    "    for key, label in metrics_to_compare.items():\n",
    "        val1 = last1[key]\n",
    "        val2 = last2[key]\n",
    "        trend = \"✅ Good Increase\" if val2 > val1 else \"❌ No Increase\"\n",
    "        print(f\"{label:20s}: {val1:.5f} → {val2:.5f} | {trend}\")\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "25163e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Comparison of Final Epoch Metrics:\n",
      "\n",
      "Box Loss (Train)    : 0.46372 → 0.75401 | ✅ Good Increase\n",
      "Cls Loss (Train)    : 0.25350 → 0.66127 | ✅ Good Increase\n",
      "DFL Loss (Train)    : 0.81862 → 0.87487 | ✅ Good Increase\n",
      "Precision           : 0.53596 → 0.63296 | ✅ Good Increase\n",
      "Recall              : 0.35983 → 0.54817 | ✅ Good Increase\n",
      "mAP@0.5             : 0.41108 → 0.62100 | ✅ Good Increase\n",
      "mAP@0.5:0.95        : 0.28971 → 0.52645 | ✅ Good Increase\n",
      "Box Loss (Val)      : 1.76688 → 0.76374 | ❌ No Increase\n",
      "Cls Loss (Val)      : 2.36712 → 0.94291 | ❌ No Increase\n",
      "DFL Loss (Val)      : 1.03949 → 0.87222 | ❌ No Increase\n"
     ]
    }
   ],
   "source": [
    "new_path = './runs/train/fine-tune-yolov8'\n",
    "old_path = './runs/train/yolov8'\n",
    "results_csv_path = find_results_csv(new_path)\n",
    "results_csv_path_1 = find_results_csv(old_path)\n",
    "\n",
    "\n",
    "\n",
    "compare_final_metrics(results_csv_path_1, results_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "33362264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found best.pt at: runs\\train\\fine-tune-yolov8\\weights\\best.pt\n"
     ]
    }
   ],
   "source": [
    "best_pt_path = find_best_model(new_path)\n",
    "# evaluate_and_save_metrics(best_pt_path, output_json_path=\"per_class_metrics_retrain.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "edceacbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Per-Class mAP@0.5:0.95 Metrics:\n",
      "\n",
      "Class Name      Class ID   mAP@0.5:0.95   \n",
      "----------------------------------------\n",
      "road            0          0.5266         \n",
      "pool            1          0.5266         \n",
      "vegetation      2          0.5266         \n",
      "roof            3          0.8812         \n",
      "wall            4          0.6481         \n",
      "window          5          0.7973         \n",
      "person          6          0.5386         \n",
      "dog             7          0.0983         \n",
      "car             8          0.5266         \n",
      "bicycle         9          0.3785         \n",
      "tree            10         0.7665         \n",
      "truck           11         0.0            \n",
      "bus             12         0.517          \n",
      "vehicle         13         0.6401         \n"
     ]
    }
   ],
   "source": [
    "print_per_class_metrics(\"per_class_metrics_retrain.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "97c1a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Comparison of mAP@0.5:0.95 per class:\n",
      "\n",
      "Class           Before     After      Change\n",
      "--------------------------------------------------\n",
      "unlabeled       0.3008     0.0000     ❌ No increase\n",
      "pool            0.8458     0.5266     ❌ No increase\n",
      "vegetation      0.1329     0.5266     ✅ Good increase\n",
      "roof            0.5681     0.8812     ✅ Good increase\n",
      "wall            0.1462     0.6481     ✅ Good increase\n",
      "window          0.2316     0.7973     ✅ Good increase\n",
      "person          0.2769     0.5386     ✅ Good increase\n",
      "dog             0.4108     0.0983     ❌ No increase\n",
      "car             0.3159     0.5266     ✅ Good increase\n",
      "bicycle         0.3551     0.3785     ✅ Good increase\n",
      "tree            0.4516     0.7665     ✅ Good increase\n",
      "truck           0.0336     0.0000     ❌ No increase\n",
      "bus             0.0000     0.5170     ✅ Good increase\n",
      "vehicle         0.1412     0.6401     ✅ Good increase\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def compare_maps(json_path1, json_path2):\n",
    "    with open(json_path1, 'r') as f1, open(json_path2, 'r') as f2:\n",
    "        metrics1 = json.load(f1)\n",
    "        metrics2 = json.load(f2)\n",
    "\n",
    "    print(\"\\n📊 Comparison of mAP@0.5:0.95 per class:\\n\")\n",
    "    print(f\"{'Class':<15} {'Before':<10} {'After':<10} {'Change'}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for class_name in metrics1:\n",
    "        map1 = metrics1[class_name].get(\"mAP@0.5:0.95\", 0)\n",
    "        map2 = metrics2.get(class_name, {}).get(\"mAP@0.5:0.95\", 0)\n",
    "\n",
    "        if map2 > map1:\n",
    "            status = \"✅ Good increase\"\n",
    "        else:\n",
    "            status = \"❌ No increase\"\n",
    "\n",
    "        print(f\"{class_name:<15} {map1:<10.4f} {map2:<10.4f} {status}\")\n",
    "\n",
    "# 🔧 Example usage:\n",
    "compare_maps(\"per_class_metrics.json\", \"per_class_metrics_retrain.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872e10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d7352ef453497080e5e40a9ab6862f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== STARTED: v1 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b590b179fd894059a077c56ecd98b747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v1:   0%|          | 0/642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v1 — Processed 642 frames\n",
      "\n",
      "========== STARTED: v10 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdee8d0a6d34b30808d3da960a60321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v10:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v10 — Processed 259 frames\n",
      "\n",
      "========== STARTED: v2 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a5157ec3594a979bef2e0f1259fa2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v2:   0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v2 — Processed 175 frames\n",
      "\n",
      "========== STARTED: v3 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02413f3adff14abc8308c6716bc7f3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v3:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v3 — Processed 176 frames\n",
      "\n",
      "========== STARTED: v4 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51ee056a2c746f38694c2867369f60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v4:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v4 — Processed 253 frames\n",
      "\n",
      "========== STARTED: v5 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddd9e0723c345aa948409b7116d35bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v5:   0%|          | 0/1114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v5 — Processed 1114 frames\n",
      "\n",
      "========== STARTED: v6 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10a3c6cbb034334aa763cb5075fef54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v6:   0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v6 — Processed 483 frames\n",
      "\n",
      "========== STARTED: v8 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b459d6996614a15b5ff00a129642024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v8:   0%|          | 0/949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v8 — Processed 949 frames\n",
      "\n",
      "========== STARTED: v9 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be0ca169b5b42d5aa0ad6b415103361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v9:   0%|          | 0/2752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[186], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvideos_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_pt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_id_to_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./datasets/final_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[158], line 55\u001b[0m, in \u001b[0;36mvideos_predictions\u001b[1;34m(yolo_weights_path, class_id_to_name, video_dir, output_base, max_frames)\u001b[0m\n\u001b[0;32m     52\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39mmax_frames \u001b[38;5;28;01mif\u001b[39;00m max_frames \u001b[38;5;28;01melse\u001b[39;00m total_frames, desc\u001b[38;5;241m=\u001b[39mvideo_id)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret \u001b[38;5;129;01mor\u001b[39;00m (max_frames \u001b[38;5;129;01mand\u001b[39;00m frame_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_frames):\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_path = './runs/train/fine-tune-yolov8'\n",
    "best_pt_path = find_best_model(new_path)\n",
    "videos_predictions(best_pt_path, class_id_to_name, video_dir='videos', output_base='./datasets/final_output', max_frames=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "216376a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found best.pt at: runs\\train\\yolov8\\weights\\best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782cad9aa02844db9464c5124fb3608c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== STARTED: v1 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77471be792fe47d2a948c20b8a9e0edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v1:   0%|          | 0/642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v1 — Processed 642 frames\n",
      "\n",
      "========== STARTED: v10 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3fa740844546f2a320d2886b982ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v10:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v10 — Processed 259 frames\n",
      "\n",
      "========== STARTED: v2 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a523e28c1241ddafd3084a29e2a4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v2:   0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v2 — Processed 175 frames\n",
      "\n",
      "========== STARTED: v3 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004ef10821ca4d9daeb1d3968b4e0fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v3:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v3 — Processed 176 frames\n",
      "\n",
      "========== STARTED: v4 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0428fa84d720481697892b97695f3587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v4:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v4 — Processed 253 frames\n",
      "\n",
      "========== STARTED: v5 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6667e1b97b1477a93a1f6dfef43b03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v5:   0%|          | 0/1114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v5 — Processed 1114 frames\n",
      "\n",
      "========== STARTED: v6 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1a2d603e804351b062762b56cf1d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v6:   0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v6 — Processed 483 frames\n",
      "\n",
      "========== STARTED: v8 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3065299ad1804d3a9cb66ecf9aa71be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v8:   0%|          | 0/949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v8 — Processed 949 frames\n",
      "\n",
      "========== STARTED: v9 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fff94389bb3402ca66057bd717b7fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v9:   0%|          | 0/2752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v9 — Processed 2752 frames\n"
     ]
    }
   ],
   "source": [
    "best_pt_path = find_best_model(old_path)\n",
    "\n",
    "videos_predictions(best_pt_path, class_id_to_name, video_dir='videos', output_base='./datasets/final_output_1', max_frames=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33828a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
