{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074d4a4f",
   "metadata": {},
   "source": [
    "## Builting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150d5af",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ec5504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built CUDA Version: 12.4\n",
      "CUDA Runtime Version: 12040\n",
      "GPU Name: NVIDIA GeForce RTX 3050 OEM\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print the CUDA version PyTorch is built with\n",
    "print(\"Built CUDA Version:\", torch.version.cuda)\n",
    "\n",
    "# Print the CUDA version runtime (if CUDA is available)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Runtime Version:\", torch._C._cuda_getCompiledVersion())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id_to_name = {\n",
    "    0:  ('road', [28, 42, 168]),\n",
    "    1:  ('pool', [0, 50, 89]),\n",
    "    2:  ('vegetation', [107, 142, 35]),\n",
    "    3:  ('roof', [70, 70, 70]),\n",
    "    4:  ('wall', [102, 102, 156]),\n",
    "    5:  ('window', [254, 228, 12]),\n",
    "    6:  ('person', [255, 22, 96]),\n",
    "    7:  ('dog', [102, 51, 0]),\n",
    "    8:  ('car', [9, 143, 150]),\n",
    "    9:  ('bicycle', [119, 11, 32]),\n",
    "    10: ('tree', [51, 51, 0]),\n",
    "    11: ('truck', [160, 160, 60]),   # added truck\n",
    "    12: ('bus', [200, 80, 80]),      # added bus\n",
    "    13: ('vehicle', [20, 80, 80]),      # added bus\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52746b61",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b06f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install opencv-python\n",
    "# !pip install pillow\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch torchvision\n",
    "# !pip install ultralytics\n",
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "892fe670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch torchvision torchaudio\n",
    "# !pip cache purge  # clean out pip's install cache\n",
    "# !pip install torch torchvision torchaudio --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdfb2866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssl49/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core packages\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "# Math and array handling\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Image and visualization\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep Learning Frameworks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.models.segmentation as segmentation\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Object Detection and Segmentation\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Automatically use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import gdown\n",
    "\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# save this as split_uavdt_train_val.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd42d4",
   "metadata": {},
   "source": [
    "### Download Datsets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f810e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_drone_dataset_download(gdrive_url, extract_to=\"extracted\"):\n",
    "    # Convert shared drive URL to direct download URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create output folder\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[INFO] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[INFO] Extracting ZIP...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Optionally, remove the ZIP file after extraction\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[DONE] Extracted files to: {extract_to}\")\n",
    "\n",
    "def uavdt_dataset_download(gdrive_url, extract_to=\"extracted\"):\n",
    "    # Convert shared drive URL to direct download URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create output folder\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[INFO] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[INFO] Extracting ZIP...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Optionally, remove the ZIP file after extraction\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[DONE] Extracted files to: {extract_to}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09011805",
   "metadata": {},
   "source": [
    "### Convert Two Datsets into yolo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ca98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Parse polygon and convert to YOLO bbox\n",
    "# ----------------------------\n",
    "# Semantic drone datasets \n",
    "def parse_yolo_style_bbox_from_xml(xml_path, class_id_to_name):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "        if class_name in [value[0] for value in class_id_to_name.values()]:\n",
    "            polygon = obj.find('polygon')\n",
    "            if polygon is not None:\n",
    "                points = polygon.findall('pt')\n",
    "                coords = [(float(pt.find('x').text), float(pt.find('y').text)) for pt in points]\n",
    "                x_min = min(coord[0] for coord in coords)\n",
    "                y_min = min(coord[1] for coord in coords)\n",
    "                x_max = max(coord[0] for coord in coords)\n",
    "                y_max = max(coord[1] for coord in coords)\n",
    "                bboxes.append(((x_min, y_min), (x_max, y_max), class_name))\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Save YOLO-format txt\n",
    "# ----------------------------\n",
    "def save_yolo_format(image_id, bboxes, image_width, image_height, output_path, class_id_to_name):\n",
    "    with open(output_path, 'w') as f:\n",
    "        for (x_min, y_min), (x_max, y_max), class_name in bboxes:\n",
    "            class_id = next(cid for cid, (name, _) in class_id_to_name.items() if name == class_name)\n",
    "            x_center = (x_min + x_max) / 2 / image_width\n",
    "            y_center = (y_min + y_max) / 2 / image_height\n",
    "            width = (x_max - x_min) / image_width\n",
    "            height = (y_max - y_min) / image_height\n",
    "            f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Convert dataset (YOLO only)\n",
    "# ----------------------------\n",
    "def convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name):\n",
    "    image_ids = [img.split('.')[0] for img in os.listdir(f\"{dataset_path}/images\") if img.endswith(\".jpg\")]\n",
    "\n",
    "    os.makedirs(f\"{output_dir}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/labels\", exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(image_ids, desc=\"Converting to YOLO\"):\n",
    "        img_path = f\"{dataset_path}/images/{image_id}.jpg\"\n",
    "        bbox_xml_path = f\"{dataset_path}/gt/bounding_box/label_me_xml/{image_id}.xml\"\n",
    "        semantic_xml_path = f\"{dataset_path}/gt/semantic/label_me_xml/{image_id}.xml\"\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"[WARNING] Image not found: {img_path}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            bboxes1 = parse_yolo_style_bbox_from_xml(bbox_xml_path, class_id_to_name)\n",
    "            bboxes2 = parse_yolo_style_bbox_from_xml(semantic_xml_path, class_id_to_name)\n",
    "            all_bboxes = bboxes1 + bboxes2\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Skipping image {image_id} due to parse error: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            image_np = np.array(image)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not load image {image_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save image\n",
    "        image.save(f\"{output_dir}/images/{image_id}.jpg\")\n",
    "\n",
    "        # Save YOLO labels\n",
    "        yolo_annotation_path = f\"{output_dir}/labels/{image_id}.txt\"\n",
    "        save_yolo_format(image_id, all_bboxes, image_np.shape[1], image_np.shape[0], yolo_annotation_path, class_id_to_name)\n",
    "\n",
    "    print(\"✅ YOLO-format annotation conversion complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a94cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 🧠 Map UAVDT class to extended class_id_to_name\n",
    "uavdt_to_extended = {\n",
    "    0: 8,   # car\n",
    "    1: 11,  # truck\n",
    "    2: 12,  # bus\n",
    "    3: 13\n",
    "}\n",
    "\n",
    "# === Function to Convert Single Annotation to YOLO Format ===\n",
    "def convert_annotation(anno_path, label_path, image_path, stats):\n",
    "    if not os.path.exists(image_path):\n",
    "        stats[\"missing_image\"] += 1\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        height, width = img.shape[:2]\n",
    "    except:\n",
    "        stats[\"missing_image\"] += 1\n",
    "        return\n",
    "\n",
    "    with open(anno_path, 'r') as fin, open(label_path, 'w') as fout:\n",
    "        for line in fin:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) < 8:\n",
    "                stats[\"malformed\"] += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                x, y, w, h = map(float, parts[0:4])\n",
    "                original_cls = int(parts[5])\n",
    "\n",
    "                # 🔁 Convert original class to extended class\n",
    "                if original_cls not in uavdt_to_extended:\n",
    "                    stats[\"skipped\"][original_cls] += 1\n",
    "                    continue\n",
    "\n",
    "                cls = uavdt_to_extended[original_cls]\n",
    "\n",
    "                x_center = (x + w / 2) / width\n",
    "                y_center = (y + h / 2) / height\n",
    "                w /= width\n",
    "                h /= height\n",
    "\n",
    "                if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and w > 0 and h > 0):\n",
    "                    stats[\"skipped\"][cls] += 1\n",
    "                    continue\n",
    "\n",
    "                fout.write(f\"{cls} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "                stats[\"converted\"] += 1\n",
    "            except Exception:\n",
    "                stats[\"malformed\"] += 1\n",
    "                continue\n",
    "\n",
    "            stats[\"total\"] += 1\n",
    "\n",
    "# === Step 1: Convert UAVDT annotations to YOLO format ===\n",
    "def convert_dataset(root_dir):\n",
    "    annotation_paths = glob(os.path.join(root_dir, \"M*/annotations/*.txt\"))\n",
    "    total_files = len(annotation_paths)\n",
    "\n",
    "    stats = {\n",
    "        \"total\": 0,\n",
    "        \"converted\": 0,\n",
    "        \"malformed\": 0,\n",
    "        \"missing_image\": 0,\n",
    "        \"skipped\": defaultdict(int)\n",
    "    }\n",
    "\n",
    "    print(f\"🔄 Converting {total_files} annotation files to YOLO format...\")\n",
    "\n",
    "    for anno_path in tqdm(annotation_paths, desc=\"Converting\", unit=\"file\"):\n",
    "        sequence_dir = os.path.dirname(os.path.dirname(anno_path))  # Mxxxx\n",
    "        file_name = os.path.basename(anno_path)\n",
    "\n",
    "        label_dir = os.path.join(sequence_dir, \"labels\")\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "        label_path = os.path.join(label_dir, file_name)\n",
    "\n",
    "        # Construct image path\n",
    "        image_name = file_name.replace(\".txt\", \".jpg\")\n",
    "        image_path = os.path.join(sequence_dir, \"images\", image_name)\n",
    "\n",
    "        convert_annotation(anno_path, label_path, image_path, stats)\n",
    "\n",
    "    print(\"\\nConversion complete.\")\n",
    "    print(f\"Total boxes:     {stats['total']}\")\n",
    "    print(f\"Converted boxes: {stats['converted']}\")\n",
    "    print(f\"Skipped boxes:   {sum(stats['skipped'].values())}\")\n",
    "    for cls, count in sorted(stats[\"skipped\"].items()):\n",
    "        print(f\"   - Skipped class {cls}: {count}\")\n",
    "    print(f\"Malformed lines: {stats['malformed']}\")\n",
    "    print(f\"Missing images: {stats['missing_image']}\")\n",
    "\n",
    "# === Step 2: Copy to train/val structure ===\n",
    "def copy_split_sequences(src_root, dst_root, train_ratio=0.8):\n",
    "    all_sequences = sorted(glob(os.path.join(src_root, \"M*\")))\n",
    "    train_seqs, val_seqs = train_test_split(all_sequences, train_size=train_ratio, random_state=42)\n",
    "\n",
    "    for split_name, split_list in zip(['train', 'val'], [train_seqs, val_seqs]):\n",
    "        for seq_path in tqdm(split_list, desc=f\"Copying {split_name}\"):\n",
    "            images_src = os.path.join(seq_path, \"images\")\n",
    "            labels_src = os.path.join(seq_path, \"labels\")\n",
    "\n",
    "            images_dst = os.path.join(dst_root, split_name, \"images\")\n",
    "            labels_dst = os.path.join(dst_root, split_name, \"labels\")\n",
    "\n",
    "            os.makedirs(images_dst, exist_ok=True)\n",
    "            os.makedirs(labels_dst, exist_ok=True)\n",
    "\n",
    "            for img_file in glob(os.path.join(images_src, \"*.jpg\")):\n",
    "                shutil.copy(img_file, os.path.join(images_dst, os.path.basename(img_file)))\n",
    "\n",
    "            for label_file in glob(os.path.join(labels_src, \"*.txt\")):\n",
    "                shutil.copy(label_file, os.path.join(labels_dst, os.path.basename(label_file)))\n",
    "\n",
    "    print(\"\\nDataset split into 'train/' and 'val/' with images and YOLO labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b9227",
   "metadata": {},
   "source": [
    "### Convert into train and Val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36ec4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Drone Datasets\n",
    "def move_files(file_list, \n",
    "               source_image_dir, \n",
    "               source_annotation_dir,\n",
    "               target_image_dir, \n",
    "               target_annotation_dir):\n",
    "    \n",
    "    os.makedirs(target_image_dir, exist_ok=True)\n",
    "    os.makedirs(target_annotation_dir, exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(file_list, desc=f\"Moving to {os.path.basename(os.path.dirname(target_image_dir))}\"):\n",
    "        image_path = os.path.join(source_image_dir, f\"{image_id}.jpg\")\n",
    "        annotation_path = os.path.join(source_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        target_image_path = os.path.join(target_image_dir, f\"{image_id}.jpg\")\n",
    "        target_annotation_path = os.path.join(target_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            shutil.copy(image_path, target_image_path)\n",
    "        else:\n",
    "            print(f\"[Warning] Missing image: {image_path}\")\n",
    "\n",
    "        if os.path.exists(annotation_path):\n",
    "            shutil.copy(annotation_path, target_annotation_path)\n",
    "        else:\n",
    "            print(f\"[Warning] Missing annotation: {annotation_path}\")\n",
    "\n",
    "def split_and_move_dataset(source_base_dir=\"./datasets/semantic_yolo\",\n",
    "                           target_base_dir=\"./datasets/new_dataset_yolo_split\",\n",
    "                           split_ratio=0.8,\n",
    "                           seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    image_dir = os.path.join(source_base_dir, \"images\")\n",
    "    label_dir = os.path.join(source_base_dir, \"labels\")\n",
    "\n",
    "    image_ids = [os.path.splitext(f)[0] for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "    random.shuffle(image_ids)\n",
    "\n",
    "    split_idx = int(len(image_ids) * split_ratio)\n",
    "    train_ids = image_ids[:split_idx]\n",
    "    val_ids = image_ids[split_idx:]\n",
    "\n",
    "    # Train\n",
    "    move_files(train_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"train/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"train/labels\"))\n",
    "\n",
    "    # Val\n",
    "    move_files(val_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"val/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"val/labels\"))\n",
    "\n",
    "    print(f\"\\n[✓] Dataset split completed: {len(train_ids)} train / {len(val_ids)} val samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110fe96d",
   "metadata": {},
   "source": [
    "### Normalize Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "541e4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label_file(label_file, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Normalize the label coordinates in a label file to ensure they are within [0, 1] range.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    with open(label_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, width, height = map(float, parts[1:])\n",
    "            \n",
    "            # Normalize coordinates to ensure they are within the range [0, 1]\n",
    "            x_center = min(1.0, max(0.0, x_center))\n",
    "            y_center = min(1.0, max(0.0, y_center))\n",
    "            width = min(1.0, max(0.0, width))\n",
    "            height = min(1.0, max(0.0, height))\n",
    "\n",
    "            # Write normalized values back to file\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "\n",
    "def get_image_size(img_path):\n",
    "    \"\"\"\n",
    "    Get the width and height of the image to normalize the coordinates properly.\n",
    "    \"\"\"\n",
    "    with Image.open(img_path) as img:\n",
    "        return img.size  # returns (width, height)\n",
    "\n",
    "\n",
    "def normalize_all_labels(labels_dir, img_dir):\n",
    "    \"\"\"\n",
    "    Normalize all label files in the specified directory.\n",
    "    \"\"\"\n",
    "    for label_file in tqdm(os.listdir(labels_dir)):\n",
    "       \n",
    "        if label_file.endswith('.txt'):  # Process only label files\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            img_path = os.path.join(img_dir, label_file.replace('.txt', '.jpg'))  # Assuming JPG images\n",
    "            if os.path.exists(img_path):\n",
    "                # Get image dimensions to normalize the labels\n",
    "                img_width, img_height = get_image_size(img_path)\n",
    "                # print(f\"Normalizing {label_file}...\")\n",
    "                normalize_label_file(label_path, img_width, img_height)\n",
    "            else:\n",
    "                print(f\"Warning: Image for label {label_file} not found!\")\n",
    "    print(\"Normalize Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce6fe5",
   "metadata": {},
   "source": [
    "### Training v8 model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2925090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo(data_yaml, epochs, imgsz, batch, name, model, dir):\n",
    "    if(model):\n",
    "        model = YOLO(dir)\n",
    "    else:\n",
    "        model = YOLO(\"yolov8m.pt\")\n",
    "\n",
    "    model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        name=name,\n",
    "        project=\"runs/train\",\n",
    "        patience=30,  # Early stopping\n",
    "        augment=True,  # Apply augmentations\n",
    "        degrees=10,  # Image rotation\n",
    "        scale=0.5,  # Scale range\n",
    "        flipud=0.2,  # Vertical flip\n",
    "        fliplr=0.5,  # Horizontal flip\n",
    "        hsv_h=0.015,  # Hue augmentation\n",
    "        hsv_s=0.7,  # Saturation augmentation\n",
    "        hsv_v=0.4,  # Value augmentation\n",
    "        mosaic=1.0,  # Mosaic augmentation\n",
    "        mixup=0.2,  # Mixup augmentation\n",
    "        lr0=0.01,  # Initial learning rate (you can tune this)\n",
    "        lrf=0.01,  # Learning rate final factor (for cosine annealing)\n",
    "        verbose=True  # Print progress\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ff318",
   "metadata": {},
   "source": [
    "### Print val metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf85d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def load_yolo_model(model_path):\n",
    "    return YOLO(model_path)\n",
    "\n",
    "def run_model_validation(model):\n",
    "    return model.val()\n",
    "\n",
    "def extract_per_class_metrics(results):\n",
    "    \"\"\"\n",
    "    Extracts mAP@0.5:0.95 per class from results.\n",
    "    NOTE: Only mAP@0.5:0.95 is available via `results.box.maps`\n",
    "    \"\"\"\n",
    "    per_class_metrics = {}\n",
    "    if hasattr(results.box, 'maps') and results.box.maps is not None:\n",
    "        maps = results.box.maps  # This is a NumPy array [num_classes]\n",
    "        for i, name in results.names.items():\n",
    "            per_class_metrics[name] = {\n",
    "                \"class_id\": i,\n",
    "                \"mAP@0.5:0.95\": round(float(maps[i]), 4)\n",
    "            }\n",
    "    else:\n",
    "        print(\"⚠️ No per-class mAP@0.5:0.95 data found.\")\n",
    "    return per_class_metrics\n",
    "\n",
    "def save_metrics_to_json(metrics, output_path):\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(f\"✅ Saved per-class metrics to {output_path}\")\n",
    "\n",
    "def evaluate_and_save_metrics(model_path, output_json_path=\"per_class_metrics.json\"):\n",
    "    model = load_yolo_model(model_path)\n",
    "    results = run_model_validation(model)\n",
    "    metrics = extract_per_class_metrics(results)\n",
    "    save_metrics_to_json(metrics, output_json_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65ca8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def print_per_class_metrics(json_path=\"per_class_metrics.json\"):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(\"📊 Per-Class mAP@0.5:0.95 Metrics:\\n\")\n",
    "    print(f\"{'Class Name':<15} {'Class ID':<10} {'mAP@0.5:0.95':<15}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for name, data in metrics.items():\n",
    "        print(f\"{name:<15} {data['class_id']:<10} {data['mAP@0.5:0.95']:<15}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22b1a8",
   "metadata": {},
   "source": [
    "### Find best model path after training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0b390e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(base_dir='runs_yolo/'):\n",
    "    best_paths = list(Path(base_dir).rglob('best.pt'))\n",
    "    if not best_paths:\n",
    "        raise FileNotFoundError(\"No 'best.pt' file found in the 'runs/' directory.\")\n",
    "    \n",
    "    # Optionally, sort by latest modified time\n",
    "    best_paths.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    print(f\"✅ Found best.pt at: {best_paths[0]}\")\n",
    "    return str(best_paths[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378bc85",
   "metadata": {},
   "source": [
    "### Prediction on vdieos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a013ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== FRAME PROCESSING ==========\n",
    "def process_frame_1(frame, yolo_model, w, h, class_id_to_name):\n",
    "    annotated = frame.copy()\n",
    "    results = yolo_model(annotated, verbose=False)[0]\n",
    "    boxes = results.boxes.xyxy.cpu().numpy()\n",
    "    class_ids = results.boxes.cls.cpu().numpy()\n",
    "\n",
    "    for box, cls_id in zip(boxes, class_ids):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        class_name, color = class_id_to_name[int(cls_id)]\n",
    "        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(annotated, class_name, (x1, max(y1 - 10, 10)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "    return annotated, boxes, class_ids\n",
    "\n",
    "# ========== VIDEO CAPTURE ==========\n",
    "def setup_video_capture_1(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    return cap, total_frames, fps, w, h\n",
    "\n",
    "# ========== MAIN FUNCTION ==========\n",
    "def videos_predictions(yolo_weights_path, class_id_to_name, video_dir='videos', output_base='./datatsets/opt', max_frames=None):\n",
    "    yolo_model = YOLO(yolo_weights_path)\n",
    "\n",
    "    image_out_dir = os.path.join(output_base, 'images')\n",
    "    label_out_dir = os.path.join(output_base, 'labels')\n",
    "    output_video_dir = os.path.join(output_base, 'output')\n",
    "\n",
    "    os.makedirs(image_out_dir, exist_ok=True)\n",
    "    os.makedirs(label_out_dir, exist_ok=True)\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    for video_file in tqdm(sorted(os.listdir(video_dir))):\n",
    "        if not video_file.lower().endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        video_id = os.path.splitext(video_file)[0]\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        output_video_path = os.path.join(output_video_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        print(f\"\\n========== STARTED: {video_id} ==========\")\n",
    "        cap, total_frames, fps, w, h = setup_video_capture_1(video_path)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(output_video_path, fourcc, fps, (w, h))\n",
    "\n",
    "        frame_count = 0\n",
    "        pbar = tqdm(total=max_frames if max_frames else total_frames, desc=video_id)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or (max_frames and frame_count >= max_frames):\n",
    "                break\n",
    "\n",
    "            annotated_bgr, boxes, class_ids = process_frame_1(frame, yolo_model, w, h, class_id_to_name)\n",
    "\n",
    "            # ✅ Save original image\n",
    "            img_filename = f'{video_id}_{frame_count:04d}.jpg'\n",
    "            img_path = os.path.join(image_out_dir, img_filename)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "\n",
    "            # ✅ Save YOLO-format label\n",
    "            label_filename = f'{video_id}_{frame_count:04d}.txt'\n",
    "            label_path = os.path.join(label_out_dir, label_filename)\n",
    "            with open(label_path, 'w') as f:\n",
    "                for box, cls_id in zip(boxes, class_ids):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    w_box = x2 - x1\n",
    "                    h_box = y2 - y1\n",
    "                    cx = x1 + w_box / 2\n",
    "                    cy = y1 + h_box / 2\n",
    "                    f.write(f\"{int(cls_id)} {cx/w:.6f} {cy/h:.6f} {w_box/w:.6f} {h_box/h:.6f}\\n\")\n",
    "\n",
    "            writer.write(annotated_bgr)\n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "        pbar.close()\n",
    "        print(f\"DONE: {video_id} — Processed {frame_count} frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce3db596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== FRAME PROCESSING ==========\n",
    "def process_frame(frame, yolo_model, w, h, class_id_to_name, valid_class_ids, conf_threshold=0.5):\n",
    "    annotated = frame.copy()\n",
    "    results = yolo_model(annotated, verbose=False)[0]\n",
    "\n",
    "    # Filter by confidence\n",
    "    mask = results.boxes.conf > conf_threshold\n",
    "    boxes = results.boxes.xyxy[mask].cpu().numpy()\n",
    "    class_ids = results.boxes.cls[mask].cpu().numpy()\n",
    "    confs = results.boxes.conf[mask].cpu().numpy()\n",
    "\n",
    "    filtered_boxes, filtered_ids, filtered_confs = [], [], []\n",
    "\n",
    "    for box, cls_id, conf in zip(boxes, class_ids, confs):\n",
    "        if int(cls_id) in valid_class_ids:\n",
    "            filtered_boxes.append(box)\n",
    "            filtered_ids.append(cls_id)\n",
    "            filtered_confs.append(conf)\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            class_name, color = class_id_to_name[int(cls_id)]\n",
    "            label = f\"{class_name} {conf:.2f}\"\n",
    "            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(annotated, label, (x1, max(y1 - 10, 10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "    return annotated, filtered_boxes, filtered_ids\n",
    "\n",
    "# ========== VIDEO CAPTURE ==========\n",
    "def setup_video_capture(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    return cap, total_frames, fps, w, h\n",
    "\n",
    "# ========== MAIN FUNCTION ==========\n",
    "def process_all_videos(yolo_weights_path, class_id_to_name, underrepresented_class_ids,\n",
    "                       video_dir='videos', output_base='./datasets/opt', max_frames=None):\n",
    "    yolo_model = YOLO(yolo_weights_path)\n",
    "\n",
    "    image_out_dir = os.path.join(output_base, 'images')\n",
    "    label_out_dir = os.path.join(output_base, 'labels')\n",
    "    output_video_dir = os.path.join(output_base, 'output')\n",
    "\n",
    "    os.makedirs(image_out_dir, exist_ok=True)\n",
    "    os.makedirs(label_out_dir, exist_ok=True)\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    for video_file in tqdm(sorted(os.listdir(video_dir))):\n",
    "        if not video_file.lower().endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        video_id = os.path.splitext(video_file)[0]\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        output_video_path = os.path.join(output_video_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        print(f\"\\n========== STARTED: {video_id} ==========\")\n",
    "        cap, total_frames, fps, w, h = setup_video_capture(video_path)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(output_video_path, fourcc, fps, (w, h))\n",
    "\n",
    "        frame_count = 0\n",
    "        pbar = tqdm(total=max_frames if max_frames else total_frames, desc=video_id)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or (max_frames and frame_count >= max_frames):\n",
    "                break\n",
    "\n",
    "            annotated_bgr, boxes, class_ids = process_frame(\n",
    "                frame, yolo_model, w, h, class_id_to_name, underrepresented_class_ids)\n",
    "\n",
    "            # ✅ Save original image\n",
    "            img_filename = f'{video_id}_{frame_count:04d}.jpg'\n",
    "            img_path = os.path.join(image_out_dir, img_filename)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "\n",
    "            # ✅ Save YOLO-format label (only if boxes found)\n",
    "            if boxes:\n",
    "                label_filename = f'{video_id}_{frame_count:04d}.txt'\n",
    "                label_path = os.path.join(label_out_dir, label_filename)\n",
    "                with open(label_path, 'w') as f:\n",
    "                    for box, cls_id in zip(boxes, class_ids):\n",
    "                        x1, y1, x2, y2 = box\n",
    "                        w_box = x2 - x1\n",
    "                        h_box = y2 - y1\n",
    "                        cx = x1 + w_box / 2\n",
    "                        cy = y1 + h_box / 2\n",
    "                        f.write(f\"{int(cls_id)} {cx/w:.6f} {cy/h:.6f} {w_box/w:.6f} {h_box/h:.6f}\\n\")\n",
    "\n",
    "            writer.write(annotated_bgr)\n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "        pbar.close()\n",
    "        print(f\"DONE: {video_id} — Processed {frame_count} frames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70f244",
   "metadata": {},
   "source": [
    "### Get Class Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcf3c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_rare_class_ids(label_dir, class_id_to_name, rare_threshold=1000):\n",
    "    class_counts = defaultdict(int)\n",
    "\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        if not label_file.endswith('.txt'):\n",
    "            continue\n",
    "        with open(os.path.join(label_dir, label_file), 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 1:\n",
    "                    cls_id = int(parts[0])\n",
    "                    class_counts[cls_id] += 1\n",
    "\n",
    "    print(\"🔍 Class-wise instance counts:\")\n",
    "    for cls_id in sorted(class_counts.keys()):\n",
    "        name = class_id_to_name.get(cls_id, (\"Unknown\", []))[0]\n",
    "        count = class_counts[cls_id]\n",
    "        print(f\"Class {cls_id:2d} ({name:10s}): {count} instances\")\n",
    "\n",
    "    rare_class_ids = {cls_id for cls_id, count in class_counts.items() if count < rare_threshold}\n",
    "    print(f\"\\n✅ Rare class IDs (threshold < {rare_threshold}): {rare_class_ids}\")\n",
    "\n",
    "    return rare_class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c030e",
   "metadata": {},
   "source": [
    "### Merge predciton and previous datatsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bdc3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def merge_yolo_datasets(source1, source2, destination):\n",
    "    # Define subfolders\n",
    "    img1_dir = os.path.join(source1, 'images')\n",
    "    lbl1_dir = os.path.join(source1, 'labels')\n",
    "    img2_dir = os.path.join(source2, 'images')\n",
    "    lbl2_dir = os.path.join(source2, 'labels')\n",
    "    dst_img_dir = os.path.join(destination, 'images')\n",
    "    dst_lbl_dir = os.path.join(destination, 'labels')\n",
    "\n",
    "    # Create destination folders\n",
    "    os.makedirs(dst_img_dir, exist_ok=True)\n",
    "    os.makedirs(dst_lbl_dir, exist_ok=True)\n",
    "\n",
    "    def copy_files(src_img_dir, src_lbl_dir, prefix):\n",
    "        for filename in sorted(os.listdir(src_img_dir)):\n",
    "            if not filename.lower().endswith('.jpg'):\n",
    "                continue\n",
    "            base = os.path.splitext(filename)[0]\n",
    "\n",
    "            # Image\n",
    "            new_img_name = f\"{prefix}_{base}.jpg\"\n",
    "            shutil.copy(os.path.join(src_img_dir, filename),\n",
    "                        os.path.join(dst_img_dir, new_img_name))\n",
    "\n",
    "            # Label\n",
    "            label_file = base + \".txt\"\n",
    "            if os.path.exists(os.path.join(src_lbl_dir, label_file)):\n",
    "                new_lbl_name = f\"{prefix}_{base}.txt\"\n",
    "                shutil.copy(os.path.join(src_lbl_dir, label_file),\n",
    "                            os.path.join(dst_lbl_dir, new_lbl_name))\n",
    "            else:\n",
    "                print(f\"⚠️ No label for {filename}\")\n",
    "\n",
    "    print(\"🔁 Merging original dataset...\")\n",
    "    copy_files(img1_dir, lbl1_dir, prefix=\"orig\")\n",
    "\n",
    "    print(\"➕ Merging predicted video dataset...\")\n",
    "    copy_files(img2_dir, lbl2_dir, prefix=\"pred\")\n",
    "\n",
    "    print(f\"\\n✅ Merge complete! Merged dataset at: {destination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adccd711",
   "metadata": {},
   "source": [
    "### Print Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbbb0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_results_csv(directory):\n",
    "    \"\"\"Find the results.csv file in the specified directory.\"\"\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if 'results.csv' in files:\n",
    "            return os.path.join(root, 'results.csv')\n",
    "    return None\n",
    "\n",
    "def load_results_csv(results_csv_path):\n",
    "    \"\"\"Load the results CSV into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(results_csv_path)\n",
    "\n",
    "def calculate_total_epochs(df):\n",
    "    \"\"\"Calculate the total number of epochs from the DataFrame.\"\"\"\n",
    "    return df['epoch'].max()\n",
    "\n",
    "def calculate_training_loss(epoch_data):\n",
    "    \"\"\"Calculate the total training loss from the given epoch data.\"\"\"\n",
    "    train_box_loss = epoch_data['train/box_loss']\n",
    "    train_cls_loss = epoch_data['train/cls_loss']\n",
    "    train_dfl_loss = epoch_data['train/dfl_loss']\n",
    "    return train_box_loss + train_cls_loss + train_dfl_loss\n",
    "\n",
    "def calculate_validation_loss(epoch_data):\n",
    "    \"\"\"Calculate the total validation loss from the given epoch data.\"\"\"\n",
    "    val_box_loss = epoch_data['val/box_loss']\n",
    "    val_cls_loss = epoch_data['val/cls_loss']\n",
    "    val_dfl_loss = epoch_data['val/dfl_loss']\n",
    "    return val_box_loss + val_cls_loss + val_dfl_loss\n",
    "\n",
    "def print_final_metrics(df):\n",
    "    \"\"\"Print the final metrics for the last epoch.\"\"\"\n",
    "    final_epoch_data = df.iloc[-1]\n",
    "\n",
    "    # Calculate total training and validation loss\n",
    "    train_loss = calculate_training_loss(final_epoch_data)\n",
    "    val_loss = calculate_validation_loss(final_epoch_data)\n",
    "\n",
    "    # Print overall metrics\n",
    "    print(\"\\n========== Final Training Metrics ==========\")\n",
    "    print(f\"Training Loss: {train_loss:.6f}\")\n",
    "    print(f\"Precision: {final_epoch_data['metrics/precision(B)']:.6f}\")\n",
    "    print(f\"Recall: {final_epoch_data['metrics/recall(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5: {final_epoch_data['metrics/mAP50(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5:0.95: {final_epoch_data['metrics/mAP50-95(B)']:.6f}\")\n",
    "\n",
    "    print(\"\\n========== Final Validation Metrics ==========\")\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "\n",
    "def print_csv_metrics(directory):\n",
    "    \"\"\"Main function to process and print final metrics.\"\"\"\n",
    "    # Find the results.csv file\n",
    "    results_csv_path = find_results_csv(directory)\n",
    "    \n",
    "    if not results_csv_path:\n",
    "        print(\"Error: 'results.csv' file not found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found results.csv at: {results_csv_path}\")\n",
    "\n",
    "    # Load results CSV\n",
    "    df = load_results_csv(results_csv_path)\n",
    "\n",
    "    # Get the total number of epochs\n",
    "    total_epochs = calculate_total_epochs(df)\n",
    "    print(f\"Total number of epochs: {total_epochs}\")\n",
    "\n",
    "    # Print columns in the CSV\n",
    "    # print(\"\\n========== Columns in CSV ==========\")\n",
    "    # print(df.columns)\n",
    "\n",
    "    # Print final metrics\n",
    "    print_final_metrics(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d7bab",
   "metadata": {},
   "source": [
    "## Calling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f592f31",
   "metadata": {},
   "source": [
    "### Download, convert, split and normalize datsets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54ca66d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading ZIP from Google Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1UppumYqYOi-kto6BWPfFxwJK2Eph46oY\n",
      "From (redirected): https://drive.google.com/uc?id=1UppumYqYOi-kto6BWPfFxwJK2Eph46oY&confirm=t&uuid=151eaff2-64e8-469c-a4e9-6b66550bf701\n",
      "To: /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/downloaded.zip\n",
      "100%|██████████| 4.14G/4.14G [01:37<00:00, 42.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting ZIP...\n",
      "[DONE] Extracted files to: datasets\n",
      "[INFO] Downloading ZIP from Google Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=12cbrTaBAMIsuU-mwAA7IgDk9wSLC9cC-\n",
      "From (redirected): https://drive.google.com/uc?id=12cbrTaBAMIsuU-mwAA7IgDk9wSLC9cC-&confirm=t&uuid=c4a86683-6152-470e-8ce5-c668311f3248\n",
      "To: /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/downloaded.zip\n",
      "100%|██████████| 5.21G/5.21G [02:09<00:00, 40.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting ZIP...\n",
      "[DONE] Extracted files to: datasets\n"
     ]
    }
   ],
   "source": [
    "gdrive_url = \"https://drive.google.com/file/d/1UppumYqYOi-kto6BWPfFxwJK2Eph46oY/view?usp=sharing\"\n",
    "semantic_drone_dataset_download(gdrive_url, extract_to=\"datasets\")\n",
    "\n",
    "gdrive_url = \"https://drive.google.com/file/d/12cbrTaBAMIsuU-mwAA7IgDk9wSLC9cC-/view?usp=sharing\"\n",
    "uavdt_dataset_download(gdrive_url, extract_to=\"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "558387bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to YOLO:  75%|███████▌  | 301/400 [01:01<00:20,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Skipping image 389 due to parse error: not well-formed (invalid token): line 1, column 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to YOLO: 100%|██████████| 400/400 [01:22<00:00,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ YOLO-format annotation conversion complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./datasets/semantic_drone_dataset/training_set\"\n",
    "output_dir = \"./datasets/semantic_yolo\"\n",
    "\n",
    "convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94505830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Converting 30946 annotation files to YOLO format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting: 100%|██████████| 30946/30946 [01:08<00:00, 453.50file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversion complete.\n",
      "Total boxes:     868139\n",
      "Converted boxes: 868139\n",
      "Skipped boxes:   0\n",
      "Malformed lines: 0\n",
      "Missing images: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train: 100%|██████████| 37/37 [00:03<00:00, 11.07it/s]\n",
      "Copying val: 100%|██████████| 10/10 [00:00<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset split into 'train/' and 'val/' with images and YOLO labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving to train: 100%|██████████| 319/319 [00:00<00:00, 797.03it/s]\n",
      "Moving to val: 100%|██████████| 80/80 [00:00<00:00, 708.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[✓] Dataset split completed: 319 train / 80 val samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#UAVDT-2024\n",
    "\n",
    "source_root = \"./datasets/UAVDT-2024\"\n",
    "output_root = \"./datasets/new_dataset_yolo_split\"\n",
    "\n",
    "convert_dataset(source_root)\n",
    "copy_split_sequences(source_root, output_root, train_ratio=0.8)\n",
    "\n",
    "\n",
    "# Semantic dorne datasets\n",
    "split_and_move_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec08031d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2354/2354 [00:00<00:00, 8143.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1181/1181 [00:00<00:00, 5739.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set your paths\n",
    "dataset_path = \"./datasets/new_dataset_yolo_split/train\"\n",
    "image_dir = os.path.join(dataset_path, \"images\")\n",
    "annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "normalize_all_labels(annotations_dir, image_dir)\n",
    "\n",
    "dataset_path = \"./datasets/new_dataset_yolo_split/val\"\n",
    "image_dir = os.path.join(dataset_path, \"images\")\n",
    "annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "normalize_all_labels(annotations_dir, image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88e922",
   "metadata": {},
   "source": [
    "### Checking how many classes have how much instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eeb923ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class-wise instance counts:\n",
      "Class  1 (pool      ): 30 instances\n",
      "Class  2 (vegetation): 5528 instances\n",
      "Class  3 (roof      ): 280 instances\n",
      "Class  4 (wall      ): 954 instances\n",
      "Class  5 (window    ): 376 instances\n",
      "Class  6 (person    ): 2475 instances\n",
      "Class  7 (dog       ): 25 instances\n",
      "Class  8 (car       ): 35045 instances\n",
      "Class  9 (bicycle   ): 222 instances\n",
      "Class 10 (tree      ): 417 instances\n",
      "Class 11 (truck     ): 129 instances\n",
      "Class 12 (bus       ): 86 instances\n",
      "Class 13 (vehicle   ): 577 instances\n",
      "\n",
      "✅ Rare class IDs (threshold < 3000): {1, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13}\n"
     ]
    }
   ],
   "source": [
    "labels_dir = './datasets/new_dataset_yolo_split/train/labels'\n",
    "\n",
    "rare_class_ids = get_rare_class_ids(label_dir=labels_dir, class_id_to_name=class_id_to_name ,rare_threshold=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63556279",
   "metadata": {},
   "source": [
    "### Training v8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56985371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deleted folder: ./datasets/semantic_yolo\n",
      "⚠️ Folder does not exist: ./datasets/new_dataset_yolo\n",
      "⚠️ Folder does not exist: ./datasets/uavdt-processed\n",
      "✅ Deleted folder: ./runs\n",
      "⚠️ Folder does not exist: ./metrics\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# List of folders to delete\n",
    "folders_to_delete = ['./datasets/semantic_yolo', './datasets/new_dataset_yolo', './datasets/uavdt-processed', './runs', \"./metrics\"]\n",
    "\n",
    "for folder_path in folders_to_delete:\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"✅ Deleted folder: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Folder does not exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "735f81a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     46/100      6.56G     0.6858     0.3894     0.8642         42        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.541      0.325      0.373      0.252\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     47/100      5.38G     0.6624     0.3682     0.8556         80        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:16<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.481      0.338      0.341      0.231\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     48/100      5.31G     0.6735      0.379     0.8593        101        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.562      0.341      0.419      0.294\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     49/100      5.66G     0.6691     0.3792     0.8593         15        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.583      0.308      0.348      0.218\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     50/100      6.52G     0.6626      0.377     0.8595         43        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.548      0.345      0.381      0.246\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     51/100      5.06G     0.6772     0.3823     0.8622         57        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393       0.51      0.336      0.393      0.262\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     52/100      5.51G     0.6505     0.3647     0.8544         42        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:16<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.496      0.352      0.379       0.25\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     53/100      6.52G     0.6528     0.3686     0.8587        174        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.542       0.38      0.383      0.234\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     54/100      5.22G     0.6555     0.3692     0.8564        113        736: 100%|██████████| 295/295 [02:06<00:00,  2.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.499      0.346      0.374      0.259\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     55/100      5.76G      0.651     0.3679     0.8556        134        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.497      0.321      0.377      0.266\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     56/100      5.49G       0.65     0.3654     0.8563         87        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:14<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.538      0.361      0.402      0.262\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     57/100      5.35G     0.6467     0.3617     0.8555         82        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.499      0.352      0.388      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     58/100      5.18G     0.6396     0.3563     0.8531         36        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:14<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.599      0.304        0.4      0.269\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     59/100      5.78G     0.6453     0.3663     0.8557         80        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.523      0.357       0.38      0.253\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     60/100      5.51G     0.6432     0.3613      0.855         53        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:16<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.544      0.322      0.378      0.254\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     61/100      5.95G     0.6325     0.3541     0.8511         23        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.552      0.353      0.394      0.267\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     62/100      5.71G     0.6203     0.3471     0.8497         80        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.515      0.331      0.378      0.263\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     63/100      5.82G     0.6241     0.3504     0.8496         19        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.521      0.327      0.384      0.263\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     64/100      6.09G     0.6268     0.3517     0.8511        125        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.498      0.353      0.381      0.266\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     65/100      5.31G     0.6085     0.3408     0.8462         61        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.607      0.359      0.412      0.279\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     66/100      5.31G     0.6085     0.3425     0.8463         34        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.506      0.359      0.362      0.245\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     67/100      4.79G     0.6146     0.3455     0.8493        122        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:16<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.493      0.355      0.399      0.272\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     68/100      5.41G     0.6189     0.3449     0.8491         29        736: 100%|██████████| 295/295 [02:06<00:00,  2.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.514       0.35      0.392      0.268\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     69/100      5.44G     0.6076     0.3386     0.8452        121        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:17<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.521      0.356      0.391      0.275\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     70/100      6.06G     0.6145     0.3466     0.8498        119        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:16<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.553      0.362      0.404      0.273\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     71/100      5.46G     0.6077     0.3388     0.8464         34        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.521      0.341      0.367      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     72/100      5.51G     0.6034     0.3396     0.8473         89        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:14<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.462      0.365      0.376      0.256\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     73/100      5.08G     0.5989      0.334      0.846         52        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.592      0.325        0.4      0.275\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     74/100      5.73G     0.5965     0.3318     0.8432         48        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.574      0.371      0.392      0.268\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     75/100      5.74G     0.5939     0.3289     0.8433         68        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.589      0.328      0.399      0.273\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     76/100       5.3G     0.5991     0.3348     0.8435        136        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.479      0.367      0.388      0.263\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     77/100      6.18G     0.5926     0.3276     0.8423         17        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:15<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.557      0.347      0.406      0.279\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     78/100      5.62G     0.5837     0.3268     0.8417        175        736: 100%|██████████| 295/295 [02:06<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:16<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.593      0.332      0.402       0.28\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 30 epochs. Best results observed at epoch 48, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=30) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "78 epochs completed in 3.098 hours.\n",
      "Optimizer stripped from runs/train/yolov8/weights/last.pt, 52.0MB\n",
      "Optimizer stripped from runs/train/yolov8/weights/best.pt, 52.0MB\n",
      "\n",
      "Validating runs/train/yolov8/weights/best.pt...\n",
      "Ultralytics 8.3.109 🚀 Python-3.10.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 OEM, 7957MiB)\n",
      "Model summary (fused): 92 layers, 25,847,866 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:35<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.561      0.359      0.427        0.3\n",
      "                  pool          9          9      0.771      0.889      0.852       0.78\n",
      "            vegetation         75       1780      0.215      0.269      0.229      0.134\n",
      "                  roof         42         79      0.124       0.81      0.678      0.572\n",
      "                  wall         62        255      0.613      0.286      0.288       0.18\n",
      "                window         38        139      0.547      0.345      0.323      0.202\n",
      "                person         75        637      0.877      0.328      0.446      0.277\n",
      "                   dog          6         12          1      0.397      0.735      0.491\n",
      "                   car       1115      53669      0.754      0.386       0.55      0.306\n",
      "               bicycle         32         53       0.69      0.358      0.466      0.235\n",
      "                  tree         38        108      0.743      0.562      0.603      0.457\n",
      "                 truck        907       3904          0          0          0          0\n",
      "                   bus        169        169          0          0          0          0\n",
      "               vehicle       1079       9579      0.965     0.0398      0.383      0.266\n",
      "Speed: 0.1ms preprocess, 21.2ms inference, 0.0ms loss, 4.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/train/yolov8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"[+] Training Start\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_yolo(\"yolov8.yaml\", 100, 720, 8, \"yolov8\", False, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c39cf0",
   "metadata": {},
   "source": [
    "### Print metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "830c11fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found best.pt at: runs/train/yolov8/weights/best.pt\n",
      "Ultralytics 8.3.109 🚀 Python-3.10.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 OEM, 7957MiB)\n",
      "Model summary (fused): 92 layers, 25,847,866 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/new_dataset_yolo_split/val/labels.cache... 1181 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1181/1181 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [00:25<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1181      70393      0.562      0.342      0.418      0.294\n",
      "                  pool          9          9      0.862      0.889      0.875      0.821\n",
      "            vegetation         75       1780       0.16       0.27      0.204      0.122\n",
      "                  roof         42         79      0.239       0.73      0.659      0.534\n",
      "                  wall         62        255       0.39      0.227      0.213      0.133\n",
      "                window         38        139      0.499      0.338      0.298      0.189\n",
      "                person         75        637      0.881      0.305      0.395      0.256\n",
      "                   dog          6         12          1      0.279      0.714      0.471\n",
      "                   car       1115      53669      0.781      0.399      0.575       0.32\n",
      "               bicycle         32         53        0.7      0.415      0.482      0.227\n",
      "                  tree         38        108      0.839      0.556      0.624      0.454\n",
      "                 truck        907       3904          0          0          0          0\n",
      "                   bus        169        169          0          0          0          0\n",
      "               vehicle       1079       9579      0.957     0.0411      0.393      0.299\n",
      "Speed: 0.2ms preprocess, 15.9ms inference, 0.0ms loss, 2.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
      "✅ Saved per-class metrics to per_class_metrics.json\n"
     ]
    }
   ],
   "source": [
    "yolov8 = './runs/train/yolov8'\n",
    "best_pt_path = find_best_model(yolov8)\n",
    "evaluate_and_save_metrics(best_pt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44ab66bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Per-Class mAP@0.5:0.95 Metrics:\n",
      "\n",
      "Class Name      Class ID   mAP@0.5:0.95   \n",
      "----------------------------------------\n",
      "unlabeled       0          0.2943         \n",
      "pool            1          0.8215         \n",
      "vegetation      2          0.1216         \n",
      "roof            3          0.5337         \n",
      "wall            4          0.1327         \n",
      "window          5          0.1891         \n",
      "person          6          0.2558         \n",
      "dog             7          0.4714         \n",
      "car             8          0.3195         \n",
      "bicycle         9          0.2275         \n",
      "tree            10         0.4541         \n",
      "truck           11         0.0            \n",
      "bus             12         0.0            \n",
      "vehicle         13         0.2993         \n"
     ]
    }
   ],
   "source": [
    "print_per_class_metrics(\"per_class_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d15b47f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found results.csv at: ./runs/train/yolov8/results.csv\n",
      "Total number of epochs: 78\n",
      "\n",
      "========== Final Training Metrics ==========\n",
      "Training Loss: 1.752180\n",
      "Precision: 0.592850\n",
      "Recall: 0.331840\n",
      "mAP@0.5: 0.401860\n",
      "mAP@0.5:0.95: 0.280340\n",
      "\n",
      "========== Final Validation Metrics ==========\n",
      "Validation Loss: 5.269870\n"
     ]
    }
   ],
   "source": [
    "print_csv_metrics(yolov8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230e616",
   "metadata": {},
   "source": [
    "### Prediciton videso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01fe5528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Folder does not exist: ./datasets/new-videos-predicted-yolo\n",
      "⚠️ Folder does not exist: ./datasets/merged_yolo_dataset\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# List of folders to delete\n",
    "folders_to_delete = ['./datasets/new-videos-predicted-yolo', \"./datasets/merged_yolo_dataset\"]\n",
    "\n",
    "for folder_path in folders_to_delete:\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"✅ Deleted folder: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Folder does not exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cc5a553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== STARTED: v1 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v1: 100%|██████████| 400/400 [00:28<00:00, 14.25it/s]\n",
      " 11%|█         | 1/9 [00:28<03:45, 28.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v1 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v10 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v10:  65%|██████▍   | 259/400 [00:18<00:10, 14.04it/s]\n",
      " 22%|██▏       | 2/9 [00:46<02:37, 22.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v10 — Processed 259 frames\n",
      "\n",
      "========== STARTED: v2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v2:  44%|████▍     | 175/400 [00:12<00:15, 14.21it/s]\n",
      " 33%|███▎      | 3/9 [00:58<01:47, 17.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v2 — Processed 175 frames\n",
      "\n",
      "========== STARTED: v3 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v3:  44%|████▍     | 176/400 [00:04<00:05, 43.65it/s]\n",
      " 44%|████▍     | 4/9 [01:03<01:01, 12.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v3 — Processed 176 frames\n",
      "\n",
      "========== STARTED: v4 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v4:  63%|██████▎   | 253/400 [00:08<00:04, 29.84it/s]\n",
      " 56%|█████▌    | 5/9 [01:11<00:43, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v4 — Processed 253 frames\n",
      "\n",
      "========== STARTED: v5 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v5: 100%|██████████| 400/400 [00:31<00:00, 12.66it/s]\n",
      " 67%|██████▋   | 6/9 [01:43<00:54, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v5 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v6 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v6: 100%|██████████| 400/400 [00:28<00:00, 14.00it/s]\n",
      " 78%|███████▊  | 7/9 [02:11<00:42, 21.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v6 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v8 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v8: 100%|██████████| 400/400 [00:16<00:00, 23.83it/s]\n",
      " 89%|████████▉ | 8/9 [02:28<00:20, 20.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v8 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v9 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v9: 100%|██████████| 400/400 [00:32<00:00, 12.28it/s]\n",
      "100%|██████████| 9/9 [03:01<00:00, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v9 — Processed 400 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_all_videos(best_pt_path, class_id_to_name, rare_class_ids, video_dir='videos', output_base='./datasets/new-videos-predicted-yolo', max_frames=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4fce1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class-wise instance counts:\n",
      "Class  3 (roof      ): 74 instances\n",
      "Class  4 (wall      ): 1020 instances\n",
      "Class  5 (window    ): 166 instances\n",
      "Class  6 (person    ): 11772 instances\n",
      "Class  9 (bicycle   ): 285 instances\n",
      "Class 10 (tree      ): 1463 instances\n",
      "Class 13 (vehicle   ): 2 instances\n",
      "\n",
      "✅ Rare class IDs (threshold < 0): set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Folder with YOLO label files\n",
    "label_dir = './datasets/new-videos-predicted-yolo/labels'\n",
    "\n",
    "get_rare_class_ids(label_dir=label_dir, class_id_to_name=class_id_to_name ,rare_threshold=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3013569",
   "metadata": {},
   "source": [
    "### Merge previous and new prediction Ddatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3807d000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Merging original dataset...\n",
      "➕ Merging predicted video dataset...\n",
      "⚠️ No label for v10_0000.jpg\n",
      "⚠️ No label for v10_0001.jpg\n",
      "⚠️ No label for v10_0002.jpg\n",
      "⚠️ No label for v10_0003.jpg\n",
      "⚠️ No label for v10_0004.jpg\n",
      "⚠️ No label for v10_0005.jpg\n",
      "⚠️ No label for v10_0006.jpg\n",
      "⚠️ No label for v10_0007.jpg\n",
      "⚠️ No label for v10_0008.jpg\n",
      "⚠️ No label for v10_0009.jpg\n",
      "⚠️ No label for v10_0010.jpg\n",
      "⚠️ No label for v10_0011.jpg\n",
      "⚠️ No label for v10_0047.jpg\n",
      "⚠️ No label for v10_0054.jpg\n",
      "⚠️ No label for v10_0064.jpg\n",
      "⚠️ No label for v10_0066.jpg\n",
      "⚠️ No label for v10_0067.jpg\n",
      "⚠️ No label for v10_0068.jpg\n",
      "⚠️ No label for v10_0070.jpg\n",
      "⚠️ No label for v10_0074.jpg\n",
      "⚠️ No label for v10_0075.jpg\n",
      "⚠️ No label for v10_0076.jpg\n",
      "⚠️ No label for v10_0077.jpg\n",
      "⚠️ No label for v10_0078.jpg\n",
      "⚠️ No label for v10_0079.jpg\n",
      "⚠️ No label for v10_0080.jpg\n",
      "⚠️ No label for v10_0081.jpg\n",
      "⚠️ No label for v10_0082.jpg\n",
      "⚠️ No label for v10_0083.jpg\n",
      "⚠️ No label for v10_0084.jpg\n",
      "⚠️ No label for v10_0085.jpg\n",
      "⚠️ No label for v10_0086.jpg\n",
      "⚠️ No label for v10_0087.jpg\n",
      "⚠️ No label for v10_0088.jpg\n",
      "⚠️ No label for v10_0089.jpg\n",
      "⚠️ No label for v10_0090.jpg\n",
      "⚠️ No label for v10_0091.jpg\n",
      "⚠️ No label for v10_0092.jpg\n",
      "⚠️ No label for v10_0093.jpg\n",
      "⚠️ No label for v10_0094.jpg\n",
      "⚠️ No label for v10_0095.jpg\n",
      "⚠️ No label for v10_0096.jpg\n",
      "⚠️ No label for v10_0097.jpg\n",
      "⚠️ No label for v10_0098.jpg\n",
      "⚠️ No label for v10_0099.jpg\n",
      "⚠️ No label for v10_0100.jpg\n",
      "⚠️ No label for v10_0101.jpg\n",
      "⚠️ No label for v10_0102.jpg\n",
      "⚠️ No label for v10_0103.jpg\n",
      "⚠️ No label for v10_0104.jpg\n",
      "⚠️ No label for v10_0105.jpg\n",
      "⚠️ No label for v10_0106.jpg\n",
      "⚠️ No label for v10_0107.jpg\n",
      "⚠️ No label for v10_0108.jpg\n",
      "⚠️ No label for v10_0109.jpg\n",
      "⚠️ No label for v10_0110.jpg\n",
      "⚠️ No label for v10_0111.jpg\n",
      "⚠️ No label for v10_0112.jpg\n",
      "⚠️ No label for v10_0113.jpg\n",
      "⚠️ No label for v10_0119.jpg\n",
      "⚠️ No label for v10_0121.jpg\n",
      "⚠️ No label for v10_0123.jpg\n",
      "⚠️ No label for v10_0124.jpg\n",
      "⚠️ No label for v10_0125.jpg\n",
      "⚠️ No label for v10_0128.jpg\n",
      "⚠️ No label for v10_0137.jpg\n",
      "⚠️ No label for v1_0033.jpg\n",
      "⚠️ No label for v1_0062.jpg\n",
      "⚠️ No label for v1_0083.jpg\n",
      "⚠️ No label for v1_0094.jpg\n",
      "⚠️ No label for v1_0096.jpg\n",
      "⚠️ No label for v1_0097.jpg\n",
      "⚠️ No label for v1_0115.jpg\n",
      "⚠️ No label for v1_0116.jpg\n",
      "⚠️ No label for v1_0117.jpg\n",
      "⚠️ No label for v1_0123.jpg\n",
      "⚠️ No label for v1_0126.jpg\n",
      "⚠️ No label for v1_0200.jpg\n",
      "⚠️ No label for v1_0201.jpg\n",
      "⚠️ No label for v1_0230.jpg\n",
      "⚠️ No label for v1_0253.jpg\n",
      "⚠️ No label for v1_0255.jpg\n",
      "⚠️ No label for v1_0256.jpg\n",
      "⚠️ No label for v1_0257.jpg\n",
      "⚠️ No label for v1_0259.jpg\n",
      "⚠️ No label for v1_0263.jpg\n",
      "⚠️ No label for v1_0265.jpg\n",
      "⚠️ No label for v1_0266.jpg\n",
      "⚠️ No label for v1_0267.jpg\n",
      "⚠️ No label for v1_0268.jpg\n",
      "⚠️ No label for v1_0269.jpg\n",
      "⚠️ No label for v1_0270.jpg\n",
      "⚠️ No label for v1_0271.jpg\n",
      "⚠️ No label for v1_0288.jpg\n",
      "⚠️ No label for v1_0293.jpg\n",
      "⚠️ No label for v2_0003.jpg\n",
      "⚠️ No label for v2_0121.jpg\n",
      "⚠️ No label for v3_0000.jpg\n",
      "⚠️ No label for v3_0001.jpg\n",
      "⚠️ No label for v3_0002.jpg\n",
      "⚠️ No label for v3_0003.jpg\n",
      "⚠️ No label for v3_0004.jpg\n",
      "⚠️ No label for v3_0005.jpg\n",
      "⚠️ No label for v3_0006.jpg\n",
      "⚠️ No label for v3_0007.jpg\n",
      "⚠️ No label for v3_0008.jpg\n",
      "⚠️ No label for v3_0009.jpg\n",
      "⚠️ No label for v3_0010.jpg\n",
      "⚠️ No label for v3_0011.jpg\n",
      "⚠️ No label for v3_0012.jpg\n",
      "⚠️ No label for v3_0013.jpg\n",
      "⚠️ No label for v3_0014.jpg\n",
      "⚠️ No label for v3_0015.jpg\n",
      "⚠️ No label for v3_0016.jpg\n",
      "⚠️ No label for v3_0017.jpg\n",
      "⚠️ No label for v3_0018.jpg\n",
      "⚠️ No label for v3_0019.jpg\n",
      "⚠️ No label for v3_0020.jpg\n",
      "⚠️ No label for v3_0021.jpg\n",
      "⚠️ No label for v3_0022.jpg\n",
      "⚠️ No label for v3_0023.jpg\n",
      "⚠️ No label for v3_0027.jpg\n",
      "⚠️ No label for v3_0028.jpg\n",
      "⚠️ No label for v3_0029.jpg\n",
      "⚠️ No label for v3_0030.jpg\n",
      "⚠️ No label for v3_0031.jpg\n",
      "⚠️ No label for v3_0032.jpg\n",
      "⚠️ No label for v3_0033.jpg\n",
      "⚠️ No label for v3_0034.jpg\n",
      "⚠️ No label for v3_0035.jpg\n",
      "⚠️ No label for v3_0037.jpg\n",
      "⚠️ No label for v3_0039.jpg\n",
      "⚠️ No label for v3_0040.jpg\n",
      "⚠️ No label for v3_0041.jpg\n",
      "⚠️ No label for v3_0042.jpg\n",
      "⚠️ No label for v3_0043.jpg\n",
      "⚠️ No label for v3_0044.jpg\n",
      "⚠️ No label for v3_0045.jpg\n",
      "⚠️ No label for v3_0046.jpg\n",
      "⚠️ No label for v3_0047.jpg\n",
      "⚠️ No label for v3_0048.jpg\n",
      "⚠️ No label for v3_0049.jpg\n",
      "⚠️ No label for v3_0051.jpg\n",
      "⚠️ No label for v3_0052.jpg\n",
      "⚠️ No label for v3_0053.jpg\n",
      "⚠️ No label for v3_0069.jpg\n",
      "⚠️ No label for v4_0000.jpg\n",
      "⚠️ No label for v4_0001.jpg\n",
      "⚠️ No label for v4_0119.jpg\n",
      "⚠️ No label for v4_0121.jpg\n",
      "⚠️ No label for v4_0123.jpg\n",
      "⚠️ No label for v4_0125.jpg\n",
      "⚠️ No label for v4_0127.jpg\n",
      "⚠️ No label for v4_0128.jpg\n",
      "⚠️ No label for v4_0129.jpg\n",
      "⚠️ No label for v4_0131.jpg\n",
      "⚠️ No label for v4_0132.jpg\n",
      "⚠️ No label for v4_0133.jpg\n",
      "⚠️ No label for v4_0134.jpg\n",
      "⚠️ No label for v4_0139.jpg\n",
      "⚠️ No label for v4_0141.jpg\n",
      "⚠️ No label for v4_0142.jpg\n",
      "⚠️ No label for v6_0001.jpg\n",
      "⚠️ No label for v6_0002.jpg\n",
      "⚠️ No label for v6_0003.jpg\n",
      "⚠️ No label for v6_0004.jpg\n",
      "⚠️ No label for v6_0005.jpg\n",
      "⚠️ No label for v6_0006.jpg\n",
      "⚠️ No label for v6_0007.jpg\n",
      "⚠️ No label for v6_0008.jpg\n",
      "⚠️ No label for v6_0009.jpg\n",
      "⚠️ No label for v6_0010.jpg\n",
      "⚠️ No label for v6_0011.jpg\n",
      "⚠️ No label for v6_0012.jpg\n",
      "⚠️ No label for v6_0013.jpg\n",
      "⚠️ No label for v6_0014.jpg\n",
      "⚠️ No label for v6_0015.jpg\n",
      "⚠️ No label for v6_0016.jpg\n",
      "⚠️ No label for v6_0017.jpg\n",
      "⚠️ No label for v6_0018.jpg\n",
      "⚠️ No label for v6_0019.jpg\n",
      "⚠️ No label for v6_0020.jpg\n",
      "⚠️ No label for v6_0052.jpg\n",
      "⚠️ No label for v6_0056.jpg\n",
      "⚠️ No label for v6_0298.jpg\n",
      "⚠️ No label for v6_0305.jpg\n",
      "⚠️ No label for v6_0308.jpg\n",
      "⚠️ No label for v6_0312.jpg\n",
      "⚠️ No label for v6_0313.jpg\n",
      "⚠️ No label for v6_0314.jpg\n",
      "⚠️ No label for v6_0315.jpg\n",
      "⚠️ No label for v6_0316.jpg\n",
      "⚠️ No label for v6_0317.jpg\n",
      "⚠️ No label for v6_0318.jpg\n",
      "⚠️ No label for v6_0319.jpg\n",
      "⚠️ No label for v6_0320.jpg\n",
      "⚠️ No label for v6_0321.jpg\n",
      "⚠️ No label for v6_0322.jpg\n",
      "⚠️ No label for v6_0323.jpg\n",
      "⚠️ No label for v6_0324.jpg\n",
      "⚠️ No label for v6_0325.jpg\n",
      "⚠️ No label for v6_0326.jpg\n",
      "⚠️ No label for v6_0327.jpg\n",
      "⚠️ No label for v6_0328.jpg\n",
      "⚠️ No label for v6_0329.jpg\n",
      "⚠️ No label for v6_0330.jpg\n",
      "⚠️ No label for v6_0331.jpg\n",
      "⚠️ No label for v6_0332.jpg\n",
      "⚠️ No label for v6_0333.jpg\n",
      "⚠️ No label for v6_0334.jpg\n",
      "⚠️ No label for v6_0335.jpg\n",
      "⚠️ No label for v6_0336.jpg\n",
      "⚠️ No label for v6_0337.jpg\n",
      "⚠️ No label for v6_0338.jpg\n",
      "⚠️ No label for v6_0339.jpg\n",
      "⚠️ No label for v6_0340.jpg\n",
      "⚠️ No label for v6_0341.jpg\n",
      "⚠️ No label for v6_0342.jpg\n",
      "⚠️ No label for v6_0343.jpg\n",
      "⚠️ No label for v6_0344.jpg\n",
      "⚠️ No label for v6_0345.jpg\n",
      "⚠️ No label for v6_0346.jpg\n",
      "⚠️ No label for v6_0347.jpg\n",
      "⚠️ No label for v6_0348.jpg\n",
      "⚠️ No label for v6_0349.jpg\n",
      "⚠️ No label for v6_0350.jpg\n",
      "⚠️ No label for v6_0351.jpg\n",
      "⚠️ No label for v6_0352.jpg\n",
      "⚠️ No label for v6_0358.jpg\n",
      "⚠️ No label for v6_0359.jpg\n",
      "⚠️ No label for v6_0360.jpg\n",
      "⚠️ No label for v6_0372.jpg\n",
      "⚠️ No label for v6_0381.jpg\n",
      "⚠️ No label for v6_0382.jpg\n",
      "⚠️ No label for v6_0383.jpg\n",
      "⚠️ No label for v6_0384.jpg\n",
      "⚠️ No label for v6_0385.jpg\n",
      "⚠️ No label for v6_0386.jpg\n",
      "⚠️ No label for v6_0387.jpg\n",
      "⚠️ No label for v6_0388.jpg\n",
      "⚠️ No label for v6_0389.jpg\n",
      "⚠️ No label for v6_0390.jpg\n",
      "⚠️ No label for v6_0391.jpg\n",
      "⚠️ No label for v6_0392.jpg\n",
      "⚠️ No label for v6_0393.jpg\n",
      "⚠️ No label for v6_0394.jpg\n",
      "⚠️ No label for v6_0395.jpg\n",
      "⚠️ No label for v6_0396.jpg\n",
      "⚠️ No label for v6_0397.jpg\n",
      "⚠️ No label for v6_0398.jpg\n",
      "⚠️ No label for v6_0399.jpg\n",
      "⚠️ No label for v8_0000.jpg\n",
      "⚠️ No label for v8_0004.jpg\n",
      "⚠️ No label for v8_0006.jpg\n",
      "⚠️ No label for v8_0007.jpg\n",
      "⚠️ No label for v8_0008.jpg\n",
      "⚠️ No label for v8_0160.jpg\n",
      "⚠️ No label for v8_0171.jpg\n",
      "⚠️ No label for v8_0172.jpg\n",
      "⚠️ No label for v8_0175.jpg\n",
      "⚠️ No label for v8_0176.jpg\n",
      "⚠️ No label for v8_0266.jpg\n",
      "⚠️ No label for v8_0269.jpg\n",
      "⚠️ No label for v8_0271.jpg\n",
      "⚠️ No label for v8_0275.jpg\n",
      "⚠️ No label for v8_0279.jpg\n",
      "⚠️ No label for v8_0280.jpg\n",
      "⚠️ No label for v8_0285.jpg\n",
      "⚠️ No label for v8_0360.jpg\n",
      "⚠️ No label for v8_0361.jpg\n",
      "⚠️ No label for v8_0362.jpg\n",
      "⚠️ No label for v8_0363.jpg\n",
      "⚠️ No label for v8_0364.jpg\n",
      "⚠️ No label for v8_0370.jpg\n",
      "⚠️ No label for v8_0371.jpg\n",
      "⚠️ No label for v8_0373.jpg\n",
      "⚠️ No label for v8_0374.jpg\n",
      "⚠️ No label for v8_0375.jpg\n",
      "⚠️ No label for v8_0376.jpg\n",
      "⚠️ No label for v8_0377.jpg\n",
      "⚠️ No label for v8_0378.jpg\n",
      "⚠️ No label for v8_0379.jpg\n",
      "⚠️ No label for v8_0380.jpg\n",
      "⚠️ No label for v8_0381.jpg\n",
      "⚠️ No label for v8_0382.jpg\n",
      "⚠️ No label for v8_0383.jpg\n",
      "⚠️ No label for v8_0384.jpg\n",
      "⚠️ No label for v8_0385.jpg\n",
      "⚠️ No label for v8_0386.jpg\n",
      "⚠️ No label for v8_0387.jpg\n",
      "⚠️ No label for v8_0388.jpg\n",
      "⚠️ No label for v8_0389.jpg\n",
      "⚠️ No label for v8_0390.jpg\n",
      "⚠️ No label for v8_0391.jpg\n",
      "⚠️ No label for v8_0392.jpg\n",
      "⚠️ No label for v8_0393.jpg\n",
      "⚠️ No label for v8_0398.jpg\n",
      "⚠️ No label for v8_0399.jpg\n",
      "\n",
      "✅ Merge complete! Merged dataset at: ./datasets/merged_yolo_dataset\n"
     ]
    }
   ],
   "source": [
    "merge_yolo_datasets(\n",
    "    source1='./datasets/new_dataset_yolo_split/train',\n",
    "    source2='./datasets/new-videos-predicted-yolo',\n",
    "    destination='./datasets/merged_yolo_dataset'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1eeb6de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class-wise instance counts:\n",
      "Class  1 (pool      ): 30 instances\n",
      "Class  2 (vegetation): 5528 instances\n",
      "Class  3 (roof      ): 354 instances\n",
      "Class  4 (wall      ): 1974 instances\n",
      "Class  5 (window    ): 542 instances\n",
      "Class  6 (person    ): 14247 instances\n",
      "Class  7 (dog       ): 25 instances\n",
      "Class  8 (car       ): 35045 instances\n",
      "Class  9 (bicycle   ): 507 instances\n",
      "Class 10 (tree      ): 1880 instances\n",
      "Class 11 (truck     ): 129 instances\n",
      "Class 12 (bus       ): 86 instances\n",
      "Class 13 (vehicle   ): 579 instances\n",
      "\n",
      "✅ Rare class IDs (threshold < 0): set()\n"
     ]
    }
   ],
   "source": [
    "# Folder with YOLO label files\n",
    "label_dir = './datasets/merged_yolo_dataset/labels'\n",
    "\n",
    "rare_class_ids = get_rare_class_ids(label_dir=label_dir, class_id_to_name=class_id_to_name ,rare_threshold=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2ca7c",
   "metadata": {},
   "source": [
    "## Retrain Model on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7f1df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Match all folders starting with 'fine-tune-yolov8' inside './runs_yolo/train/'\n",
    "folders_to_delete = glob.glob('./runs/train/fine-tune-yolov8*')\n",
    "\n",
    "for folder_path in folders_to_delete:\n",
    "    if os.path.isdir(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"✅ Deleted folder: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Not a directory or doesn't exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "103f6cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.109 🚀 Python-3.10.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 OEM, 7957MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=runs/train/yolov8/weights/best.pt, data=yolo_retrain.yaml, epochs=30, time=None, patience=30, batch=8, imgsz=720, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs/train, name=fine-tune-yolov8, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=10, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.2, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.2, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/train/fine-tune-yolov8\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3783802  ultralytics.nn.modules.head.Detect           [14, [192, 384, 576]]         \n",
      "Model summary: 169 layers, 25,864,426 parameters, 25,864,410 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 475/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "WARNING ⚠️ imgsz=[720] must be multiple of max stride 32, updating to [736]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/merged_yolo_dataset/labels... 4919 images, 301 backgrounds, 0 corrupt: 100%|██████████| 5217/5217 [00:01<00:00, 3948.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/merged_yolo_dataset/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/new-videos-predicted-yolo/labels... 2565 images, 298 backgrounds, 0 corrupt: 100%|██████████| 2863/2863 [00:00<00:00, 3199.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/new-videos-predicted-yolo/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/train/fine-tune-yolov8/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000556, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "Image sizes 736 train, 736 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/fine-tune-yolov8\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30      5.64G     0.8068     0.6317     0.9179          4        736: 100%|██████████| 653/653 [04:42<00:00,  2.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.709      0.525      0.559      0.441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30      5.77G     0.8166     0.6045     0.9126         15        736: 100%|██████████| 653/653 [04:39<00:00,  2.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.481      0.615        0.6      0.477\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30      5.27G     0.8363     0.6153     0.9133          6        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.703      0.575      0.623      0.499\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30      4.84G     0.8305     0.5945     0.9109         39        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.577      0.746      0.719      0.573\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      5.57G     0.8277     0.5948     0.9095         25        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.691      0.551      0.679      0.512\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30      5.26G     0.8182     0.5886     0.9082         16        736: 100%|██████████| 653/653 [04:37<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.735      0.532      0.607      0.485\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30      5.46G     0.8112      0.575      0.904          3        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.737      0.569      0.629      0.522\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30      4.92G     0.8071     0.5779     0.9025         42        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.678      0.579       0.62      0.517\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30       5.1G     0.7949     0.5581     0.8976         13        736: 100%|██████████| 653/653 [04:38<00:00,  2.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.658      0.764      0.737      0.591\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30      6.04G     0.7855     0.5535     0.8975         79        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.697      0.587      0.673      0.532\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30      5.28G     0.7805      0.553     0.8959         61        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.628      0.766      0.789      0.668\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30      4.97G     0.7701     0.5444     0.8961         42        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.706      0.645      0.764      0.635\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30       5.1G     0.7668     0.5379     0.8948         12        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.727      0.681       0.74      0.615\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30      5.18G     0.7545     0.5318     0.8906         52        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.634      0.789      0.759       0.64\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30      5.02G     0.7434     0.5207     0.8874          8        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.682      0.766      0.803      0.685\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30      5.19G      0.746      0.527     0.8872          2        736: 100%|██████████| 653/653 [05:02<00:00,  2.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:40<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.665      0.761      0.776       0.66\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30      5.58G     0.7408     0.5223      0.888          3        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.745      0.716      0.816      0.692\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30      5.05G     0.7241     0.5042     0.8824         45        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.649      0.798      0.776      0.669\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30      5.12G     0.7066      0.495     0.8772         12        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.727      0.752      0.799      0.691\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30      5.17G     0.7042     0.4906     0.8764         21        736: 100%|██████████| 653/653 [04:38<00:00,  2.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.722      0.749       0.82      0.697\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/30      4.75G     0.6426     0.4572     0.8625         41        736: 100%|██████████| 653/653 [04:35<00:00,  2.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.727      0.715      0.809      0.675\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30      5.18G     0.6277     0.4396     0.8584          8        736: 100%|██████████| 653/653 [04:35<00:00,  2.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.766      0.698      0.827      0.721\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30      4.81G     0.6146     0.4353      0.854          2        736: 100%|██████████| 653/653 [04:35<00:00,  2.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.689       0.78      0.821      0.718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30       4.8G      0.605     0.4245     0.8521         10        736: 100%|██████████| 653/653 [04:35<00:00,  2.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.749      0.753      0.836      0.733\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30      4.85G     0.5994     0.4181     0.8503         10        736: 100%|██████████| 653/653 [04:35<00:00,  2.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782        0.7      0.835       0.84      0.722\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30      4.93G     0.5884     0.4093     0.8466         10        736: 100%|██████████| 653/653 [04:35<00:00,  2.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.707      0.789      0.827      0.719\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30      4.88G     0.5732     0.3994     0.8449          8        736: 100%|██████████| 653/653 [04:35<00:00,  2.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782       0.72       0.82      0.835      0.718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30      5.26G      0.563     0.3953      0.845          8        736: 100%|██████████| 653/653 [04:35<00:00,  2.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.726      0.815      0.841      0.727\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30      5.26G     0.5518     0.3872      0.841         11        736: 100%|██████████| 653/653 [04:35<00:00,  2.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782       0.74      0.801      0.845      0.735\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30      5.23G     0.5417     0.3864     0.8368          0        736: 100%|██████████| 653/653 [04:35<00:00,  2.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:31<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.817      0.756      0.851      0.744\n",
      "\n",
      "30 epochs completed in 2.590 hours.\n",
      "Optimizer stripped from runs/train/fine-tune-yolov8/weights/last.pt, 52.0MB\n",
      "Optimizer stripped from runs/train/fine-tune-yolov8/weights/best.pt, 52.0MB\n",
      "\n",
      "Validating runs/train/fine-tune-yolov8/weights/best.pt...\n",
      "Ultralytics 8.3.109 🚀 Python-3.10.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 OEM, 7957MiB)\n",
      "Model summary (fused): 92 layers, 25,847,866 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [01:07<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.774      0.735      0.851      0.748\n",
      "                  roof         59         74      0.722      0.838      0.862      0.792\n",
      "                  wall        856       1020       0.86      0.852      0.927      0.888\n",
      "                window        162        166      0.819      0.693      0.842       0.72\n",
      "                person       1870      11772       0.84      0.518      0.782      0.598\n",
      "               bicycle        268        285      0.826      0.384      0.643      0.529\n",
      "                  tree       1001       1463      0.857      0.864      0.906      0.863\n",
      "               vehicle          2          2      0.491          1      0.995      0.846\n",
      "Speed: 0.1ms preprocess, 22.3ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/train/fine-tune-yolov8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_yolo(\"yolo_retrain.yaml\", 30, 720, 8, \"fine-tune-yolov8\", True, dir=best_pt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28391f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found results.csv at: ./runs/train/fine-tune-yolov8/results.csv\n",
      "Total number of epochs: 30\n",
      "\n",
      "========== Final Training Metrics ==========\n",
      "Training Loss: 1.764910\n",
      "Precision: 0.817130\n",
      "Recall: 0.755970\n",
      "mAP@0.5: 0.851300\n",
      "mAP@0.5:0.95: 0.744380\n",
      "\n",
      "========== Final Validation Metrics ==========\n",
      "Validation Loss: 2.098990\n"
     ]
    }
   ],
   "source": [
    "new_path = './runs/train/fine-tune-yolov8'\n",
    "print_csv_metrics(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1ac9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_final_metrics(csv1_path, csv2_path):\n",
    "    # Load both result CSVs\n",
    "    df1 = pd.read_csv(csv1_path)\n",
    "    df2 = pd.read_csv(csv2_path)\n",
    "    # print(df1.head())\n",
    "    # Use the final row (last epoch)\n",
    "    last1 = df1.iloc[-1]\n",
    "    last2 = df2.iloc[-1]\n",
    "\n",
    "    metrics_to_compare = {\n",
    "        \"train/box_loss\": \"Box Loss (Train)\",\n",
    "        \"train/cls_loss\": \"Cls Loss (Train)\",\n",
    "        \"train/dfl_loss\": \"DFL Loss (Train)\",\n",
    "        \"metrics/precision(B)\": \"Precision\",\n",
    "        \"metrics/recall(B)\": \"Recall\",\n",
    "        \"metrics/mAP50(B)\": \"mAP@0.5\",\n",
    "        \"metrics/mAP50-95(B)\": \"mAP@0.5:0.95\",\n",
    "        \"val/box_loss\": \"Box Loss (Val)\",\n",
    "        \"val/cls_loss\": \"Cls Loss (Val)\",\n",
    "        \"val/dfl_loss\": \"DFL Loss (Val)\"\n",
    "    }\n",
    "\n",
    "    print(\"🔍 Comparison of Final Epoch Metrics:\\n\")\n",
    "    for key, label in metrics_to_compare.items():\n",
    "        val1 = last1[key]\n",
    "        val2 = last2[key]\n",
    "        trend = \"✅ Good Increase\" if val2 > val1 else \"❌ No Increase\"\n",
    "        print(f\"{label:20s}: {val1:.5f} → {val2:.5f} | {trend}\")\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25163e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Comparison of Final Epoch Metrics:\n",
      "\n",
      "Box Loss (Train)    : 0.58367 → 0.54170 | ❌ No Increase\n",
      "Cls Loss (Train)    : 0.32681 → 0.38641 | ✅ Good Increase\n",
      "DFL Loss (Train)    : 0.84170 → 0.83680 | ❌ No Increase\n",
      "Precision           : 0.59285 → 0.81713 | ✅ Good Increase\n",
      "Recall              : 0.33184 → 0.75597 | ✅ Good Increase\n",
      "mAP@0.5             : 0.40186 → 0.85130 | ✅ Good Increase\n",
      "mAP@0.5:0.95        : 0.28034 → 0.74438 | ✅ Good Increase\n",
      "Box Loss (Val)      : 1.78693 → 0.54063 | ❌ No Increase\n",
      "Cls Loss (Val)      : 2.43818 → 0.70702 | ❌ No Increase\n",
      "DFL Loss (Val)      : 1.04476 → 0.85134 | ❌ No Increase\n"
     ]
    }
   ],
   "source": [
    "new_path = './runs/train/fine-tune-yolov8'\n",
    "old_path = './runs/train/yolov8'\n",
    "\n",
    "results_csv_path = find_results_csv(new_path)\n",
    "results_csv_path_1 = find_results_csv(old_path)\n",
    "\n",
    "compare_final_metrics(results_csv_path_1, results_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33362264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found best.pt at: runs/train/fine-tune-yolov8/weights/best.pt\n",
      "Ultralytics 8.3.109 🚀 Python-3.10.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 OEM, 7957MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 92 layers, 25,847,866 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ssl49/Desktop/Automated-Labeling-for-Aerial-Images-main/Automated_SegmentAndYolo/datasets/new-videos-predicted-yolo/labels.cache... 2565 images, 298 backgrounds, 0 corrupt: 100%|██████████| 2863/2863 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 179/179 [00:51<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2863      14782      0.818      0.756      0.852      0.745\n",
      "                  roof         59         74      0.681      0.838      0.848      0.792\n",
      "                  wall        856       1020      0.895      0.835      0.932      0.892\n",
      "                window        162        166      0.733      0.759      0.847      0.758\n",
      "                person       1870      11772      0.808      0.587      0.792      0.635\n",
      "               bicycle        268        285      0.748      0.418      0.642      0.528\n",
      "                  tree       1001       1463      0.865      0.853      0.905      0.867\n",
      "               vehicle          2          2      0.996          1      0.995      0.747\n",
      "Speed: 0.2ms preprocess, 16.6ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n",
      "✅ Saved per-class metrics to per_class_metrics_retrain.json\n"
     ]
    }
   ],
   "source": [
    "best_pt_path = find_best_model(new_path)\n",
    "evaluate_and_save_metrics(best_pt_path, output_json_path=\"per_class_metrics_retrain.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edceacbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Per-Class mAP@0.5:0.95 Metrics:\n",
      "\n",
      "Class Name      Class ID   mAP@0.5:0.95   \n",
      "----------------------------------------\n",
      "road            0          0.7454         \n",
      "pool            1          0.7454         \n",
      "vegetation      2          0.7454         \n",
      "roof            3          0.7915         \n",
      "wall            4          0.8921         \n",
      "window          5          0.7584         \n",
      "person          6          0.6346         \n",
      "dog             7          0.7454         \n",
      "car             8          0.7454         \n",
      "bicycle         9          0.5281         \n",
      "tree            10         0.8665         \n",
      "truck           11         0.7454         \n",
      "bus             12         0.7454         \n",
      "vehicle         13         0.7467         \n"
     ]
    }
   ],
   "source": [
    "print_per_class_metrics(\"per_class_metrics_retrain.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97c1a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Comparison of mAP@0.5:0.95 per class:\n",
      "\n",
      "Class           Before     After      Change\n",
      "--------------------------------------------------\n",
      "unlabeled       0.2943     0.0000     ❌ No increase\n",
      "pool            0.8215     0.7454     ❌ No increase\n",
      "vegetation      0.1216     0.7454     ✅ Good increase\n",
      "roof            0.5337     0.7915     ✅ Good increase\n",
      "wall            0.1327     0.8921     ✅ Good increase\n",
      "window          0.1891     0.7584     ✅ Good increase\n",
      "person          0.2558     0.6346     ✅ Good increase\n",
      "dog             0.4714     0.7454     ✅ Good increase\n",
      "car             0.3195     0.7454     ✅ Good increase\n",
      "bicycle         0.2275     0.5281     ✅ Good increase\n",
      "tree            0.4541     0.8665     ✅ Good increase\n",
      "truck           0.0000     0.7454     ✅ Good increase\n",
      "bus             0.0000     0.7454     ✅ Good increase\n",
      "vehicle         0.2993     0.7467     ✅ Good increase\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def compare_maps(json_path1, json_path2):\n",
    "    with open(json_path1, 'r') as f1, open(json_path2, 'r') as f2:\n",
    "        metrics1 = json.load(f1)\n",
    "        metrics2 = json.load(f2)\n",
    "\n",
    "    print(\"\\n📊 Comparison of mAP@0.5:0.95 per class:\\n\")\n",
    "    print(f\"{'Class':<15} {'Before':<10} {'After':<10} {'Change'}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for class_name in metrics1:\n",
    "        map1 = metrics1[class_name].get(\"mAP@0.5:0.95\", 0)\n",
    "        map2 = metrics2.get(class_name, {}).get(\"mAP@0.5:0.95\", 0)\n",
    "\n",
    "        if map2 > map1:\n",
    "            status = \"✅ Good increase\"\n",
    "        else:\n",
    "            status = \"❌ No increase\"\n",
    "\n",
    "        print(f\"{class_name:<15} {map1:<10.4f} {map2:<10.4f} {status}\")\n",
    "\n",
    "# 🔧 Example usage:\n",
    "compare_maps(\"per_class_metrics.json\", \"per_class_metrics_retrain.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db73f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found best.pt at: runs/train/fine-tune-yolov8/weights/best.pt\n",
      "✅ Found best.pt at: runs/train/yolov8/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "new_path = './runs/train/fine-tune-yolov8'\n",
    "old_path = './runs/train/yolov8'\n",
    "\n",
    "best_pt_path_retrain = find_best_model(new_path)\n",
    "best_pt_path = find_best_model(old_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "933909df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== STARTED: v1 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v1: 100%|██████████| 400/400 [00:28<00:00, 13.93it/s]\n",
      " 11%|█         | 1/9 [00:28<03:50, 28.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v1 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v10 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v10:  65%|██████▍   | 259/400 [00:19<00:10, 13.28it/s]\n",
      " 22%|██▏       | 2/9 [00:48<02:43, 23.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v10 — Processed 259 frames\n",
      "\n",
      "========== STARTED: v2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v2:  44%|████▍     | 175/400 [00:12<00:16, 13.65it/s]\n",
      " 33%|███▎      | 3/9 [01:01<01:51, 18.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v2 — Processed 175 frames\n",
      "\n",
      "========== STARTED: v3 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v3:  44%|████▍     | 176/400 [00:04<00:05, 42.94it/s]\n",
      " 44%|████▍     | 4/9 [01:05<01:04, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v3 — Processed 176 frames\n",
      "\n",
      "========== STARTED: v4 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v4:  63%|██████▎   | 253/400 [00:08<00:05, 29.39it/s]\n",
      " 56%|█████▌    | 5/9 [01:13<00:45, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v4 — Processed 253 frames\n",
      "\n",
      "========== STARTED: v5 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v5: 100%|██████████| 400/400 [00:35<00:00, 11.41it/s]\n",
      " 67%|██████▋   | 6/9 [01:49<00:58, 19.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v5 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v6 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v6: 100%|██████████| 400/400 [00:42<00:00,  9.47it/s]\n",
      " 78%|███████▊  | 7/9 [02:31<00:53, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v6 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v8 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v8: 100%|██████████| 400/400 [00:32<00:00, 12.49it/s]\n",
      " 89%|████████▉ | 8/9 [03:03<00:28, 28.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v8 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v9 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v9: 100%|██████████| 400/400 [00:40<00:00,  9.87it/s]\n",
      "100%|██████████| 9/9 [03:44<00:00, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v9 — Processed 400 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "videos_predictions(best_pt_path, class_id_to_name, video_dir='videos', output_base='./datasets/final_output', max_frames=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7872e10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== STARTED: v1 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v1: 100%|██████████| 400/400 [00:28<00:00, 13.98it/s]\n",
      " 11%|█         | 1/9 [00:28<03:49, 28.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v1 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v10 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v10:  65%|██████▍   | 259/400 [00:18<00:10, 13.83it/s]\n",
      " 22%|██▏       | 2/9 [00:47<02:39, 22.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v10 — Processed 259 frames\n",
      "\n",
      "========== STARTED: v2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v2:  44%|████▍     | 175/400 [00:12<00:16, 13.94it/s]\n",
      " 33%|███▎      | 3/9 [01:00<01:49, 18.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v2 — Processed 175 frames\n",
      "\n",
      "========== STARTED: v3 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v3:  44%|████▍     | 176/400 [00:03<00:05, 44.07it/s]\n",
      " 44%|████▍     | 4/9 [01:04<01:02, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v3 — Processed 176 frames\n",
      "\n",
      "========== STARTED: v4 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v4:  63%|██████▎   | 253/400 [00:08<00:04, 29.85it/s]\n",
      " 56%|█████▌    | 5/9 [01:12<00:44, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v4 — Processed 253 frames\n",
      "\n",
      "========== STARTED: v5 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v5: 100%|██████████| 400/400 [00:31<00:00, 12.61it/s]\n",
      " 67%|██████▋   | 6/9 [01:44<00:54, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v5 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v6 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v6: 100%|██████████| 400/400 [00:28<00:00, 13.96it/s]\n",
      " 78%|███████▊  | 7/9 [02:13<00:43, 21.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v6 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v8 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v8: 100%|██████████| 400/400 [00:16<00:00, 23.72it/s]\n",
      " 89%|████████▉ | 8/9 [02:29<00:20, 20.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v8 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v9 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v9: 100%|██████████| 400/400 [00:32<00:00, 12.13it/s]\n",
      "100%|██████████| 9/9 [03:03<00:00, 20.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v9 — Processed 400 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "videos_predictions(best_pt_path_retrain, class_id_to_name, video_dir='videos', output_base='./datasets/final_output_retrain', max_frames=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
