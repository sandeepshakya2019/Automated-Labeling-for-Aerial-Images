{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074d4a4f",
   "metadata": {},
   "source": [
    "## Builting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150d5af",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59ec5504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built CUDA Version: None\n",
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print the CUDA version PyTorch is built with\n",
    "print(\"Built CUDA Version:\", torch.version.cuda)\n",
    "\n",
    "# Print the CUDA version runtime (if CUDA is available)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Runtime Version:\", torch._C._cuda_getCompiledVersion())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "206e74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id_to_name = {\n",
    "    0:  ('unlabeled', [28, 42, 168]),\n",
    "    1:  ('pool', [0, 50, 89]),\n",
    "    2:  ('vegetation', [107, 142, 35]),\n",
    "    3:  ('roof', [70, 70, 70]),\n",
    "    4:  ('wall', [102, 102, 156]),\n",
    "    5:  ('window', [254, 228, 12]),\n",
    "    6:  ('person', [255, 22, 96]),\n",
    "    7:  ('dog', [102, 51, 0]),\n",
    "    8:  ('car', [9, 143, 150]),\n",
    "    9:  ('bicycle', [119, 11, 32]),\n",
    "    10: ('tree', [51, 51, 0]),\n",
    "    11: ('truck', [160, 160, 60]),   # added truck\n",
    "    12: ('bus', [200, 80, 80]),      # added bus\n",
    "    13: ('vehicle', [20, 80, 80]),      # added bus\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52746b61",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7b06f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install opencv-python\n",
    "# !pip install pillow\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch torchvision\n",
    "# !pip install ultralytics\n",
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "892fe670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch torchvision torchaudio\n",
    "# !pip cache purge  # clean out pip's install cache\n",
    "# !pip install torch torchvision torchaudio --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cdfb2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models.segmentation as segmentation\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import gdown\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd42d4",
   "metadata": {},
   "source": [
    "### Download Datsets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f810e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_drone_dataset_download(gdrive_url, extract_to=\"extracted\"):\n",
    "    # Convert shared drive URL to direct download URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create output folder\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[INFO] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[INFO] Extracting ZIP...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Optionally, remove the ZIP file after extraction\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[DONE] Extracted files to: {extract_to}\")\n",
    "\n",
    "def uavdt_dataset_download(gdrive_url, extract_to=\"extracted\"):\n",
    "    # Convert shared drive URL to direct download URL\n",
    "    file_id = gdrive_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Create output folder\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, \"downloaded.zip\")\n",
    "\n",
    "    print(\"[INFO] Downloading ZIP from Google Drive...\")\n",
    "    gdown.download(download_url, zip_path, quiet=False)\n",
    "\n",
    "    print(\"[INFO] Extracting ZIP...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Optionally, remove the ZIP file after extraction\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    print(f\"[DONE] Extracted files to: {extract_to}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09011805",
   "metadata": {},
   "source": [
    "### Convert Two Datsets into yolo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30ca98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Parse polygon and convert to YOLO bbox\n",
    "# ----------------------------\n",
    "# Semantic drone datasets \n",
    "def parse_yolo_style_bbox_from_xml(xml_path, class_id_to_name):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "        if class_name in [value[0] for value in class_id_to_name.values()]:\n",
    "            polygon = obj.find('polygon')\n",
    "            if polygon is not None:\n",
    "                points = polygon.findall('pt')\n",
    "                coords = [(float(pt.find('x').text), float(pt.find('y').text)) for pt in points]\n",
    "                x_min = min(coord[0] for coord in coords)\n",
    "                y_min = min(coord[1] for coord in coords)\n",
    "                x_max = max(coord[0] for coord in coords)\n",
    "                y_max = max(coord[1] for coord in coords)\n",
    "                bboxes.append(((x_min, y_min), (x_max, y_max), class_name))\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Save YOLO-format txt\n",
    "# ----------------------------\n",
    "def save_yolo_format(image_id, bboxes, image_width, image_height, output_path, class_id_to_name):\n",
    "    with open(output_path, 'w') as f:\n",
    "        for (x_min, y_min), (x_max, y_max), class_name in bboxes:\n",
    "            class_id = next(cid for cid, (name, _) in class_id_to_name.items() if name == class_name)\n",
    "            x_center = (x_min + x_max) / 2 / image_width\n",
    "            y_center = (y_min + y_max) / 2 / image_height\n",
    "            width = (x_max - x_min) / image_width\n",
    "            height = (y_max - y_min) / image_height\n",
    "            f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Convert dataset (YOLO only)\n",
    "# ----------------------------\n",
    "def convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name):\n",
    "    image_ids = [img.split('.')[0] for img in os.listdir(f\"{dataset_path}/images\") if img.endswith(\".jpg\")]\n",
    "\n",
    "    os.makedirs(f\"{output_dir}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/labels\", exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(image_ids, desc=\"Converting to YOLO\"):\n",
    "        img_path = f\"{dataset_path}/images/{image_id}.jpg\"\n",
    "        bbox_xml_path = f\"{dataset_path}/gt/bounding_box/label_me_xml/{image_id}.xml\"\n",
    "        semantic_xml_path = f\"{dataset_path}/gt/semantic/label_me_xml/{image_id}.xml\"\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"[WARNING] Image not found: {img_path}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            bboxes1 = parse_yolo_style_bbox_from_xml(bbox_xml_path, class_id_to_name)\n",
    "            bboxes2 = parse_yolo_style_bbox_from_xml(semantic_xml_path, class_id_to_name)\n",
    "            all_bboxes = bboxes1 + bboxes2\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Skipping image {image_id} due to parse error: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            image_np = np.array(image)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not load image {image_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save image\n",
    "        image.save(f\"{output_dir}/images/{image_id}.jpg\")\n",
    "\n",
    "        # Save YOLO labels\n",
    "        yolo_annotation_path = f\"{output_dir}/labels/{image_id}.txt\"\n",
    "        save_yolo_format(image_id, all_bboxes, image_np.shape[1], image_np.shape[0], yolo_annotation_path, class_id_to_name)\n",
    "\n",
    "    print(\"✅ YOLO-format annotation conversion complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a94cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 🧠 Map UAVDT class to extended class_id_to_name\n",
    "uavdt_to_extended = {\n",
    "    0: 8,   # car\n",
    "    1: 11,  # truck\n",
    "    2: 12,  # bus\n",
    "    3: 13\n",
    "}\n",
    "\n",
    "# === Function to Convert Single Annotation to YOLO Format ===\n",
    "def convert_annotation(anno_path, label_path, image_path, stats):\n",
    "    if not os.path.exists(image_path):\n",
    "        stats[\"missing_image\"] += 1\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        height, width = img.shape[:2]\n",
    "    except:\n",
    "        stats[\"missing_image\"] += 1\n",
    "        return\n",
    "\n",
    "    with open(anno_path, 'r') as fin, open(label_path, 'w') as fout:\n",
    "        for line in fin:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) < 8:\n",
    "                stats[\"malformed\"] += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                x, y, w, h = map(float, parts[0:4])\n",
    "                original_cls = int(parts[5])\n",
    "\n",
    "                # 🔁 Convert original class to extended class\n",
    "                if original_cls not in uavdt_to_extended:\n",
    "                    stats[\"skipped\"][original_cls] += 1\n",
    "                    continue\n",
    "\n",
    "                cls = uavdt_to_extended[original_cls]\n",
    "\n",
    "                x_center = (x + w / 2) / width\n",
    "                y_center = (y + h / 2) / height\n",
    "                w /= width\n",
    "                h /= height\n",
    "\n",
    "                if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and w > 0 and h > 0):\n",
    "                    stats[\"skipped\"][cls] += 1\n",
    "                    continue\n",
    "\n",
    "                fout.write(f\"{cls} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "                stats[\"converted\"] += 1\n",
    "            except Exception:\n",
    "                stats[\"malformed\"] += 1\n",
    "                continue\n",
    "\n",
    "            stats[\"total\"] += 1\n",
    "\n",
    "# === Step 1: Convert UAVDT annotations to YOLO format ===\n",
    "def convert_dataset(root_dir):\n",
    "    annotation_paths = glob(os.path.join(root_dir, \"M*/annotations/*.txt\"))\n",
    "    total_files = len(annotation_paths)\n",
    "\n",
    "    stats = {\n",
    "        \"total\": 0,\n",
    "        \"converted\": 0,\n",
    "        \"malformed\": 0,\n",
    "        \"missing_image\": 0,\n",
    "        \"skipped\": defaultdict(int)\n",
    "    }\n",
    "\n",
    "    print(f\"🔄 Converting {total_files} annotation files to YOLO format...\")\n",
    "\n",
    "    for anno_path in tqdm(annotation_paths, desc=\"Converting\", unit=\"file\"):\n",
    "        sequence_dir = os.path.dirname(os.path.dirname(anno_path))  # Mxxxx\n",
    "        file_name = os.path.basename(anno_path)\n",
    "\n",
    "        label_dir = os.path.join(sequence_dir, \"labels\")\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "        label_path = os.path.join(label_dir, file_name)\n",
    "\n",
    "        # Construct image path\n",
    "        image_name = file_name.replace(\".txt\", \".jpg\")\n",
    "        image_path = os.path.join(sequence_dir, \"images\", image_name)\n",
    "\n",
    "        convert_annotation(anno_path, label_path, image_path, stats)\n",
    "\n",
    "    print(\"\\nConversion complete.\")\n",
    "    print(f\"Total boxes:     {stats['total']}\")\n",
    "    print(f\"Converted boxes: {stats['converted']}\")\n",
    "    print(f\"Skipped boxes:   {sum(stats['skipped'].values())}\")\n",
    "    for cls, count in sorted(stats[\"skipped\"].items()):\n",
    "        print(f\"   - Skipped class {cls}: {count}\")\n",
    "    print(f\"Malformed lines: {stats['malformed']}\")\n",
    "    print(f\"Missing images: {stats['missing_image']}\")\n",
    "\n",
    "# === Step 2: Copy to train/val structure ===\n",
    "def copy_split_sequences(src_root, dst_root, train_ratio=0.8):\n",
    "    all_sequences = sorted(glob(os.path.join(src_root, \"M*\")))\n",
    "    train_seqs, val_seqs = train_test_split(all_sequences, train_size=train_ratio, random_state=42)\n",
    "\n",
    "    for split_name, split_list in zip(['train', 'val'], [train_seqs, val_seqs]):\n",
    "        for seq_path in tqdm(split_list, desc=f\"Copying {split_name}\"):\n",
    "            images_src = os.path.join(seq_path, \"images\")\n",
    "            labels_src = os.path.join(seq_path, \"labels\")\n",
    "\n",
    "            images_dst = os.path.join(dst_root, split_name, \"images\")\n",
    "            labels_dst = os.path.join(dst_root, split_name, \"labels\")\n",
    "\n",
    "            os.makedirs(images_dst, exist_ok=True)\n",
    "            os.makedirs(labels_dst, exist_ok=True)\n",
    "\n",
    "            for img_file in glob(os.path.join(images_src, \"*.jpg\")):\n",
    "                shutil.copy(img_file, os.path.join(images_dst, os.path.basename(img_file)))\n",
    "\n",
    "            for label_file in glob(os.path.join(labels_src, \"*.txt\")):\n",
    "                shutil.copy(label_file, os.path.join(labels_dst, os.path.basename(label_file)))\n",
    "\n",
    "    print(\"\\nDataset split into 'train/' and 'val/' with images and YOLO labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b9227",
   "metadata": {},
   "source": [
    "### Convert into train and Val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36ec4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Drone Datasets\n",
    "def move_files(file_list, \n",
    "               source_image_dir, \n",
    "               source_annotation_dir,\n",
    "               target_image_dir, \n",
    "               target_annotation_dir):\n",
    "    \n",
    "    os.makedirs(target_image_dir, exist_ok=True)\n",
    "    os.makedirs(target_annotation_dir, exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(file_list, desc=f\"Moving to {os.path.basename(os.path.dirname(target_image_dir))}\"):\n",
    "        image_path = os.path.join(source_image_dir, f\"{image_id}.jpg\")\n",
    "        annotation_path = os.path.join(source_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        target_image_path = os.path.join(target_image_dir, f\"{image_id}.jpg\")\n",
    "        target_annotation_path = os.path.join(target_annotation_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        if os.path.exists(image_path) and os.path.exists(annotation_path):\n",
    "            shutil.copy(image_path, target_image_path)\n",
    "            shutil.copy(annotation_path, target_annotation_path)\n",
    "\n",
    "\n",
    "def split_and_move_dataset(source_base_dir=\"./datasets/semantic_yolo\",\n",
    "                           target_base_dir=\"./datasets/new_dataset_yolo_split\",\n",
    "                           split_ratio=0.8,\n",
    "                           seed=42):\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    image_dir = os.path.join(source_base_dir, \"images\")\n",
    "    label_dir = os.path.join(source_base_dir, \"labels\")\n",
    "\n",
    "    image_ids = [os.path.splitext(f)[0] for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "    random.shuffle(image_ids)\n",
    "\n",
    "    split_idx = int(len(image_ids) * split_ratio)\n",
    "    train_ids = image_ids[:split_idx]\n",
    "    val_ids = image_ids[split_idx:]\n",
    "\n",
    "    # Train\n",
    "    move_files(train_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"train/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"train/labels\"))\n",
    "\n",
    "    # Val\n",
    "    move_files(val_ids,\n",
    "               source_image_dir=image_dir,\n",
    "               source_annotation_dir=label_dir,\n",
    "               target_image_dir=os.path.join(target_base_dir, \"val/images\"),\n",
    "               target_annotation_dir=os.path.join(target_base_dir, \"val/labels\"))\n",
    "\n",
    "    print(f\"\\n[✓] Dataset split completed: {len(train_ids)} train / {len(val_ids)} val samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110fe96d",
   "metadata": {},
   "source": [
    "### Normalize Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "541e4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label_file(label_file, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Normalize the label coordinates in a label file to ensure they are within [0, 1] range.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    with open(label_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, width, height = map(float, parts[1:])\n",
    "            \n",
    "            # Normalize coordinates to ensure they are within the range [0, 1]\n",
    "            x_center = min(1.0, max(0.0, x_center))\n",
    "            y_center = min(1.0, max(0.0, y_center))\n",
    "            width = min(1.0, max(0.0, width))\n",
    "            height = min(1.0, max(0.0, height))\n",
    "\n",
    "            # Write normalized values back to file\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "\n",
    "def get_image_size(img_path):\n",
    "    \"\"\"\n",
    "    Get the width and height of the image to normalize the coordinates properly.\n",
    "    \"\"\"\n",
    "    with Image.open(img_path) as img:\n",
    "        return img.size  # returns (width, height)\n",
    "\n",
    "\n",
    "def normalize_all_labels(labels_dir, img_dir):\n",
    "    \"\"\"\n",
    "    Normalize all label files in the specified directory.\n",
    "    \"\"\"\n",
    "    for label_file in tqdm(os.listdir(labels_dir)):\n",
    "       \n",
    "        if label_file.endswith('.txt'):  # Process only label files\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            img_path = os.path.join(img_dir, label_file.replace('.txt', '.jpg'))  # Assuming JPG images\n",
    "            if os.path.exists(img_path):\n",
    "                # Get image dimensions to normalize the labels\n",
    "                img_width, img_height = get_image_size(img_path)\n",
    "                # print(f\"Normalizing {label_file}...\")\n",
    "                normalize_label_file(label_path, img_width, img_height)\n",
    "            else:\n",
    "                print(f\"Warning: Image for label {label_file} not found!\")\n",
    "    print(\"Normalize Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce6fe5",
   "metadata": {},
   "source": [
    "### Training v8 model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "480ae8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "def train_yolo_from_scratch(data_yaml, epochs, imgsz, batch, name, model_variant=\"yolov8n.pt\"):\n",
    "    print(f\"[+] Training from scratch using base model: {model_variant}\")\n",
    "    model = YOLO(model_variant)\n",
    "\n",
    "    model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        name=name,\n",
    "        project=\"runs/train\",\n",
    "        augment=True,\n",
    "        degrees=10,\n",
    "        scale=0.5,\n",
    "        flipud=0.2,\n",
    "        fliplr=0.5,\n",
    "        hsv_h=0.015,\n",
    "        hsv_s=0.7,\n",
    "        hsv_v=0.4,\n",
    "        mosaic=1.0,\n",
    "        mixup=0.2,\n",
    "        lr0=0.01,\n",
    "        lrf=0.01,\n",
    "        verbose=True,\n",
    "        patience=30\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2925090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def fine_tune_yolo(data_yaml, epochs, imgsz, batch, name, base_model_path):\n",
    "    print(f\"[+] Fine-tuning model from: {base_model_path}\")\n",
    "    \n",
    "    # Load the already trained model\n",
    "    model = YOLO(base_model_path)\n",
    "\n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Fine-tune with lower LR\n",
    "    model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        name=name,\n",
    "        project=\"runs/train\",\n",
    "        lr0=0.0001,  # Lower learning rate for fine-tuning\n",
    "        lrf=0.01,\n",
    "        augment=True,\n",
    "        degrees=10,\n",
    "        scale=0.5,\n",
    "        flipud=0.2,\n",
    "        fliplr=0.5,\n",
    "        hsv_h=0.015,\n",
    "        hsv_s=0.7,\n",
    "        hsv_v=0.4,\n",
    "        mosaic=1.0,\n",
    "        mixup=0.2,\n",
    "        verbose=True,\n",
    "        patience=10\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ff318",
   "metadata": {},
   "source": [
    "### Print val metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf85d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def load_yolo_model(model_path):\n",
    "    return YOLO(model_path)\n",
    "\n",
    "def run_model_validation(model):\n",
    "    return model.val()\n",
    "\n",
    "def extract_per_class_metrics(results):\n",
    "    \"\"\"\n",
    "    Extracts mAP@0.5:0.95 per class from results.\n",
    "    NOTE: Only mAP@0.5:0.95 is available via `results.box.maps`\n",
    "    \"\"\"\n",
    "    per_class_metrics = {}\n",
    "    if hasattr(results.box, 'maps') and results.box.maps is not None:\n",
    "        maps = results.box.maps  # This is a NumPy array [num_classes]\n",
    "        for i, name in results.names.items():\n",
    "            per_class_metrics[name] = {\n",
    "                \"class_id\": i,\n",
    "                \"mAP@0.5:0.95\": round(float(maps[i]), 4)\n",
    "            }\n",
    "    else:\n",
    "        print(\"⚠️ No per-class mAP@0.5:0.95 data found.\")\n",
    "    return per_class_metrics\n",
    "\n",
    "def save_metrics_to_json(metrics, output_path):\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(f\"✅ Saved per-class metrics to {output_path}\")\n",
    "\n",
    "def evaluate_and_save_metrics(model_path, output_json_path=\"per_class_metrics.json\"):\n",
    "    model = load_yolo_model(model_path)\n",
    "    results = run_model_validation(model)\n",
    "    metrics = extract_per_class_metrics(results)\n",
    "    save_metrics_to_json(metrics, output_json_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "65ca8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def print_per_class_metrics(json_path=\"per_class_metrics.json\"):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(\"📊 Per-Class mAP@0.5:0.95 Metrics:\\n\")\n",
    "    print(f\"{'Class Name':<15} {'Class ID':<10} {'mAP@0.5:0.95':<15}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for name, data in metrics.items():\n",
    "        print(f\"{name:<15} {data['class_id']:<10} {data['mAP@0.5:0.95']:<15}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22b1a8",
   "metadata": {},
   "source": [
    "### Find best model path after training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0b390e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(base_dir='runs_yolo/'):\n",
    "    best_paths = list(Path(base_dir).rglob('best.pt'))\n",
    "    if not best_paths:\n",
    "        raise FileNotFoundError(\"No 'best.pt' file found in the 'runs/' directory.\")\n",
    "    \n",
    "    # Optionally, sort by latest modified time\n",
    "    best_paths.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    print(f\"✅ Found best.pt at: {best_paths[0]}\")\n",
    "    return str(best_paths[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378bc85",
   "metadata": {},
   "source": [
    "### Prediction on vdieos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a013ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== FRAME PROCESSING ==========\n",
    "import cv2\n",
    "\n",
    "def process_frame_1(frame, yolo_model, w, h, class_id_to_name, conf_threshold=0.5):\n",
    "    annotated = frame.copy()\n",
    "    results = yolo_model(annotated, verbose=False)[0]\n",
    "    boxes = results.boxes.xyxy.cpu().numpy()\n",
    "    class_ids = results.boxes.cls.cpu().numpy()\n",
    "    confidences = results.boxes.conf.cpu().numpy()  # Confidence scores for each box\n",
    "\n",
    "    for box, cls_id, confidence in zip(boxes, class_ids, confidences):\n",
    "        if confidence > conf_threshold:  # Filter based on confidence\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            class_name, color = class_id_to_name[int(cls_id)]\n",
    "            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(annotated, f\"{class_name} {confidence:.2f}\", (x1, max(y1 - 10, 10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "    return annotated, boxes, class_ids\n",
    "\n",
    "\n",
    "# ========== VIDEO CAPTURE ==========\n",
    "def setup_video_capture_1(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    return cap, total_frames, fps, w, h\n",
    "\n",
    "# ========== MAIN FUNCTION ==========\n",
    "def videos_predictions(yolo_weights_path, class_id_to_name, video_dir='videos', output_base='./datatsets/opt', max_frames=None):\n",
    "    yolo_model = YOLO(yolo_weights_path)\n",
    "\n",
    "    image_out_dir = os.path.join(output_base, 'images')\n",
    "    label_out_dir = os.path.join(output_base, 'labels')\n",
    "    output_video_dir = os.path.join(output_base, 'output')\n",
    "\n",
    "    os.makedirs(image_out_dir, exist_ok=True)\n",
    "    os.makedirs(label_out_dir, exist_ok=True)\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    for video_file in tqdm(sorted(os.listdir(video_dir))):\n",
    "        if not video_file.lower().endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        video_id = os.path.splitext(video_file)[0]\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        output_video_path = os.path.join(output_video_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        print(f\"\\n========== STARTED: {video_id} ==========\")\n",
    "        cap, total_frames, fps, w, h = setup_video_capture_1(video_path)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(output_video_path, fourcc, fps, (w, h))\n",
    "\n",
    "        frame_count = 0\n",
    "        pbar = tqdm(total=max_frames if max_frames else total_frames, desc=video_id)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or (max_frames and frame_count >= max_frames):\n",
    "                break\n",
    "\n",
    "            annotated_bgr, boxes, class_ids = process_frame_1(frame, yolo_model, w, h, class_id_to_name)\n",
    "\n",
    "            # ✅ Save original image\n",
    "            img_filename = f'{video_id}_{frame_count:04d}.jpg'\n",
    "            img_path = os.path.join(image_out_dir, img_filename)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "\n",
    "            # ✅ Save YOLO-format label\n",
    "            label_filename = f'{video_id}_{frame_count:04d}.txt'\n",
    "            label_path = os.path.join(label_out_dir, label_filename)\n",
    "            with open(label_path, 'w') as f:\n",
    "                for box, cls_id in zip(boxes, class_ids):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    w_box = x2 - x1\n",
    "                    h_box = y2 - y1\n",
    "                    cx = x1 + w_box / 2\n",
    "                    cy = y1 + h_box / 2\n",
    "                    f.write(f\"{int(cls_id)} {cx/w:.6f} {cy/h:.6f} {w_box/w:.6f} {h_box/h:.6f}\\n\")\n",
    "\n",
    "            writer.write(annotated_bgr)\n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "        pbar.close()\n",
    "        print(f\"DONE: {video_id} — Processed {frame_count} frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce3db596",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ========== FRAME CHANGE DETECTION ========== \n",
    "def get_different_frame_indices(video_path, diff_threshold=30.0):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    _, prev_frame = cap.read()\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    selected_indices = [0]  # always include the first frame\n",
    "    frame_idx = 1\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        diff = cv2.absdiff(gray, prev_gray)\n",
    "        avg_diff = np.count_nonzero(diff) / diff.size * 100\n",
    "\n",
    "        # If the difference is above threshold, consider it a visually different frame\n",
    "        if avg_diff > diff_threshold:\n",
    "            selected_indices.append(frame_idx)\n",
    "            prev_gray = gray\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    return selected_indices\n",
    "\n",
    "# ========== FRAME PROCESSING ========== \n",
    "def process_frame(frame, yolo_model, w, h, class_id_to_name, valid_class_ids, conf_threshold=0.5):\n",
    "    annotated = frame.copy()\n",
    "    results = yolo_model(annotated, verbose=False)[0]\n",
    "\n",
    "    mask = results.boxes.conf > conf_threshold\n",
    "    boxes = results.boxes.xyxy[mask].cpu().numpy()\n",
    "    class_ids = results.boxes.cls[mask].cpu().numpy()\n",
    "    confs = results.boxes.conf[mask].cpu().numpy()\n",
    "\n",
    "    filtered_boxes, filtered_ids = [], []\n",
    "\n",
    "    for box, cls_id, conf in zip(boxes, class_ids, confs):\n",
    "        if int(cls_id) in valid_class_ids:\n",
    "            filtered_boxes.append(box)\n",
    "            filtered_ids.append(cls_id)\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            class_name, color = class_id_to_name[int(cls_id)]\n",
    "            label = f\"{class_name} {conf:.2f}\"\n",
    "            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(annotated, label, (x1, max(y1 - 10, 10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "    return annotated, filtered_boxes, filtered_ids\n",
    "\n",
    "# ========== VIDEO CAPTURE ========== \n",
    "def setup_video_capture(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    return cap, total_frames, fps, w, h\n",
    "\n",
    "# ========== MAIN FUNCTION ========== \n",
    "# ========== MAIN FUNCTION ========== \n",
    "def process_all_videos(yolo_weights_path, class_id_to_name, underrepresented_class_ids,\n",
    "                       video_dir='videos', output_base='./datasets/opt', diff_threshold=30.0):\n",
    "    print(\"[+] Using Model\", yolo_weights_path)\n",
    "    yolo_model = YOLO(yolo_weights_path)\n",
    "\n",
    "    image_out_dir = os.path.join(output_base, 'images')\n",
    "    label_out_dir = os.path.join(output_base, 'labels')\n",
    "    output_video_dir = os.path.join(output_base, 'output')\n",
    "\n",
    "    os.makedirs(image_out_dir, exist_ok=True)\n",
    "    os.makedirs(label_out_dir, exist_ok=True)\n",
    "    os.makedirs(output_video_dir, exist_ok=True)\n",
    "\n",
    "    total_frame_count = 0\n",
    "    total_bounding_boxes = 0\n",
    "    total_label_files = 0\n",
    "\n",
    "    for video_file in tqdm(sorted(os.listdir(video_dir))):\n",
    "        if not video_file.lower().endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        video_id = os.path.splitext(video_file)[0]\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        output_video_path = os.path.join(output_video_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        print(f\"\\n========== STARTED: {video_id} ==========\")\n",
    "        cap, total_frames, fps, w, h = setup_video_capture(video_path)\n",
    "        writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "\n",
    "        selected_frames = get_different_frame_indices(video_path, diff_threshold)\n",
    "        frame_count = 0\n",
    "        current_index = 0\n",
    "\n",
    "        pbar = tqdm(total=len(selected_frames), desc=f\"{video_id} (sampled)\")\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if current_index in selected_frames:\n",
    "                annotated_bgr, boxes, class_ids = process_frame(\n",
    "                    frame, yolo_model, w, h, class_id_to_name, underrepresented_class_ids)\n",
    "\n",
    "                if boxes:  # Only save if there are valid detections\n",
    "                    # Save original image for the current frame\n",
    "                    img_filename = f'{video_id}_{frame_count:04d}.jpg'\n",
    "                    img_path = os.path.join(image_out_dir, img_filename)\n",
    "                    cv2.imwrite(img_path, frame)\n",
    "\n",
    "                    # Save YOLO label for the current frame\n",
    "                    label_filename = f'{video_id}_{frame_count:04d}.txt'\n",
    "                    label_path = os.path.join(label_out_dir, label_filename)\n",
    "                    with open(label_path, 'w') as f:\n",
    "                        for box, cls_id in zip(boxes, class_ids):\n",
    "                            x1, y1, x2, y2 = box\n",
    "                            w_box = x2 - x1\n",
    "                            h_box = y2 - y1\n",
    "                            cx = x1 + w_box / 2\n",
    "                            cy = y1 + h_box / 2\n",
    "                            f.write(f\"{int(cls_id)} {cx/w:.6f} {cy/h:.6f} {w_box/w:.6f} {h_box/h:.6f}\\n\")\n",
    "\n",
    "                    # Increment label file counter\n",
    "                    total_label_files += 1\n",
    "                    total_bounding_boxes += len(boxes)\n",
    "\n",
    "                    # Write annotated frame to video\n",
    "                    writer.write(annotated_bgr)\n",
    "\n",
    "                    # Increment only if we saved something\n",
    "                    frame_count += 1\n",
    "                    pbar.update(1)\n",
    "\n",
    "            current_index += 1\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "        pbar.close()\n",
    "        print(f\"DONE: {video_id} — Processed {frame_count} meaningful frames\")\n",
    "        total_frame_count += frame_count\n",
    "\n",
    "    print(f\"DONE: Total Processed {total_frame_count} meaningful frames\")\n",
    "    print(f\"Total Bounding Boxes Detected: {total_bounding_boxes}\")\n",
    "    print(f\"Total Label Files Created: {total_label_files}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70f244",
   "metadata": {},
   "source": [
    "### Get Class Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bcf3c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_rare_class_ids(label_dir, class_id_to_name, rare_threshold=1000):\n",
    "    class_counts = defaultdict(int)\n",
    "\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        if not label_file.endswith('.txt'):\n",
    "            continue\n",
    "        with open(os.path.join(label_dir, label_file), 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 1:\n",
    "                    cls_id = int(parts[0])\n",
    "                    class_counts[cls_id] += 1\n",
    "\n",
    "    print(\"🔍 Class-wise instance counts:\")\n",
    "    for cls_id in sorted(class_counts.keys()):\n",
    "        name = class_id_to_name.get(cls_id, (\"Unknown\", []))[0]\n",
    "        count = class_counts[cls_id]\n",
    "        print(f\"Class {cls_id:2d} ({name:10s}): {count} instances\")\n",
    "\n",
    "    rare_class_ids = {cls_id for cls_id, count in class_counts.items() if count < rare_threshold}\n",
    "    print(f\"\\n✅ Rare class IDs (threshold < {rare_threshold}): {rare_class_ids}\")\n",
    "\n",
    "    return rare_class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c030e",
   "metadata": {},
   "source": [
    "### Merge predciton and previous datatsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0bdc3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def merge_yolo_datasets(source1, source2, destination):\n",
    "    # Define subfolders\n",
    "    img1_dir = os.path.join(source1, 'images')\n",
    "    lbl1_dir = os.path.join(source1, 'labels')\n",
    "    img2_dir = os.path.join(source2, 'images')\n",
    "    lbl2_dir = os.path.join(source2, 'labels')\n",
    "    dst_img_dir = os.path.join(destination, 'images')\n",
    "    dst_lbl_dir = os.path.join(destination, 'labels')\n",
    "\n",
    "    # Create destination folders\n",
    "    os.makedirs(dst_img_dir, exist_ok=True)\n",
    "    os.makedirs(dst_lbl_dir, exist_ok=True)\n",
    "\n",
    "    def copy_files(src_img_dir, src_lbl_dir, prefix):\n",
    "        for filename in sorted(os.listdir(src_img_dir)):\n",
    "            if not filename.lower().endswith('.jpg'):\n",
    "                continue\n",
    "            base = os.path.splitext(filename)[0]\n",
    "\n",
    "            # Image\n",
    "            new_img_name = f\"{prefix}_{base}.jpg\"\n",
    "            shutil.copy(os.path.join(src_img_dir, filename),\n",
    "                        os.path.join(dst_img_dir, new_img_name))\n",
    "\n",
    "            # Label\n",
    "            label_file = base + \".txt\"\n",
    "            if os.path.exists(os.path.join(src_lbl_dir, label_file)):\n",
    "                new_lbl_name = f\"{prefix}_{base}.txt\"\n",
    "                shutil.copy(os.path.join(src_lbl_dir, label_file),\n",
    "                            os.path.join(dst_lbl_dir, new_lbl_name))\n",
    "            else:\n",
    "                print(f\"⚠️ No label for {filename}\")\n",
    "\n",
    "    print(\"🔁 Merging original dataset...\")\n",
    "    copy_files(img1_dir, lbl1_dir, prefix=\"orig\")\n",
    "\n",
    "    print(\"➕ Merging predicted video dataset...\")\n",
    "    copy_files(img2_dir, lbl2_dir, prefix=\"pred\")\n",
    "\n",
    "    print(f\"\\n✅ Merge complete! Merged dataset at: {destination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adccd711",
   "metadata": {},
   "source": [
    "### Print Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fbbb0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_results_csv(directory):\n",
    "    \"\"\"Find the results.csv file in the specified directory.\"\"\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if 'results.csv' in files:\n",
    "            return os.path.join(root, 'results.csv')\n",
    "    return None\n",
    "\n",
    "def load_results_csv(results_csv_path):\n",
    "    \"\"\"Load the results CSV into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(results_csv_path)\n",
    "\n",
    "def calculate_total_epochs(df):\n",
    "    \"\"\"Calculate the total number of epochs from the DataFrame.\"\"\"\n",
    "    return df['epoch'].max()\n",
    "\n",
    "def calculate_training_loss(epoch_data):\n",
    "    \"\"\"Calculate the total training loss from the given epoch data.\"\"\"\n",
    "    train_box_loss = epoch_data['train/box_loss']\n",
    "    train_cls_loss = epoch_data['train/cls_loss']\n",
    "    train_dfl_loss = epoch_data['train/dfl_loss']\n",
    "    return train_box_loss + train_cls_loss + train_dfl_loss\n",
    "\n",
    "def calculate_validation_loss(epoch_data):\n",
    "    \"\"\"Calculate the total validation loss from the given epoch data.\"\"\"\n",
    "    val_box_loss = epoch_data['val/box_loss']\n",
    "    val_cls_loss = epoch_data['val/cls_loss']\n",
    "    val_dfl_loss = epoch_data['val/dfl_loss']\n",
    "    return val_box_loss + val_cls_loss + val_dfl_loss\n",
    "\n",
    "def print_final_metrics(df):\n",
    "    \"\"\"Print the final metrics for the last epoch.\"\"\"\n",
    "    final_epoch_data = df.iloc[-1]\n",
    "\n",
    "    # Calculate total training and validation loss\n",
    "    train_loss = calculate_training_loss(final_epoch_data)\n",
    "    val_loss = calculate_validation_loss(final_epoch_data)\n",
    "\n",
    "    # Print overall metrics\n",
    "    print(\"\\n========== Final Training Metrics ==========\")\n",
    "    print(f\"Training Loss: {train_loss:.6f}\")\n",
    "    print(f\"Precision: {final_epoch_data['metrics/precision(B)']:.6f}\")\n",
    "    print(f\"Recall: {final_epoch_data['metrics/recall(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5: {final_epoch_data['metrics/mAP50(B)']:.6f}\")\n",
    "    print(f\"mAP@0.5:0.95: {final_epoch_data['metrics/mAP50-95(B)']:.6f}\")\n",
    "\n",
    "    print(\"\\n========== Final Validation Metrics ==========\")\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "\n",
    "def print_csv_metrics(directory):\n",
    "    \"\"\"Main function to process and print final metrics.\"\"\"\n",
    "    # Find the results.csv file\n",
    "    results_csv_path = find_results_csv(directory)\n",
    "    \n",
    "    if not results_csv_path:\n",
    "        print(\"Error: 'results.csv' file not found in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found results.csv at: {results_csv_path}\")\n",
    "\n",
    "    # Load results CSV\n",
    "    df = load_results_csv(results_csv_path)\n",
    "\n",
    "    # Get the total number of epochs\n",
    "    total_epochs = calculate_total_epochs(df)\n",
    "    print(f\"Total number of epochs: {total_epochs}\")\n",
    "\n",
    "    # Print columns in the CSV\n",
    "    # print(\"\\n========== Columns in CSV ==========\")\n",
    "    # print(df.columns)\n",
    "\n",
    "    # Print final metrics\n",
    "    print_final_metrics(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38c90689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from torchvision.ops import nms\n",
    "import torch\n",
    "\n",
    "# Helper to apply NMS and combine boxes\n",
    "def merge_detections(boxes1, scores1, cls1, boxes2, scores2, cls2, iou_thresh=0.5):\n",
    "    # Combine both sets\n",
    "    boxes = torch.cat([boxes1, boxes2], dim=0)\n",
    "    scores = torch.cat([scores1, scores2], dim=0)\n",
    "    classes = torch.cat([cls1, cls2], dim=0)\n",
    "\n",
    "    # Apply torchvision NMS\n",
    "    keep = nms(boxes, scores, iou_thresh)\n",
    "\n",
    "    return boxes[keep], scores[keep], classes[keep]\n",
    "\n",
    "# Convert model outputs to tensors\n",
    "def extract_boxes(results):\n",
    "    boxes = results.boxes.xyxy\n",
    "    scores = results.boxes.conf\n",
    "    classes = results.boxes.cls\n",
    "    return boxes, scores, classes\n",
    "\n",
    "# Final ensemble prediction function\n",
    "def predict_with_ensemble(image, conf_thresh=0.5, iou_thresh=0.5, class_id_to_name=None):\n",
    "    # Run both models\n",
    "    results1 = model1(image, verbose=False)[0]\n",
    "    results2 = model2(image, verbose=False)[0]\n",
    "\n",
    "    # Extract predictions\n",
    "    b1, s1, c1 = extract_boxes(results1)\n",
    "    b2, s2, c2 = extract_boxes(results2)\n",
    "\n",
    "    # Filter by confidence\n",
    "    mask1 = s1 > conf_thresh\n",
    "    mask2 = s2 > conf_thresh\n",
    "    b1, s1, c1 = b1[mask1], s1[mask1], c1[mask1]\n",
    "    b2, s2, c2 = b2[mask2], s2[mask2], c2[mask2]\n",
    "\n",
    "    # Merge predictions\n",
    "    boxes, scores, classes = merge_detections(b1, s1, c1, b2, s2, c2, iou_thresh)\n",
    "\n",
    "    # Annotate image\n",
    "    annotated = image.copy()\n",
    "    for box, score, cls in zip(boxes, scores, classes):\n",
    "        x1, y1, x2, y2 = map(int, box.tolist())\n",
    "        label = f\"{class_id_to_name[int(cls)] if class_id_to_name else int(cls)} {score:.2f}\"\n",
    "        cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(annotated, label, (x1, max(10, y1 - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "    return annotated, boxes, classes, scores\n",
    "\n",
    "# Optional: Run on video\n",
    "def run_ensemble_on_video(video_path, output_path, class_id_to_name=None):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        annotated, _, _, _ = predict_with_ensemble(frame, class_id_to_name=class_id_to_name)\n",
    "        writer.write(annotated)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    print(f\"[✓] Saved ensemble output to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d7bab",
   "metadata": {},
   "source": [
    "## Calling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f592f31",
   "metadata": {},
   "source": [
    "### Download, convert, split and normalize datsets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54ca66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdrive_url = \"https://drive.google.com/file/d/1UppumYqYOi-kto6BWPfFxwJK2Eph46oY/view?usp=sharing\"\n",
    "# semantic_drone_dataset_download(gdrive_url, extract_to=\"datasets\")\n",
    "\n",
    "# gdrive_url = \"https://drive.google.com/file/d/12cbrTaBAMIsuU-mwAA7IgDk9wSLC9cC-/view?usp=sharing\"\n",
    "# uavdt_dataset_download(gdrive_url, extract_to=\"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "558387bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"./datasets/semantic_drone_dataset/training_set\"\n",
    "# output_dir = \"./datasets/semantic_yolo\"\n",
    "\n",
    "# convert_fulldataset_yolo_only(dataset_path, output_dir, class_id_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "94505830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #UAVDT-2024\n",
    "\n",
    "# source_root = \"./datasets/UAVDT-2024\"\n",
    "# output_root = \"./datasets/new_dataset_yolo_split\"\n",
    "\n",
    "# convert_dataset(source_root)\n",
    "# copy_split_sequences(source_root, output_root, train_ratio=0.8)\n",
    "\n",
    "\n",
    "# # Semantic dorne datasets\n",
    "# split_and_move_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec08031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set your paths\n",
    "# dataset_path = \"./datasets/new_dataset_yolo_split/train\"\n",
    "# image_dir = os.path.join(dataset_path, \"images\")\n",
    "# annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# normalize_all_labels(annotations_dir, image_dir)\n",
    "\n",
    "# dataset_path = \"./datasets/new_dataset_yolo_split/val\"\n",
    "# image_dir = os.path.join(dataset_path, \"images\")\n",
    "# annotations_dir = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "# normalize_all_labels(annotations_dir, image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88e922",
   "metadata": {},
   "source": [
    "### Checking how many classes have how much instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eeb923ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class-wise instance counts:\n",
      "Class  1 (pool      ): 30 instances\n",
      "Class  2 (vegetation): 6178 instances\n",
      "Class  3 (roof      ): 300 instances\n",
      "Class  4 (wall      ): 989 instances\n",
      "Class  5 (window    ): 448 instances\n",
      "Class  6 (person    ): 2589 instances\n",
      "Class  7 (dog       ): 23 instances\n",
      "Class  8 (car       ): 35054 instances\n",
      "Class  9 (bicycle   ): 205 instances\n",
      "Class 10 (tree      ): 464 instances\n",
      "Class 11 (truck     ): 129 instances\n",
      "Class 12 (bus       ): 86 instances\n",
      "Class 13 (vehicle   ): 577 instances\n",
      "\n",
      "✅ Rare class IDs (threshold < 3000): {1, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13}\n"
     ]
    }
   ],
   "source": [
    "labels_dir = './datasets/new_dataset_yolo_split/train/labels'\n",
    "\n",
    "rare_class_ids = get_rare_class_ids(label_dir=labels_dir, class_id_to_name=class_id_to_name ,rare_threshold=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63556279",
   "metadata": {},
   "source": [
    "### Training v8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "56985371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # List of folders to delete\n",
    "# folders_to_delete = ['./datasets/semantic_yolo', './datasets/new_dataset_yolo', './datasets/uavdt-processed', './runs', \"./metrics\"]\n",
    "\n",
    "# for folder_path in folders_to_delete:\n",
    "#     if os.path.exists(folder_path):\n",
    "#         shutil.rmtree(folder_path)\n",
    "#         print(f\"✅ Deleted folder: {folder_path}\")\n",
    "#     else:\n",
    "#         print(f\"⚠️ Folder does not exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "735f81a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_yolo_from_scratch(\n",
    "#     data_yaml=\"yolov8.yaml\",\n",
    "#     epochs=100,\n",
    "#     imgsz=720,\n",
    "#     batch=8,\n",
    "#     name=\"yolov8\",\n",
    "#     model_variant=\"yolov8n.pt\"  # or yolov8s.pt, yolov8m.pt, etc.\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c39cf0",
   "metadata": {},
   "source": [
    "### Print metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "830c11fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found best.pt at: runs\\train\\yolov8\\weights\\best.pt\n"
     ]
    }
   ],
   "source": [
    "yolov8 = './runs/train/yolov8'\n",
    "best_pt_path = find_best_model(yolov8)\n",
    "# evaluate_and_save_metrics(best_pt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44ab66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_per_class_metrics(\"per_class_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d15b47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_csv_metrics(yolov8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230e616",
   "metadata": {},
   "source": [
    "### Prediciton videso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "01fe5528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deleted folder: ./datasets/new-videos-predicted-yolo\n",
      "⚠️ Folder does not exist: ./datasets/merged_yolo_dataset\n",
      "⚠️ Folder does not exist: ./datasets/split_videos_dataset\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# List of folders to delete\n",
    "folders_to_delete = ['./datasets/new-videos-predicted-yolo', \"./datasets/merged_yolo_dataset\", \"./datasets/split_videos_dataset\"]\n",
    "\n",
    "for folder_path in folders_to_delete:\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"✅ Deleted folder: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Folder does not exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "57b725d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "print(list(rare_class_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9cc5a553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Using Model runs\\train\\yolov8\\weights\\best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a98ee124ca4501bef5a883235b143e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== STARTED: v1 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee626d74fcd40e297d127e7584b32ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v1 (sampled):   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v1 — Processed 1 meaningful frames\n",
      "\n",
      "========== STARTED: v12 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8f588df9db467b8afe2721d65530ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v12 (sampled):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v12 — Processed 2 meaningful frames\n",
      "\n",
      "========== STARTED: v2 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea5f91af6d84c22a0aa7659b6163433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v2 (sampled):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v2 — Processed 2 meaningful frames\n",
      "\n",
      "========== STARTED: v3 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177013de0c594528bff7e3864440af26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v3 (sampled):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v3 — Processed 1 meaningful frames\n",
      "\n",
      "========== STARTED: v4 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851c0cd5acdc49689c31f1dd3c09ee40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v4 (sampled):   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v4 — Processed 2 meaningful frames\n",
      "\n",
      "========== STARTED: v5 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3d7822dccb422890e5dd5602d53ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v5 (sampled):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v5 — Processed 0 meaningful frames\n",
      "\n",
      "========== STARTED: v6 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b376998e6984d8489ffbd8da5b7b673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v6 (sampled):   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v6 — Processed 1 meaningful frames\n",
      "\n",
      "========== STARTED: v8 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabcddb28ff045bc8d64895293606dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v8 (sampled):   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v8 — Processed 0 meaningful frames\n",
      "DONE: Total Processed 9 meaningful frames\n",
      "Total Bounding Boxes Detected: 21\n",
      "Total Label Files Created: 9\n"
     ]
    }
   ],
   "source": [
    "process_all_videos(best_pt_path, class_id_to_name, list(rare_class_ids),\n",
    "                       video_dir='videos', output_base='./datasets/new-videos-predicted-yolo', diff_threshold=99.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4fce1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Class-wise instance counts:\n",
      "Class  3 (roof      ): 1 instances\n",
      "Class  4 (wall      ): 3 instances\n",
      "Class  6 (person    ): 6 instances\n",
      "Class 10 (tree      ): 9 instances\n",
      "Class 13 (vehicle   ): 2 instances\n",
      "\n",
      "✅ Rare class IDs (threshold < 0): set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Folder with YOLO label files\n",
    "label_dir = './datasets/new-videos-predicted-yolo/labels'\n",
    "\n",
    "get_rare_class_ids(label_dir=label_dir, class_id_to_name=class_id_to_name ,rare_threshold=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e192e723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in './datasets/new-videos-predicted-yolo/images': 9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_directory(directory_path):\n",
    "    return len([f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))])\n",
    "\n",
    "# Example usage\n",
    "directory = \"./datasets/new-videos-predicted-yolo/images\"  # Change this to your target directory\n",
    "file_count = count_files_in_directory(directory)\n",
    "print(f\"Number of files in '{directory}': {file_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bc6a5518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afee9f157c0b4957902fa530a28a8898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Moving to train:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04da3474a484367a8d159bec75cb852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Moving to val:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[✓] Dataset split completed: 6 train / 3 val samples\n"
     ]
    }
   ],
   "source": [
    "split_and_move_dataset(source_base_dir=\"./datasets/new-videos-predicted-yolo\",\n",
    "                           target_base_dir=\"./datasets/split_videos_dataset\",\n",
    "                           split_ratio=0.7,\n",
    "                           seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92deee",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2146513351.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[89], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    .....\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3013569",
   "metadata": {},
   "source": [
    "### Merge previous and new prediction Ddatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_yolo_datasets(\n",
    "    source1='./datasets/new_dataset_yolo_split/train',\n",
    "    source2='./datasets/split_videos_dataset/train',\n",
    "    destination='./datasets/merged_yolo_dataset'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with YOLO label files\n",
    "label_dir = './datasets/merged_yolo_dataset/labels'\n",
    "\n",
    "rare_class_ids = get_rare_class_ids(label_dir=label_dir, class_id_to_name=class_id_to_name ,rare_threshold=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2ca7c",
   "metadata": {},
   "source": [
    "## Retrain Model on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f1df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# import glob\n",
    "\n",
    "# # Match all folders starting with 'fine-tune-yolov8' inside './runs_yolo/train/'\n",
    "# folders_to_delete = glob.glob('./runs/train/fine-tune-yolov8*')\n",
    "\n",
    "# for folder_path in folders_to_delete:\n",
    "#     if os.path.isdir(folder_path):\n",
    "#         shutil.rmtree(folder_path)\n",
    "#         print(f\"✅ Deleted folder: {folder_path}\")\n",
    "#     else:\n",
    "#         print(f\"⚠️ Not a directory or doesn't exist: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine_tune_yolo(\n",
    "#     data_yaml=\"yolo_retrain.yaml\",        # Your updated dataset YAML\n",
    "#     epochs=40,\n",
    "#     imgsz=720,\n",
    "#     batch=8,\n",
    "#     name=\"fine-tune-yolov8\",\n",
    "#     base_model_path=best_pt_path  # Your previous model path\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28391f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_path = './runs/train/fine-tune-yolov8'\n",
    "# print_csv_metrics(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def compare_final_metrics(csv1_path, csv2_path):\n",
    "#     # Load both result CSVs\n",
    "#     df1 = pd.read_csv(csv1_path)\n",
    "#     df2 = pd.read_csv(csv2_path)\n",
    "#     # print(df1.head())\n",
    "#     # Use the final row (last epoch)\n",
    "#     last1 = df1.iloc[-1]\n",
    "#     last2 = df2.iloc[-1]\n",
    "\n",
    "#     metrics_to_compare = {\n",
    "#         \"train/box_loss\": \"Box Loss (Train)\",\n",
    "#         \"train/cls_loss\": \"Cls Loss (Train)\",\n",
    "#         \"train/dfl_loss\": \"DFL Loss (Train)\",\n",
    "#         \"metrics/precision(B)\": \"Precision\",\n",
    "#         \"metrics/recall(B)\": \"Recall\",\n",
    "#         \"metrics/mAP50(B)\": \"mAP@0.5\",\n",
    "#         \"metrics/mAP50-95(B)\": \"mAP@0.5:0.95\",\n",
    "#         \"val/box_loss\": \"Box Loss (Val)\",\n",
    "#         \"val/cls_loss\": \"Cls Loss (Val)\",\n",
    "#         \"val/dfl_loss\": \"DFL Loss (Val)\"\n",
    "#     }\n",
    "\n",
    "#     print(\"🔍 Comparison of Final Epoch Metrics:\\n\")\n",
    "#     for key, label in metrics_to_compare.items():\n",
    "#         val1 = last1[key]\n",
    "#         val2 = last2[key]\n",
    "#         trend = \"✅ Good Increase\" if val2 > val1 else \"❌ No Increase\"\n",
    "#         print(f\"{label:20s}: {val1:.5f} → {val2:.5f} | {trend}\")\n",
    "\n",
    "# # Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25163e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_path = './runs/train/fine-tune-yolov8'\n",
    "# old_path = './runs/train/yolov8'\n",
    "\n",
    "# results_csv_path = find_results_csv(new_path)\n",
    "# results_csv_path_1 = find_results_csv(old_path)\n",
    "\n",
    "# compare_final_metrics(results_csv_path_1, results_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33362264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_pt_path = find_best_model(new_path)\n",
    "# evaluate_and_save_metrics(best_pt_path, output_json_path=\"per_class_metrics_retrain.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edceacbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_per_class_metrics(\"per_class_metrics_retrain.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def compare_maps(json_path1, json_path2):\n",
    "#     with open(json_path1, 'r') as f1, open(json_path2, 'r') as f2:\n",
    "#         metrics1 = json.load(f1)\n",
    "#         metrics2 = json.load(f2)\n",
    "\n",
    "#     print(\"\\n📊 Comparison of mAP@0.5:0.95 per class:\\n\")\n",
    "#     print(f\"{'Class':<15} {'Before':<10} {'After':<10} {'Change'}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "#     for class_name in metrics1:\n",
    "#         map1 = metrics1[class_name].get(\"mAP@0.5:0.95\", 0)\n",
    "#         map2 = metrics2.get(class_name, {}).get(\"mAP@0.5:0.95\", 0)\n",
    "\n",
    "#         if map2 > map1:\n",
    "#             status = \"✅ Good increase\"\n",
    "#         else:\n",
    "#             status = \"❌ No increase\"\n",
    "\n",
    "#         print(f\"{class_name:<15} {map1:<10.4f} {map2:<10.4f} {status}\")\n",
    "\n",
    "# # 🔧 Example usage:\n",
    "# compare_maps(\"per_class_metrics.json\", \"per_class_metrics_retrain.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found best.pt at: runs\\train\\fine-tune-yolov8\\weights\\best.pt\n",
      "✅ Found best.pt at: runs\\train\\yolov8\\weights\\best.pt\n"
     ]
    }
   ],
   "source": [
    "new_path = './runs/train/fine-tune-yolov8'\n",
    "old_path = './runs/train/yolov8'\n",
    "\n",
    "best_pt_path_retrain = find_best_model(new_path)\n",
    "best_pt_path = find_best_model(old_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933909df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos_predictions(best_pt_path, class_id_to_name, video_dir='videos', output_base='./datasets/final_output', max_frames=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7872e10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df469289f1964e98bb01ec24afd9e6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== STARTED: v1 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a117674aa1248f49d89fcbd9cf6d99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v1:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v1 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v12 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d47bb2ec8b401a939a43ac6b86810b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v12:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v12 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v2 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002ea4cedcc34884ba06f615909e582b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v2:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v2 — Processed 175 frames\n",
      "\n",
      "========== STARTED: v3 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c54414f9b143008fdde9161d3b65ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v3:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v3 — Processed 176 frames\n",
      "\n",
      "========== STARTED: v4 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc24d4b23cba4081ab1a8878925d810c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v4:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v4 — Processed 253 frames\n",
      "\n",
      "========== STARTED: v5 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e7d16310664a1cb8ac9088e6b757b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v5:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v5 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v6 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94bb1c6b1a243d9adec746eeab7979c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v6:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v6 — Processed 400 frames\n",
      "\n",
      "========== STARTED: v8 ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67cfccc9cfe4054965537674cf6d768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v8:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: v8 — Processed 400 frames\n"
     ]
    }
   ],
   "source": [
    "videos_predictions(best_pt_path_retrain, class_id_to_name, video_dir='videos', output_base='./datasets/final_output_retrain', max_frames=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
